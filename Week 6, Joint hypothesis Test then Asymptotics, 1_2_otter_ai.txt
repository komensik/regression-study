Unknown Speaker  0:00  
What's called a joint hypothesis, where we're testing a set of restrictions. The most common example is sort of the join analog of the standard output you get in NLM, which is testing each coefficient against zero. The kind of standard analog for that in the joint context, is testing a set of coefficients against zero. So for this sort of generic joint null hypothesis test, we're testing the null hypothesis that beta one through beta k are jointly equal to zero, or in other words, that they're all equal to zero. So what does that mean? Well, there's a subtle point here that's important, and that's the if we reject this null, and we'll see examples of this in a second, it doesn't tell us that any one of these coefficients is significant meaning if we reject this, no, it just tells us that the CO fit, that this joint hypothesis can be rejected. Now, what that means in practice is a little bit hard to delete, right? It's not actually entirely clear how to interpret it, because all you're saying is that this, I can I feel confident that this is not true, but it doesn't tell you that, for example, beta one is not equal to zero, the beta two is not equal to zero. It doesn't tell you that beta one and beta two are not equal to zero. It tells you exactly what this says, that this joint hypothesis can be rejected. So when we get to some examples, we can try to explore this a bit more. But the thing to remember, first and foremost is that you are testing a set of restrictions and your and if you reject that set of restrictions, that's all you can say. Reject that set so the first thing you might ask is, you know, this seems this might be easy, right? Imagine we have, you know, a regression where we've got K predictors, and we want to know that, you know, whether or not that set of K predictors as a whole contributes to explaining the detail variable. So one way to think about that would be, is my model doing better than the model with only an intercept. That's something we often think about. Is the model I'm estimating doing better than a model that only the intercept. That's what r square tells us as long, and so maybe we want to test that hypothesis. So one thing you could ask is, well, let's just take out those k predictors and see if model fit changes. If you might think to yourself, right, if this set of predictors jointly contributes to explaining why? Then if we take them out, fit should go down, and we should be able to conclude that that null hypothesis can be rejected. Now what's the problem with that?

Unknown Speaker  3:16  
Why can we just remove the predictor to see how model fit changes and conclude

Unknown Speaker  3:31  
that the novel so it would be an intercept on the model. So if we're imagining a scenario where we have K predictors, and we think this is false, but we want to test it. We take out the K predictor and estimate an intercept only model, and we see if it goes down, and if it goes down, we reject that. Why can't we do just? Why is that not sufficient?

Unknown Speaker  3:55  
Because mechanically, every time we take away a predictor, model

Unknown Speaker  4:02  
and fit decreases. Yeah, exactly. That's exactly right, right. We talked about this in the context of r squared. If you add a noise predictor to a model, r squared will go up. So if you just take out a bunch of predictors, fit will go down. But the question for us is not whether fit goes down at all, right, it's whether fit goes down more than we would expect by chance alone, right? If these predictors are actually explaining some of the dependencies. And so this is really just another way of saying that we need to do some kind of statistical test here to see if the decrease in model fit is more than would be expected as a result of simply giving our model more degrees of freedom, we will always get worse a bit if we add degrees of freedom, if we take out predictors, the question is, is the model fit decreased enough to conclude that this null is false or that can be

Unknown Speaker  4:58  
rejected? Does that also count if you look at the adjusted R square, then you're adjusting

Unknown Speaker  5:03  
for yes. So one thing you could you can potentially start thinking about doing where I'm going to talk about a particular kind of significance test that we can do. But one thing alternative that you might think about are alternatives that adjust for exactly this problem. So adjusted R square would be one. One thing you could look at towards the end of the semester, I think we'll talk a little bit about fit statistics, such as the Bayesian information criterion, or high Kay's information criterion, that penalize you for losses of degrees of freedom, and that people often use those statistics as a way to compare models without doing significance tests. We'll also talk about out of sample prediction as a way of comparing models as well. So that's totally right, and those are trying to solve the same problem. And so what I'm gonna talk about for today is just a sort of significance test oriented way of solving that

Unknown Speaker  6:01  
problem that makes sense other questions so far, that basic point.

Unknown Speaker  6:11  
Ok, so to do a join null hypothesis test, we're going to use what's called the f statistic, and we're going to construct the f statistic like this. This looks pretty gruesome, but it's not actually that bad. Let's see what's going on here, right? So SSR just means sum of squared residuals. The sum of the squared residuals is quite literally taking all the residuals, squaring them and summing them up. We've talked about that sum square before, and we have, we just have one sort of new notational court here, which is that we're distinguishing sum of square residual subscript u r from sum of squared residual subscript r. And those, I mean, this is, I guess, my notation probably, but this just stands for unrestricted and restricted model. So the unrestricted model is the model that keeps all the predictors in the model. It's unrestricted because it estimates the extra parameters that you're interested in. The restricted model is the one that sets the it sets those parameters to some value or to zero, in this case. So the restricted model is restricted in the sense that it assumes this by excluding those predictors from the model. It's restricted through that exclusion, the unrestricted model freely estimates those parameters, so it has fewer degrees of freedom. It's in that sense, unrestricted. So in the denominator, then we have the sum squared residuals for the unrestricted model, where we estimate all those parameters. In the numerator, oh, and then what are we dividing by? We're dividing by n minus k minus one degrees of freedom, which is just the degrees of freedom for the unrestricted model. And if you notice that's actually just the mean square error or our unbiased estimator for the error variance of the unrestricted so we have the estimated error variance for the unrestricted model in the denominator. What do we have in the numerator? Well, we have a difference in sum of squared residuals. We have the sum of squared residuals for the restricted model minus the SSR for the unrestricted model. And what is that? You know? What should that? How should we think about that? Well, this will always be positive for exactly the reason we just talked about. The sum of squared residual will always go up if we take parameters out of the model, because the fit, the Fit goes down. That's just another way of saying the Fit gets worse. And the Fit gets worse because we've removed predictors. So even if they're noise predictors, we'll still get a lack of a decrease in fit. So that decrease in fit going from u r to r leads to a positive value of this difference. This is a higher value than this, and we're dividing by q, and q is just the number of restrictions that are present for this hypothesis. So we have K predictors in the unrestricted model, and we have some fewer number of predictors in the restricted model, Q tells us how many predictors, how many fewer parameters we are estimating. So if we take out all K predictors, then Q is equal to k. If we take out one predictor, Q is equal to 122, so the number of restrictions q is the number. So one way to think about this right is that we've got the the unbiased estimator of the error variance of the unrestricted model in the denominator, and we've got a very similar kind of quantity in the numerator. But instead of being, you know, the you know, the the error variance for a particular model, it's something that looks a lot like the difference in the error variance between the two. So it's basically a ratio of error variance. One other way to look at it would be that we're taking, if you think about this as a product of two ratios, it's the ratio of the difference in the sum of the squared errors compared to the base line, restricted unrestricted model times the ratio of the number of restrictions divided by the difference of freedom for the unrestricted model. But it's probably more intuitive to think about it in terms of error variance of unrestricted and then this kind of variance associated with the difference between the difference between the two models in the numerator, because that ratio is going to give us a statistic that's distributed F so for the F Distribution in general, is formed from ratio to chi square distribution divided by the degrees of Freedom. Both of these are chi square divided by their respective freedom. So this is a ratio of two chi square, chi square distributed random variables divided by their respective degrees of freedom. That's just the definition of an F distributed random variable. And so what that means is that this statistic is distributed F and so, if f and so, this statistic F is distributed F, I should notation, right? But there's an F statistic distributed F. The F distribution has two parameters associated with it, one, two, and it's going to be for this hypothesis test, it's going to be an F distribution on q and n minus k minus one degrees of freedom. So a t distribution has one least the t distribution we use has one parameter. It's the degrees of freedom. F distribution has two parameters, the numerator, degrees of freedom and the denominator, where q is the number of restrictions, n minus k minus one is the degrees of freedom for the unrestricted model. So all of this is a very long way to say right, if you estimate the model you're interested in, right, the alternative hypothesis, the unrestricted model, and then you also estimate the restricted model, where you remove some subset of the parameters right you can calculate the sum of the squared residuals for both of those models, and then you can easily construct an F statistic using those values, and you know that that statistic is distributed F on q and n minus K minus one degrees of freedom under the null hypothesis.

Unknown Speaker  12:48  
So by putting that all together, we were going to be able to construct a standard null hypothesis test where we say something like, if f our calculated statistic is large enough, right? Large enough that it would be very unlikely if the MEL hypothesis were true, then we can reject it. This can be a standard sort of logic, null hypothesis. If this is big enough compared to this distribution, then we reject it. When will this be big? It'll be big when this difference is very big compared to the degrees of freedom associated with the test. This has this difference in some sort of residuals has to be bigger the more restrictions you place. You're dividing it by the number of restrictions. So the number of restrictions is going to is essentially penalizing you for increasing bit. Okay, one I want to say one more thing that we can ask questions, the F is always positive. And you can see that, right, this is always going to be positive. And so the one difference between t testing normal testing and testing with an F distribution is that the F is always greater than zero, and so we're always going to have, quote, unquote, one tail. There's no such thing as a two tail f test here. We're always going to be trying to reject in the positive side of the distribution. And I'll show you that in a second. Okay, so that was a lot. Are there questions so far? Kind

Unknown Speaker  14:32  
of makes sense this. I'm finding it like, the intuition behind

Unknown Speaker  14:50  
like, why? I guess maybe it's just a like, I need to think about this more, but the intuition behind why? I'm wondering if you can put kind of a general intuition behind why this allows us to reject the null in a different way.

Unknown Speaker  15:10  
It's not that it

Unknown Speaker  15:11  
allows us or like the large. I mean I and I understand it's if it's large enough. Maybe I just need to think about what it what the numerator actually is, let me try to say it one more time. Thank you. Yeah.

Unknown Speaker  15:29  
So, so the first thing to think about is exactly that what the numerator is, the numerator is, is essentially, you know, a decrease in model fit, how much, how much model fit you're losing by throwing out those q parameter and so under the null hypothesis, right? And remember, what the null hypothesis is? It's that all these parameters really are zero in the population, or all these predictors are not actually good explaining anything of the dependent variable. So that's the null. This is telling you how much fit you're losing by kicking those parameters out. Now if the null is true, you will still lose fit mechanically because of the increase in degrees of freedom, but our question is whether that decrease in fit from throwing those parameters out is more than we would expect due to that mechanical increase in degrees of freedom. Are we throwing out predictors that actually explain the different variable, or are we just gaining degrees of freedom and getting some mechanical increase in model that it's not interesting at all, because it's just we're capitalizing on chance pieces of the data and so, so those are, that's the first two pieces. The next piece is to say, what was this calculation? How is this calculation, this statistic, distributed? If the null is true, and the answer right is that it's distributed exactly this F on q and n minus k minus one degrees of freedom. And so what that's going to look like in practice is that you're going to get a distribution that it looks kind of like that. And this is like an F isn't a particular f distribution under the null hypothesis that all these parameters are truly equal to zero, so this statistic f will be larger when this is larger relative to its degrees of freedom. And what we're going to say is, when we calculate that F statistic, let's call that f hat or F stat or something. Is it large enough due to this numerator, this change in fit such that this null hypothesis seems unlike, and we'll say it's unlikely, if it's so far out on the tail that the probability you would get that by chance alone under the null hypothesis,

Unknown Speaker  18:12  
that's super helpful. Thank you. Why

Unknown Speaker  18:16  
is this telling us

Unknown Speaker  18:22  
that we wouldn't be do by interpreting

Unknown Speaker  18:24  
the coefficient? Yeah, so that's a great question. And it gets back to this sort of subtlety of what you're doing testing a join cell hypothesis, which is that you can't look at each individual hypothesis test. Then you get from your LM output and say, Okay, I can reject this, when I can reject this, when I can reject this, one, therefore the join. And I'll show you an example in a second. It can absolutely be the case, and oxygen will be the case that a joint alpha will be rejected even when individual coefficients cannot be rejected. In

Unknown Speaker  19:01  
theoretical forms. This is, this is so much more about just the mechanics of model fit than your theory and the

Unknown Speaker  19:11  
effect of the variables, right? I want to say I kind of agree with what you just said, but, but, like, I'm a little uncomfortable with the it's just about model fit rather than theory, because the cases where you would use this are situations where you might want to say something like my theory says that the inclusion of these two predictors in the model should increase model fifth or should go some way towards explaining the decom variable. And so it's the appropriate test to do that won't always be the case. It'll it may sometimes be the case that most of the time, there's only single parameter, but like for example, so let's say you have a three way interaction. We'll talk about that a bit more in a couple weeks. But three way interaction comes with three two way interactions and one three way interaction. So to fully specify a three way interaction, you need all three two way interactions and the three way if you want to. I mean, one thing you could do is just test that COVID. You might want to test whether all four of those parameters are jointly zero or not even that. Oh, here's a better example. Sorry, when we get in like two weeks, when we get to qualitative variables. So imagine you've got religious identification as an independent variable that's usually going to come with a set of dummy variables, right where you have, you know, identity, identification as Catholic, Protestant, Muslim, whatever, and you have a dummy variable for every one of those categories, and you want to know whether that variable contributes significantly to the decent variable. You can't look at each dummy variable because that just tells you a test of each of those categories versus the base line. And so to see if religious identification matters, you have to do a join test. Is that example?

Unknown Speaker  21:11  
Yeah, I think the interaction, But your point is well taken in the sense that this is going to be less common than you will be other questions. Yeah. Questions.

Unknown Speaker  21:27  
Okay, all right, so, so let's actually look at an example. And this is a nice example, because it'll, I think this, this will complete our, you know, our treatment of regression output using LM so that you'll now know everything in LMS output. So let's compare we're going back to this European integration, this British election Panel Study data, where we're predicting opposition to European integration on a set of independent variables, including age, gender, retrospection, both household and national and political knowledge. And let's say that we want to know right whether our model, which is this model, let's call it our model is our model has these five predictors in it. We want to know if our model does better than a baseline model, which is only has an intercept. So are we, is our model doing a better job explaining opposition to European integration than a model that just guesses the mean of that dependent variable? Every time, are we doing a better job explaining the dependent variable based on these independent variables than we were just guessing the mean of the dependent variable? Mean. So to do that, we're going to, we're going to do an F test. We're going to start, you know, we start by estimating our model, and we've already done that in previous slides, right? So this is m, x is this model down here? That's the unrestricted model. So I'm going to rename it m1, underscore, u, r. It's unrestricted because we're estimating all the parameters of interest. Our restricted model, in this case, is corresponds with the null hypothesis that all of these predictors can be thrown out without any cost. Basically, we're not gaining anything by using these variables compared to an intercept only model. That's the null, that's the restricted model. So I can estimate the restricted model by just putting in an intercept to the regression. So this is saying, use OLS to regress opposition to European integration on an intercept. That's the code for getting an intercept only model. I'll talk about this a little bit later. Don't worry about that quite yet, for now, right? I'm estimating an unrestricted model and a restricted model, and what we want to know right is whether we're gaining anything from our unrestricted model relative to the restricted model. Okay, so what does the f test look like? Let's calculate each of those components. So we need the numerator difference which has the difference in the sum of the squared residuals between the restricted and unrestricted model. So here we're just taking the sum of the squared residuals of the restricted model minus the sum of the squared residuals from the unrestricted model, I can pull out right. And so remember, in the numerator, we also have we're dividing this right by the difference in the degrees of freedom. Right q is the number of restrictions, and the number of restrictions is the difference in the degrees of freedom between the two models. So here I'm just calculating the difference in the degrees of freedom as the number of degrees of freedom in the restricted model versus the unrestricted model. And this will always be positive, because we're throwing out parameters and gaining degrees of freedom. We also in the in the denominator, we need the unrestricted model sum of squares, which is, again just that. And we need the degrees of freedom for the unrestricted model, which is n minus k minus one. But I can again just pull that out of the model object as well. Once we have that, the S statistic is very easy to calculate. The numerator is the difference in the sum of the square residuals divided by Q, the number of restrictions, divided by the sum of the square root of the unrestricted model divided by the degrees of freedom to the unrestricted model. That ratio is an F staff. And so now right, I just want to see if that F statistic is far enough out on the null distribution to reject a null and I do that using a quote, unquote, one tail test, because f is always positive. So here I'm just saying, OK, my F stat is equal to that calculation. What's the p value associated with that F stat? Well, let's think about what a p value is. Again, it's the area under the curve to the right of my statistic. And so I just need to carry I need to calculate the area under the curve beyond the f statistic on an F distribution with degrees of freedom equal to q and n minus k minus one. So this is the cumulative distribution function for the F on q and n minus k minus one degrees of freedom evaluated at the statistic is possible if I take one minus that, right? So this calculation here, because it's the cumulative distribution, gives me the area under the curve up to here. But what I want is the opposite of that. I want the area of the curve beyond that. So I do one minus. One minus that gives me this. PF of the F stack gives me this. I take one minus and I hit that, and that's my P value. And so you'll see, in this case, the f statistic is 25 which is very big for an F statistic, reasonably big. And so we have a corresponding p value, approximately zero. It's effectively zero. This isn't truly zero, but it's very, very, very, very restricted model and the unrestricted model do

Unknown Speaker  27:15  
we get to? Obviously, this is the interface. Inter concept form, but all of the other

Unknown Speaker  27:26  
parameters. But is there a way in which we could, if we see that one or two parameters is highly significant, we

Unknown Speaker  27:34  
could have a restrictive model that

Unknown Speaker  27:37  
still includes, oh, yeah, yeah, totally. So this first example, and I'll show you another example in the second reading, just exactly that this example I'm doing for a very particular reason. I'm going to show you something in a second. But it is not the case that you can only do a null hypothesis test of an intercept phony model versus a saturated model. You can, you know, you could do it with just age and gender taken out, or just you need to do it with just one of these taken out, and you can still do that with us. So anything you can think of, you can do. This is just a simple example.

Unknown Speaker  28:17  
Okay, more questions. So far, I it, right? So, so again, showing you, sort of the mechanics of the test. An easier way to do this that does the exact same thing is to use the ANOVA function, which is technically in the stats package in R but you don't ever have to load the stats package. It just it's automatically loaded with base r. So if you just call ANOVA, it'll do this test. And so to do that, you just plug in the restricted model, the unrestricted model as the first two entries of the ANOVA function, and it'll give you an app test on it'll give you exactly that test, and you can see that 25.22 effectively zero. 25.22 effectively Sure. So this is doing this exactly

Unknown Speaker  29:11  
with the normal function. Can you actually specify

Unknown Speaker  29:15  
more models

Unknown Speaker  29:18  
and just run them together? Or can you only compare the two at a time? You can only compare two to

Unknown Speaker  29:27  
10? Yeah, I think that's just, I think that's the blank answer. I'm trying to think about, like a situation where that wouldn't be true. But, yeah, it's just you compare two models. But you know, if you wanted, if you want to specify, like, 10 different models, and you wanted to go one by one, testing them against some baseline model you could write, like a loop or

Unknown Speaker  29:54  
something over that's true. Well, you would have to write LM code. I'm thinking of different there comes a point where spending spending the time thinking of ways to do it more efficiently becomes less efficient, and you have to, like, just do it the long term, but

Unknown Speaker  30:12  
usually with with this, if you specify each model and just like, pull one variable out and do an app test all those against the base one at a time. You

Unknown Speaker  30:23  
could do that, but that's that's the same thing as doing a t test on those individual variables. So you automatically get that with your output your regression.

Unknown Speaker  30:36  
Other questions, I question,

Unknown Speaker  30:46  
an F test taking only one variable at a time is actually just like a t test. F is, in that case, I think F is just the square t the f statistic is the t statistic square? Do? So this is only valuable if you're doing something more than just that. Okay, all right, so, so let's think about what this all means, right? You can do this more easily this way. So it's highly statistically significant. What does that mean? Again, it means that all the predictors, taken together, are jointly significant, or in other words, we can reject and know that they're all jointly equal to zero. And that's basically it, in this context, right that that sort of has a natural interpretation of our model is doing better than the intercept building model, which is the same kind of thing that r square r squared is telling us. Now, what I want to show you here again is that this is kind of the last piece of our regression output. You'll see at the bottom here. When you get, you know, this is our R model at the bottom of l n output, when you summarize it, you'll get F statistic on five and 1519, unit of freedom and a p value for that F statistic. What that's giving you is exactly what we just did here. And here it's when you see this F statistic on your LM output, it is testing your model against an intercept only model using an app text. So you'll see that this is exactly the same 25 225, 1519, and zero. 22 2205,

Unknown Speaker  32:34  
1519, all from the that's what this slide this last line, I think at this point now you have enough knowledge to interpret this whole questions

Unknown Speaker  32:59  
about that so question. It doesn't have to be versus an intercept only model. So let's, let's again, think about our model right as this model. And let's say we're testing a null hypothesis that instead of an intercept only model, we're testing the model without age and gender. So we want to see if adding age and gender to the model improves model fifth, above and beyond just including retrospections and political knowledge. And I mean, why you want to do this with us? Maybe it's like, oh, new demographics matter. So we're going to test the null hypothesis that these two parameters are jointly equal to zero. And here I'm going to show you a way to do this that you might not have seen this update function before. So it can be convenient in some situations. So ANOVA, right again. ANOVA says, give me the restricted model and then the unrestricted model. So the unrestricted model is just the same thing we've been estimating. And what I'm going to tell it for the restricted model now is to take out age and gender from the unrestricted model, and I'm going to do that using the update function. And so update says, give me some model object. I'm going to give it my unrestricted model object, and then it's got a little bit of a weird syntax that is confusing at first, but isn't too bad period tilde. Period just means keep the same dependent variable as in this model object. Tilde regress this on right, so same dependent variable regress on all the same predictors from that model, except take out age and take out gender. So minus age minus gender just means period minus age minus gender means keep everything the same, but take out these variables. And so update is just going to re run whatever is in here with whatever specification you put here, and then whatever data. So another thing you could use update for, for example, would be to run the exact same model, but with different data. Does that make sense? Okay? But in any case, you play around with that. It's just there's a tip that ever becomes useful to you. But the bigger point is that we're testing this. You know, this, this null, with only a subset of the variables taken out as infinite. And what do we get? We get an F statistic of this time, only 4.5 still reasonably large for an F but much, much smaller than the 25 we got against the intercept only model, and we get a corresponding p value that's still statistically significant at point o5 but it's quite a bit larger than the effect of the zero we got to allow the test. So what does this all mean? It means we do reject the nut hypothesis, that these two coefficients are jointly equal to zero. Now this is sort of the situation people were asking about right? The F test is significant for the joint hypothesis, but if we go back to the table, the coefficient for gender is not statistically significant. So we have a joint statistical significance test for these two together, and that's significant. So they're not jointly equal to zero, even though we can't reject the novel of one of the two that part of it. That's the way it is. Yeah, and that's just a characteristic of, sort of the thing we're doing here, right? We're not testing each individual coefficient and then kind of summing them up somehow. We're testing whether, if you put all those variables in the model at the same time, you improve model fit, that's it. And so another way to think about this would be compared to the intercept only model right? If you have one really strong predictor, and then you add a bunch of noise predictors, you're still going to get a significant app test. But that doesn't mean your noise predictors are all meaningful. So the join test can be significant even when individual coefficient are equal to zero. Questions about that.

Unknown Speaker  37:16  
Maybe one more thing that's worth mentioning. So if you, if you came up in psychology, as I did, you spend a lot of time with a nova in sort of your intro statistics classes. And one way that a Nova is often used historically in psychology is that is what's called an omnibus class. And the idea here was that you might have lots of different variables that you think are important, and the but to sort of keep you from going fishing for statistical significance, like, let's say you have 10 different variables and you're not sure if any of them are important, instead of going and testing each individual variable, which would be 10 separate tests, which you would be worried about false positives. You first do an omnibus test, and only if that's significant Are you allowed to go in and look at each of the individual variable. So that's kind of what's going on here, right? You might say, let's imagine this versus the intercept only models we did before. You might say, the first thing I'm going to do is see if my model is OK. Is my model doing better than the model, and only if that's significant am I going to allow myself to do these individual tests, because I'm worried about going in and doing separate tests for each individual co efficient. So there is this is much more common in experiments and psychology historically, but sometimes you will see f test using that in this way, like as an omnibus test, before you dive in and try to test individual coefficients

Unknown Speaker  38:59  
against each other. All right, so that's it for the f test. There. It's I'm

Unknown Speaker  39:08  
happy to stay here minutes, fine. If this is a question for another time that is that part of like I noticed that a nova, I still see it more often in Psych, and like it's taught in the psych department here. I guess I kind of feel like I'm still unsure about the relationship between a nova and regression. And you know when one or the other?

Unknown Speaker  39:36  
Yeah, I mean, they're they're very tightly related to each other, and so I you know, when you run an ANOVA, you're doing exactly what we just did here. You're trying to see if a set of joint restrictions can be rejected. And it's an analysis of variance, in the sense that it's partitioning the dependent variable into variants that can be attributed to predictors versus error, and it's trying to see if the change in the variance associated with the predictors is more than you would expect. It's doing exactly what I got here. Regression is doing that right, as we just I just showed you, but it's also doing mean estimation, and difference in mean estimation. And so, like with an ANOVA, you might do an omnibus, like, if these were experimental conditions and the base line is the control condition, you might do an omnibus ANOVA. And if that's significant, you would say my experiment did something. Something is happening in my experiment that's different from the control, but what it is, I'm not sure. And then you would dive into each of the individual conditions and see if they had effects for root versus the control. And so the ANOVA is giving you sort of a test of whether your experiment did anything. And then the individual coefficients are telling you the difference between each treatment condition under control. Does that make sense? A note also comes up a lot in hierarchical modeling and multi level modeling. And so if you because you think of hierarchical modeling as variance prediction across different like levels, like individual states, countries, so you'll see it again if you take, like a multi level model, yeah. But just doing ANOVAs is something that is not very common, really. When I first started graduate school, I took an ANOVA class. It was just about very rare. Okay, so the last thing I want to show you in this section is to use a convenient is how to use a convenient function from the car package in R. Car has a lot of convenient functions. We'll see a couple more as we go on. But this linear hypothesis one is especially convenient because it makes it very easy to test different kinds of restrictions on your coefficients that are not as easy to do that. Testing the difference between two coefficients requires you to calculate the square root of the combined variance in the denominator, and that's kind of hard, if that's okay, so you wouldn't want to do it linear. Hypothesis will just do that for you. When we get heteroscedasticity as well, this will become even more convenient, because the variance calculation becomes more complicated in any case for now, right? The basic idea is that you're going to give linear hypothesis a model, and then you're going to tell it what you want to test in symbolic terms. So here I'm doing something that you wouldn't ever do, because you get it from the regression output. Just to show you an example, I'm using the same model we've been talking about right here, and I say I want to get a significance test on the age coefficient. Now you get that from the regression Alpha. You don't need to do this, but if you want to see what are your hypothesis doing, saying, OK, test the null that age is equal to zero, and it's going to give me an F test instead of a instead of a t test, but it's testing the same hypothesis that we're testing here, and actually is 2.525 squared equal to that. Yeah, I wasn't telling you a lot, right? The F is actually the square t6 so this is testing the same hypothesis, and you're getting actually the same exact P value, which makes sense, just the square of the T, and you'll notice there's one degree of freedom on that F test Q is equal to one, just one restriction, and now one restriction is setting age coefficient to zero. So that's a way that you could reproduce, you know, the null hypothesis test in your regression output using linear hypothesis. But why would you do that? Not necessary. Where linear hypothesis shines is when you want to do more complicated things. So we might want to, as we did with that last pep test, right? We might might want to join a test, a joint novel that both age and gender are equal to zero. Now we did that with a no vote, but we had to, you know, use a kind of either run a separate model, or we had to use the Update function here. Linear hypothesis says this in a very sort of straight forward can be way that's easy to see, right? Age is equal to zero and gender is equal to zero, and we get a math test on that, and then the same as what we did, even more complicated, right? Let's say you don't want to test a null against zero. You want to test a null against some specific value, like we talked about last week, right? Let's test whether economic, national economic retrospections, if that coefficient is significantly different from negative point five. Very easy to specify the linear hypothesis. And again, we get enough test. You can get more complicated. We did the difference in two coefficients, beta one minus beta two last week. And that's also very easy to specify as linear hypothesis, and it gives you the same test.

Unknown Speaker  45:34  
Maybe one more example. You can even do even more complicated things, right? You do the coefficient for national retrospections, minus two times the coefficient for personal retrospections. Why you would want to do that? Don't worry about it for now, if you needed to do that for something, you could there so easy, easy to do, relatively complicated you

Unknown Speaker  46:01  
can turn around with that function. So one thing that comes up in goldridge that is very important to remember, and at some point you'll forget this. There's a chance I've forgotten it somewhere in these slides. These tests require that you use the same observations for both the restricted and the unrestricted model. So if you try to run an app test, you estimate an unrestricted model, and then you estimate a restricted model. If the observations used to estimate those two different models are different, these tests are no longer valid. Now you might say, Well, why would that ever be true? It matters when you have missing values in your data, because the default of pretty much every software package you're ever going to encounter is that when you have missing values, and you try to run OLS, it'll just drop any observations that have a missing value on any variable. That's called this wise deletion, since that's the norm, right? When you estimate the unrescripted model versus the restricted model, if you have missing values on any of the variables that are being restricted, you will lose observations going from restricted to unrestricted. If you estimate an intercept only model, and then you estimate a model with five independent variables and some observations have missing values on any of those variables, those observations get dropped. Now you've got a different data set that you're working with relative to the restricted model. So it is very common to lose observations when you move from a more restricted to a less restrictive model. And if that happens, you're test minimum so you need to be aware of that.

Unknown Speaker  47:42  
So common, you just need to be thinking about, what was I Oh, and that so getting back guys that I was going to

Unknown Speaker  47:52  
say something about this, right? So, so here's my unrescripted model, and here's my restricted model, right? There are missing values in these data. So if I move from restricted to unrestricted, I'm going to lose observation. So what I want to do is to estimate the restricted model on the same data as I used for my unrestricted model. And I do that by saying, use the data that's in the model object for my unrestricted model. That's just you get that as a model that will give you all the same variables, but it will use that restricted data, that subset of the data that was used for this estimation. So now I know I'm estimating on the

Unknown Speaker  48:41  
same data that would be the same as if you just cleaned your data set before anything that has missing data along all of the unrestricted list wise, delete every

Unknown Speaker  48:51  
single one. If you did any data omit on a data, a subset of the data that included these variables that would give you a data set that would list wise, would be all and then you wouldn't worry about in fact, you might say that's actually the better way to think about it, if you're structure often will be clear in terms of what's going on, notation that tells people very clearly, I'm doing this for these reasons, Rather than hiding it in a data call.

Unknown Speaker  49:27  
Okay, all right. So I talked a little, I talked a little bit about this, but, you know, I just want to sort of emphasize, once again, sort of the weirdness of null hypothesis testing, and maybe I'll get it from slightly different angle. Think about what the null hypothesis is. Let's just in a standard case, it's that beta k is equal to zero. That's what we get in our regression Alpha. Do you ever believe that that's truly the case? Is that in most situations, we believe that some variable at zero, literally exactly zero, margin multiple times. It seems very unlikely that that's ever true. It might be really, really small or really close to zero, but is it exactly zero? No. Now why does that matter? It matters because if the true value is not exactly equal to zero, then all a null hypothesis test tells you is whether you have a sample size big enough to find that it's not significant, that it's not actually zero. So another way to put that would be, you don't believe that this is ever true, and yet that's what you're testing, and you already know that's not true. So the only thing you're actually ever finding out is whether you got a big enough sample size to detect the fact that you already are aware of that this is false. So that's weird. That's a weird thing to do. And so that's, you know, that's a sort of a mark against null hypothesis testing, I think, sort of a corollary of that is that it's shifting focus away from what matters onto something that we already reject out of hand, and it's shifting attention away from the thing that really matters, which is namely. Number one, what is the nature of the relationship between our independent variable and our dependent variable and number two, how much information do we have about that relationship. That's what we're trying to do. We're trying to estimate the relationship, and we're trying to characterize our uncertainty in that relationship. Null Hypothesis Testing, at best, does that indirectly in a way that's hard to see. So What's better is to focus on substantively interpreting the coefficient and clearly articulating your uncertainty. And what's nice about that, right? Is that you know, with null hypothesis testing, if you have a very small sample size, there's no point even doing this, because there's no way you can reject the null and so you And so basically, just throughout your entire study. But that's silly, right? You have information, you gather data, it contributes to the scientific interface. And so if you focus more on what is the relationship and how much uncertainty Do you have, right? It's always the case that collecting data is useful, because you can say something about the world no matter how much data you have, even if you only have 10 observations that says something, right? You have some information, and you can characterize exactly how much information you have, what it tells us, what it can't tell us, so on and so forth. You know, in sort of a better world, right? You would just upload that data to the internet, and then people would integrate it into the priors for the next study. That's not the way it works. But you know, in practice, even if you are, you know, you're not rejecting the null, you can still say something interesting about like the values of the parameter of interest. So let's take an example. Let's say that this is, you know, possible values of beta, like negative, five, five. Now you're estimating what value. So maybe zero is here, so no hypothesis. So I'm going to, let's say your coefficient estimate is here, and you've got a 95% confidence interval that looks like that. You fail to reject the null because your 95% confidence interval overlaps zero. You fail to reject the null. Should you just ignore this now? Probably not, because you have information, and your information says it is very unlikely that the true value in the population is less than zero. You can even say exactly how unlikely it is. You can say other things that are interesting. You can say, and people are increasingly doing things like this. You could say, you know, I can't really rule out zero with confidence, but I can rule out a lot of values on the negative side. So even if it's the case that I can't rule out, like, really small values on the positive side, I can rule out values, you know, like one in greater with confidence. And that's super helpful, right? If you can tell the world that I can rule out values that are beyond this value with a lot of confidence, then you've learned a lot about whatever it is you're studying, and all hypothesis doesn't like that. So that's just again, my one more one more time pushing you to think when you're reporting results about what is the estimate? What does it mean? And how much information do I actually have about it? How uncertain Am I about it? So about any other thoughts for anyone who has some thoughts.

Unknown Speaker  55:02  
Okay, yes, all right. So so far we've talked about, when we talk about properties of OS, we've been focused on what we call finite sample properties, and these are properties that hold no matter what your sample size is. So we said that if we can assume the conditional mean zero assumption that the expected value of the error term conditional on x is equal to zero, then if we can assume that, then we know that OLS is unbiased, and unbiasedness is a finite sample property. It holds no matter what your sample size is. Similarly, we calculated, oh, yeah, similarly, we said that if we're estimating the we can assume almost elasticity, and if we are estimating the error variance, right? We know the exact sampling distribution for beta and the family that is t, n minus theta. Two those are finite sample properties. They hold no matter what the sample size is. Asymptotic properties are properties that hold, that obtain as n goes to infinity, they're asymptotic in the sense that they obtain as your sample size grows to infinity. So because they only hold in the limit, right by their very nature, we can't say things like, Oh, well, if you only if this distribution, if you have a sample size that's 100 or greater, as a general matter, right? The very nature of this claim is such that you can't say things like that. It's instead we're saying something like we're approximating some unknown sampling distribution with the limiting distribution as n goes to infinity. Put that another way, the asymptotic properties of OLS are going to tell us what the sampling distribution is when n gets to infinity. In the limit, as n goes to infinity. But for any finite sample, we don't actually know what the sampling distribution is. It's unknown. It's not something that you can calculate or prove. So one way to say this is that when we're relying on asymptotic properties, we're approximating the unknown thing that we're actually interested in with the limiting distribution that obtains as n goes to an approximation because we don't know what that is. When we know what happens as n grows. So it's always an approximation. It's always, you know, it's always wrong in some sense, we just hope it's not wrong by much. There are, you know, I don't want to be too strong at this point, but there might be situations where you can do simulations to get a sense of how good your approximation is in a finite sample, but it always requires assumptions, and those assumptions may be contentious, and so you can't really usually rely on OK,

Unknown Speaker  58:25  
all right, so let's talk a little bit more about asy properties in general. We've already talked about this idea of consistency, and again, it's this kind of ugly expression, but all this expression is telling us is that as n goes to infinity, the you know, the probability that our estimate diverges from the true value by any small, you know, negligibly small amount, it goes to zero. And so what that's telling us is that as n goes to infinity, our estimate is our estimator converges on the true value with probability one for any given finite sample, right? There's going to be some difference between the two, and we don't know the exact nature of that difference. We don't know if it's positive, if it's negative, or how big it is. All we can say is that as n goes to infinity, the probability that these two diverge by any negligibly small amount goes to zero. So what does that actually I'm going to actually just show this now, instead of later. Here's an example. So here I'm just estimating a mean. In particular, I'm estimating the mean of a normally, a standard, normal random variable. So it has a mean. It has a true mean of zero and a standard deviation of one. And what I'm doing is trying to estimate that mean. I know what it is, but I'm going to try to estimate it with increasingly large sample sizes. So I'm starting with a sample size of two, all the way up to 1000 in in terms of two, starting with a sample size of two all the way up to a sample size of 1000 and I'm just doing that over and over again, right? I'm taking a sample trying to estimate, taking a sample trying to estimate the mean, but I'm doing it with increasingly large sample sizes. And as you can see, right? What we're seeing is convergence on the true value as the sample size gets larger and larger. So for this very simple case, right? Even at a sample size is 1000 we're getting convergence, pretty strong convergence. What we're doing simple. It doesn't require very large sample size, but at very small sample sizes, you can see this sort of like random scattering around the true value. So that's what we mean by consistency. When this goes to infinity, this would converge on if this converges to that line, that makes

Unknown Speaker  1:00:59  
sense. If that's true, then we can say that beta true, beta is the probability limit of beta hat, the estimator. And we'll abbreviate that sometimes with P Lin. Another way to say that exact same thing is beta hat converges in probability to beta. So the probability limit means beta hat converges in probability to beta. And so the little P under come to the arrow mean converges in probability. There's another type of convergence that we're interested in, as n goes to infinity, and that's convergence and distribution. So we can say that a variable an estimator. Let's reduce in generic terms. So an estimator x hat, some estimator converges in distribution to some other random variable. Call that x, and then again, this looks kind of ugly. Let's see what it says. Because if so, capital F usually stands for the cumulative distribution function of some variable. Some variable. So this is the cumulative distribution function for the estimator X, hat, evaluated at some value u, where u here is just some generic value. It's not the original it's just some generic value. We're saying x hat converges in distribution to x if the cumulative distribution function for X, hat evaluated at u, converges to the cumulative distribution function of x evaluated at u, as n goes to infinity for all u. So what does that mean? It just means that the two cumulative distribution functions are converging on each other. They're becoming the same distribution as you get as n goes to infinity. So the distributions are becoming the same as n goes to infinity. And if that's true, we say that x hat converges in distribution to x. Now what are we going to do with that in practice? But when we get to the little bit further in this lecture, what we're going to do is say that beta hat converges on a normal distribution as n goes to so this is a very generic statement of what convergence in distribution means. But in practice, what we're going to want to do in practice is to not only say that beta hat converges in probability to beta but that beta hat converges in distribution to a normal distribution, and that's going to allow us to do hypothesis tests, even if we can't allow us to do approximate hypothesis test, even if we can't say what the finite sample distribution is that theta is normally distribution, Also it's not going to what it's saying, right? It's saying, it's convert. Are you saying the error term? No, the true value.

Unknown Speaker  1:03:51  
True value of beta is constant. So that's why I'm doing this in generic terms, rather than saying beta hat converges in distribution to beta, because beta is a constant, but the way we're going to use this is it's going to be beta hat converges in distribution to a random variable that is normally distributed with a mean equal to beta and variance covering matrix equal to sigma squared.

Unknown Speaker  1:04:22  
And the key point to take away here is just that what we're going to care what we want to ultimately say is beta hat converges to beta it's consistent in probability and that beta hat is going to the distribution for beta hat converges to a normal distribution as n goes to those are the two things that are going to come away from us, and those are going to be very useful to us. And I think in most cases, that's ultimately what you're probably relying on a lot of the time. Wait more questions.

Unknown Speaker  1:05:00  
Show that. So remember that, and that will give us the condition that allows for consistency as well. So remember that we can write beta hat minus beta as the linear transformation of the error term. So if that looks unfamiliar, go back to our previous lectures where we proved unbiasness for OLS and we showed that we can write this expression here. Theta hat minus beta is equal to this thing on the right. So now what I'm going to do is just multiply each of these things by one. You'll see why in a second, n over n is just one. So I can multiply that by N over N that's multiplied by one. I can do the same thing over here. That's totally fine. Now I want to pull out the numerator n. In both cases, if I pull out the numerator N from an inverse. It's actually one over N. I pull out the numerator n over here, it's just n. Well, now you can see one over n times n is just one. So that goes away, and I'm left with one over n x, transpose x. Inverse. One over n times x transpose u. Now I want to take probability limits. So what I want to know is what the probability limit is of this difference. Why do I want to know that? I want to know if these two, if this converges to this as n goes to infinity. But what I want is for this, the probability limit of this difference to go to zero. So I take the probability limit on the left hand side, and I take it on the right take it on the right hand side as well. Now our question is, well, what's this on the right hand side? So the weak law of large numbers says that if you calculate the sample mean of some transformation of a random variable with a finite mean that converges in probability to the population expected value. Why is that useful thing? Right? We're basically taking this as fixed, but it doesn't even matter, right? What we're interested in is this over here. So what we're doing is taking each variable in x, this is k by m, and we're multiplying it by the error objective, and then we're dividing the whole we're dividing each of those by n. So what we're essentially doing is calculating the sample covariance between each of the x's and the error this calculation is essentially the sample covariance between each of the x's, each of our predictors, and the error, the weak law of large numbers says that this any sample mean of some transformation of a random variable where this is a transformation of the random variable, u any sample mean of that, and that's exactly what we're doing here. A sample mean of this transformation converges in probability to the expected value of that in the population. So that means that this thing over here, n goes to infinity, converges to the expected value of X, transpose u. Now why is that interesting? Well, it's interesting because if we can then assume that the expected value of that in the population is zero, the whole thing is going to drop out, and we're going to get probability limit of beta hat minus beta is equal to zero, and that's exactly what we want. So what this group is saying is that when we get to here, if we can make this assumption, we can get the thing we want. So this is the key assumption now for consistency, that the expected value of X transposed u is equal to zero, you might say, Well, what does that get us? And it turns out that it actually does get us something, because it's a weaker assumption than the assumption we previously made. So we previously to prove unbiasness. If you remember, we had to assume the expected value of u given X is zero. Here we're assuming the expected value of X transpose u is equal to zero. Those are different, and this is weaker than this. In particular, what this says is that we need two different conditions to be true. The expected value of u is zero, and we get that for free with the intercept. And we need the covariance of each independent variable with u to be zero as well for all independent variables. So this is different from this assumption, because instead of saying that any function trying to decrease that, instead of saying this condition says that any function of the x's has to be uncorrelated with u, this says that each individual predictor has to be uncorrelated with u. That's a weaker assumption, because it means that there can be functions of the x that are correlated with U, even just so long as you predict your independence. So it's weaker in that sense, and we get a weaker result. Instead of getting unbiasedness, which is a finite sample property, we get consistency, which is an asymptotic so it's a weaker assumption with a weaker result. But that's useful. So you might say to yourself, that doesn't sound like it's possible. So let me show you an example where it is. But this is also going to show us how rare this is going to be the case. So what I'm going to do here is to draw a 10 observations. I'm going to start with a small sample. I'm going to draw 10 observations of some variable and fall of x from a normal distribution with a mean of zero and a standard deviation of one. I'm then going to create an X matrix that not only includes my first column of ones for my intercept, it includes that variable I just created as my first predictor, but then it also includes x squared as the second predictor. So if we have a mean, zero, standard deviation, one normally distributed random variable, and we take a square, it's basically looked like that quadratic. What that means is that knowing X, and here's x, knowing x doesn't actually tell me much about x. Doesn't tell me anything about x squared and expectation, because if x is one, I don't know. I mean, I'm sorry if you know that. You know x squared is one. You don't know whether it's whether x is negative one or one. So there's no correlation between x and x squared. It's a normally distributed random variable. So here I've got a situation where this is uncorrelated with this, but they're both predictors that I'm going to use to generate y. So I've got a real I've got a situation where the reality that I've constructed is that y is a function of x and x square in equal parts, but x is uncorrelated x squared. And now I can estimate two different models. Let's imagine first that I estimate a population model where I include both x and x squared as predictors that's obviously going to be unbiased, and estimating the true model. What if I estimate a model without x squared instead and so I exclude the second variable that we know is truly in the model. Now you might think, Well, the first thing you think is that bias the estimates, and that's true, right? That's exactly right. Here's my over 10,000 or 100,000 simulations, even with a sample size of 10, right? My unbiased estimator, including both x and x squared as predictors, hits the true value on average. So it's unbiased. If I exclude x squared, I get a ridiculous result. I get a hugely biased estimator of beta one, not even close. That's probably not surprising, right? I included a variable that you expect to be in the model. What is more surprising is that even excluding x squared in the model, OLS is still consistent for beta one. So here this was with the sample size of 10. Here's with a sample size of 1000 my unbiased estimator is still unbiased. That's not surprising, but look at my bias estimator. It's almost hitting the true value for the sample size of 1000 I'm excluding a predictor I know is in there. That predictor is constructed from the other variable, and I'm still getting a consistent estimator of beta one. And the reason that's true, it goes exactly back to this distinction. In when I exclude x squared from the model, I'm violating this assumption, which is more stringent. And so I have a biased estimator, but because x is uncorrelated with x squared, I'm not violating this assumption. And so my estimator is consistent, but bias. Consistent, but bias, and that's exactly what we're seeing, bias, but converging on the true value as n goes to infinity.

Unknown Speaker  1:14:29  
So you're saying that if you are getting a variable, the model

Unknown Speaker  1:14:39  
could be bias. That's yeah, and that's true, and that's exactly what I'm saying, but the condition number of that's true is very restrictive. You have to meet this assumption, otherwise it's inconsistent and bias. So to get consistency, each independent variable, each predictor, has to be uncorrelated with U, and this that's true in this case because x is uncorrelated with u, because x is uncorrelated with x squared. But the only reason X is uncorrelated with x squared is because we have a very particular case here where x squared doesn't tell you anything about x. But there that won't be the normal case is

Unknown Speaker  1:15:24  
Yeah, so, so the the assumption we need for consistency, well, we need to, but this one again, is free. So the key assumption that we need is that each independent variable, x, k is uncorrelated with the error term, and that's true for all the predictors from one to k. So for all k, there's zero covariance between each individual predictor in the error but functions of the predictors can be correlated with the error term. So in the case that I just showed you right, the individual predictor X is uncorrelated the error term, but x squared is in the error term, so a function of x is correlated with the error term that and so this assumption, this more restrictive assumption for unbiasedness, is saying that All functions of predictors must be unrelated here. This assumption is that each individual predictor, but there can be functions that are correlated here and still get consistent. But when that's actually happens where you get bias and consistency, it's pretty narrow set of circumstances, like, I had to construct something fairly unusual to get that result.

Unknown Speaker  1:16:50  
Okay, so, all right, we're going to stop there for over time. Yeah, mull that over. But I'm happy to talk more about that. I will say a couple more things about this thing. Kind of went down a rabbit hole. You kind of just, like, talked about this, and then just kind of like, leave it there. But there's like, a very subtle, okay, so okay, sorry, I'll see you on Thursday and talk.

Transcribed by https://otter.ai
