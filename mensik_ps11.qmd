---
title: "mensik_pset_11"
format: html
editor: visual
---

## NOTE TO SELF: When done, copy into R markdown file (a la instructions) and delete noise

## Data

the `refug` dataset of judge decisions by the Canadian Federal Court of Appeal. The judges were making decisions about people seeking refugee status. The variables are as follows:

-   `decision`: the DV; 2=grants refugee's appeal, 1=denies appeal
-   decision_num is 1 = grants 0 = denies
-   `judge`: name of the judge hearing the case; a factor variable
    -   came in as character
    -   judge_factor is as factor
-   `nation`: nation of origin for the refugee making the appeal; a factor variable
-   `rater`: judgment of the merit of the appeal by an independent rater; a factor variable with two levels, yes or no
    -   rater_factor
-   `language`: language of the case, English or French; a factor variable
-   `location`: location of original claim by refugee, Montreal, Toronto, or other; a factor variable
-   `success`: the logit of the success rate ($\text{ln} \left( \frac{p}{1-p} \right)$) of all cases from refugee's nation of origin (numeric)

Load the data and name it refug.

```{r, echo=FALSE, warning=FALSE}
library(lmtest)
library(sandwich)
library(dplyr)
library(knitr)
library(kableExtra)
library(ggplot2)
library(carData)


getwd()
refug <- read.csv("/Users/kristina/Documents/GitHub/regression-study/refug.csv")

# recode variable decision so that 1 equals grants the appeal and 0 equals denies the appeal (right now is 2 = grants the appeal and 1 = denies)
refug$decision_num <- ifelse(refug$decision == "yes", 2, 1)


# check judge -> recode to factor
refug$judge_factor <- factor(refug$judge)
levels(refug$judge_factor) #ordered alphabetically

#check rater -> factor
refug$rater_factor <- factor(refug$rater)
levels(refug$rater_factor)

#language to factor
refug$language_factor <- factor(refug$language)

#location to factor
refug$location <- factor(refug$location)

#success
unique(refug$success)
class(refug$success)

levels(refug$language_factor)
levels(refug$location)

class(refug$success)

```

## 1a

```{r}

m1 <- lm(decision_num ~ judge_factor + rater_factor + language_factor + location + success, data = refug)

# clustered standard errors by nation

cov_clust <- sandwich::vcovCL(m1, cluster = ~ nation, type = "HC1")

cte <- lmtest::coeftest(m1, vcov. = cov_clust)

# table
coef_data <- data.frame(
  Coefficient = cte[, "Estimate"],
  Standard_Error = cte[, "Std. Error"],
  T_Statistic = cte[, "t value"],
  P_Value = cte[, "Pr(>|t|)"]
)

coef_table <- knitr::kable(
  coef_data,
  format = "html",
  longtable = FALSE,
  caption = "Regression Coeff. Estimates with Clustered Standard Errors"
)

styled_table <- kable_styling(
  coef_table,
  full_width = FALSE,
  bootstrap_options = c("striped", "hover")
)
print(styled_table)
```

judgeHeald - the -0.238 coefficient means that having Judge Heald as opposed to Judge Desjardin is associated with a 0.24 decrease in the likelihood of having a case granted, holding all else constant. This effect is highly significant.

locationToronto - holding all else constant, having a case heard or seeking refugee status in Toronto instead of Montreal is associated with a 11.3 percentage point higher probability of being granted the case compared to a case/seeking refugee status in Montreal. However this is not statistically significant.

success - holding all else constant, a one unit increase in the logit of the success rate of all cases from the refugee's nation of origin is associated with a \~20.8 percentage point increase in the probability that a judge grants the appeal. A unit increase in logit is the natural log of the probability of a nation's success rate – so a country with a 30% success rate's logit would be ln(0.30/0.70) \~ -0.8. This is nonlinear, but roughly corresponds to fairly significant changes. It is statistically significant as well.

## 1b

```{r, echo=FALSE}
#doing this a different way/lab way need to check that i created same se's 

clust_se <- sqrt(diag(cov_clust))

lci <- m1$coefficients - 2*clust_se
uci <- m1$coefficients + 2*clust_se

results <- data.frame(coefficient = coef(m1)[-1],
                      lower.ci = lci[-1],
                      upper.ci = uci[-1])

ggplot(results, aes(x = rownames(results), y = coefficient))+
  geom_point(size = 2) + 
  geom_errorbar(aes(ymin = lower.ci, ymax = upper.ci), width = 0.2) + 
  labs(title = "Coefficient Plot with 95% Confidence Bounds",
       x = "variable",
       y = "Effect on Probability of Case Granted") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  coord_flip() +
  theme_minimal()

```

### 1c.

```{r, echo=FALSE}
#training proc set up
set.seed(123)
m1_training <- caret::trainControl(method = "cv", number = 10)

#train model
m1_10fold <- caret::train(formula(m1), data = refug, method = "lm",
                          trControl = m1_training)
#cross-validation results
print(m1_10fold)

#vs m1 values
m1_sr <- c(summary(m1)["sigma"], summary(m1)["r.squared"])
m1_10f_sr <- c(m1_10fold$results["RMSE"], m1_10fold$results["Rsquared"])
cbind(m1_sr, m1_10f_sr)

training
```

The out of sample (test) MSE is slightly (0.0081) larger than the in-sample MSE/residual standard error, which suggests the model was slightly overfitted (the DV is a 0-1 variable, so 0.41 means predictions are off by 41 percentage points on average – less stark when compared to 40 percentage points).

The out of sample R2 drops by about 0.034 (from explaining about 25% of the variance to 21.5%) - so 3.4 percentage points of explained variance disappear when moving to new data. This is not so severe as to suggest the model doesn't have predictive capacities, or that the model is severely biased.

There are differences because the model is created to fit (OLS) the observed data. Held-out data doesn't have the noise on the in sample data.

### 1d. - Fixed effects

new model uses fixed effects for 'nation' to account for all nation-level variance in 'decision' (HINT: what other changes do you need to make to estimate this model?)

-FE adjust for possibility of unobserved heterogeneity X groups by controlling for group level ommitted/confounding variables

-each group (except one) gets a binary variable in the estimated model

indicator for group j gives the difference in average y hat i comparing group j to the excluded (baseline) group

removing what is shared across observations within groups makes the errors independent and SEs unbiased

```{r, echo=FALSE}

#dropping success bc success is constant with nation

m1_fe <- lm(decision_num ~ judge_factor + rater_factor + language_factor + location + as.factor(nation),data = refug)

```

### 1e. - 10-fold

```{r, echo=FALSE}
#training proc set up
m1fe_training <- caret::trainControl(method = "cv", number = 10)

#train model
m1fe_10fold <- caret::train(formula(m1_fe), data = refug, method = "lm",
                          trControl = m1fe_training)
#cross-validation results
print(m1fe_10fold)

#vs m1 values
fe_sr <- c(summary(m1_fe)["sigma"], summary(m1_fe)["r.squared"])
fe_10f_sr <- c(m1fe_10fold$results["RMSE"], m1fe_10fold$results["Rsquared"])

cbind(m1_sr, m1_10f_sr, fe_sr, fe_10f_sr)
cbind(m1_sr, fe_sr)
cbind(m1_10f_sr, fe_10f_sr)
summary(m1_fe)

(m1_sr$sigma) - (fe_sr$sigma)

```

First, both models have lower $R^2$ and higher error under 10-fold cross-validation because out-of-sample tests do not “reward"the original models for being fitted to the data's particular random noise.

Nation fixed effects modestly improves in-sample fit but increase variance, which may explain its bigger to cross validation drop in the out of sample. Moreover, the fixed effects out of sample does not account for the fixed effects models' training of data to nation-specific noise - which also explains the \~0.3 to 0.2 drop in R-squared/explained variance, and increase in prediction error (0.4 to 0.41).

Still, the fixed effects models' higher out-of-sample $R^2$ suggests some predictive benefits to accounting for nation-level differences, and underscoring the bias-variance tradeoff. In m1, success it imposes a functional form on how 'nation' matters/says how a unit fares in relation to others in their country, or allows m1 control for country rates. Compared to the fixed effects model, this somewhat obscures country-level variation. The fixed effects model (nation as a factor makes each nation (except for the baseline) a parameter and gives each its own baseline), which allows the model to more/less absorb nation-level differences.

As with the tradeoff, adding country level parameters also muddy interpretability at the cross-national level, but it is in this case potentially useful (depending on your theory/aims) to see by-country variation.

# 2a.

```{r}



```
