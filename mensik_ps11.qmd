---
title: "mensik_pset_11"
format: html
editor: visual
---

## NOTE TO SELF: When done, copy into R markdown file (a la instructions) and delete noise

## Data

the `refug` dataset of judge decisions by the Canadian Federal Court of Appeal. The judges were making decisions about people seeking refugee status. The variables are as follows:

-   `decision`: the DV; 2=grants refugee's appeal, 1=denies appeal
-   decision_num is 1 = grants 0 = denies
-   `judge`: name of the judge hearing the case; a factor variable
    -   came in as character
    -   judge_factor is as factor
-   `nation`: nation of origin for the refugee making the appeal; a factor variable
-   `rater`: judgment of the merit of the appeal by an independent rater; a factor variable with two levels, yes or no
    -   rater_factor
-   `language`: language of the case, English or French; a factor variable
-   `location`: location of original claim by refugee, Montreal, Toronto, or other; a factor variable
-   `success`: the logit of the success rate ($\text{ln} \left( \frac{p}{1-p} \right)$) of all cases from refugee's nation of origin (numeric)

Load the data and name it refug.

```{r, echo=FALSE, warning=FALSE}
library(lmtest)
library(sandwich)
library(dplyr)
library(knitr)
library(kableExtra)
library(ggplot2)
library(carData)


getwd()
refug <- read.csv("/Users/kristina/Documents/GitHub/regression-study/refug.csv")

# recode variable decision so that 1 equals grants the appeal and 0 equals denies the appeal (right now is 2 = grants the appeal and 1 = denies)
refug$decision_num <- ifelse(refug$decision == "yes", 2, 1)


# check judge -> recode to factor
refug$judge_factor <- factor(refug$judge)
levels(refug$judge_factor) #ordered alphabetically

#check rater -> factor
refug$rater_factor <- factor(refug$rater)
levels(refug$rater_factor)

#language to factor
refug$language_factor <- factor(refug$language)

#location to factor
refug$location <- factor(refug$location)

#success
unique(refug$success)
class(refug$success)

levels(refug$language_factor)
levels(refug$location)

class(refug$success)

```

## 1a

```{r}

m1 <- lm(decision_num ~ judge_factor + rater_factor + language_factor + location + success, data = refug)

# clustered standard errors by nation

cov_clust <- sandwich::vcovCL(m1, cluster = ~ nation, type = "HC1")

cte <- lmtest::coeftest(m1, vcov. = cov_clust)

# table
coef_data <- data.frame(
  Coefficient = cte[, "Estimate"],
  Standard_Error = cte[, "Std. Error"],
  T_Statistic = cte[, "t value"],
  P_Value = cte[, "Pr(>|t|)"]
)

coef_table <- knitr::kable(
  coef_data,
  format = "html",
  longtable = FALSE,
  caption = "Regression Coeff. Estimates with Clustered Standard Errors"
)

styled_table <- kable_styling(
  coef_table,
  full_width = FALSE,
  bootstrap_options = c("striped", "hover")
)
print(styled_table)
```

judgeHeald - the -0.238 coefficient means that having Judge Heald as opposed to Judge Desjardin is associated with a 0.24 decrease in the likelihood of having a case granted, holding all else constant. This effect is highly significant.

locationToronto - holding all else constant, having a case heard or seeking refugee status in Toronto instead of Montreal is associated with a 11.3 percentage point higher probability of being granted the case compared to a case/seeking refugee status in Montreal. However this is not statistically significant.

success - holding all else constant, a one unit increase in the logit of the success rate of all cases from the refugee's nation of origin is associated with a \~20.8 percentage point increase in the probability that a judge grants the appeal. A unit increase in logit is the natural log of the probability of a nation's success rate – so a country with a 30% success rate's logit would be ln(0.30/0.70) \~ -0.8. This is nonlinear, but roughly corresponds to fairly significant changes. It is statistically significant as well.

## 1b

```{r, echo=FALSE}
#doing this a different way/lab way need to check that i created same se's 

clust_se <- sqrt(diag(cov_clust))

lci <- m1$coefficients - 2*clust_se
uci <- m1$coefficients + 2*clust_se

results <- data.frame(coefficient = coef(m1)[-1],
                      lower.ci = lci[-1],
                      upper.ci = uci[-1])

ggplot(results, aes(x = rownames(results), y = coefficient))+
  geom_point(size = 2) + 
  geom_errorbar(aes(ymin = lower.ci, ymax = upper.ci), width = 0.2) + 
  labs(title = "Coefficient Plot with 95% Confidence Bounds",
       x = "variable",
       y = "Effect on Probability of Case Granted") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  coord_flip() +
  theme_minimal()

```

### 1c.

```{r, echo=FALSE}
#training proc set up
set.seed(123)
m1_training <- caret::trainControl(method = "cv", number = 10)

#train model
m1_10fold <- caret::train(formula(m1), data = refug, method = "lm",
                          trControl = m1_training)
#cross-validation results
print(m1_10fold)

#vs m1 values
m1_sr <- c(summary(m1)["sigma"], summary(m1)["r.squared"])
m1_10f_sr <- c(m1_10fold$results["RMSE"], m1_10fold$results["Rsquared"])
cbind(m1_sr, m1_10f_sr)

training
```

The out of sample (test) MSE is slightly (0.0081) larger than the in-sample MSE/residual standard error, which suggests the model was slightly overfitted (the DV is a 0-1 variable, so 0.41 means predictions are off by 41 percentage points on average – less stark when compared to 40 percentage points).

The out of sample R2 drops by about 0.034 (from explaining about 25% of the variance to 21.5%) - so 3.4 percentage points of explained variance disappear when moving to new data. This is not so severe as to suggest the model doesn't have predictive capacities, or that the model is severely biased.

There are differences because the model is created to fit (OLS) the observed data. Held-out data doesn't have the noise on the in sample data.

### 1d. - Fixed effects

new model uses fixed effects for 'nation' to account for all nation-level variance in 'decision' (HINT: what other changes do you need to make to estimate this model?)

-FE adjust for possibility of unobserved heterogeneity X groups by controlling for group level ommitted/confounding variables

-each group (except one) gets a binary variable in the estimated model

indicator for group j gives the difference in average y hat i comparing group j to the excluded (baseline) group

removing what is shared across observations within groups makes the errors independent and SEs unbiased

```{r, echo=FALSE}

#dropping success bc success is constant with nation

m1_fe <- lm(decision_num ~ judge_factor + rater_factor + language_factor + location + as.factor(nation),data = refug)

```

### 1e. - 10-fold

```{r, echo=FALSE}
#training proc set up
m1fe_training <- caret::trainControl(method = "cv", number = 10)

#train model
m1fe_10fold <- caret::train(formula(m1_fe), data = refug, method = "lm",
                          trControl = m1fe_training)
#cross-validation results
print(m1fe_10fold)

#vs m1 values
fe_sr <- c(summary(m1_fe)["sigma"], summary(m1_fe)["r.squared"])
fe_10f_sr <- c(m1fe_10fold$results["RMSE"], m1fe_10fold$results["Rsquared"])

cbind(m1_sr, m1_10f_sr, fe_sr, fe_10f_sr)
cbind(m1_sr, fe_sr)
cbind(m1_10f_sr, fe_10f_sr)
summary(m1_fe)

(m1_sr$sigma) - (fe_sr$sigma)

```

First, both models have lower $R^2$ and higher error under 10-fold cross-validation because out-of-sample tests do not “reward"the original models for being fitted to the data's particular random noise.

Nation fixed effects modestly improves in-sample fit but increase variance, which may explain its bigger to cross validation drop in the out of sample. Moreover, the fixed effects out of sample does not account for the fixed effects models' training of data to nation-specific noise - which also explains the \~0.3 to 0.2 drop in R-squared/explained variance, and increase in prediction error (0.4 to 0.41).

Still, the fixed effects models' higher out-of-sample $R^2$ suggests some predictive benefits to accounting for nation-level differences, and underscoring the bias-variance tradeoff. In m1, success it imposes a functional form on how 'nation' matters/says how a unit fares in relation to others in their country, or allows m1 control for country rates. Compared to the fixed effects model, this somewhat obscures country-level variation. The fixed effects model (nation as a factor makes each nation (except for the baseline) a parameter and gives each its own baseline), which allows the model to more/less absorb nation-level differences.

As with the tradeoff, adding country level parameters also muddy interpretability at the cross-national level, but it is in this case potentially useful (depending on your theory/aims) to see by-country variation.

# 2

Let's turn to new data. use the `Baumann` data from the `carData` package. They are data from an experiment with 66 undergraduate subjects who were randomly assigned to one of three treatment conditions: (1) `Basal`: a traditional method of teaching; (2) `DRTA`: an innovative method of teaching; or (3) `Strat`: a different innovative teaching method. The variables are as follows:

-   `group`: student treatment assignment, a factor variable
-   `pretest.1`: student pre-treatment performance scores on measure 1
-   `pretest.2`: student pre-treatment performance scores on measure 1
-   `post.test.1`: student post-treatment performance scores on measure 1
-   `post.test.2`: student post-treatment performance scores on measure 2
-   `post.test.3`: student post-treatment performance scores on measure 3

Load the data and name it `baum`.

```{r}
library(carData)
data("Baumann")
```

# 2a.

You will note that there are two pre-treatment measures of student performance and these are modestly correlated. Presumably, each of these is measured with error. Create one pre-treatment performance measure by averaging the two individual measures, and call it `pretest`.

```{r}
summary(Baumann$group)
view(Baumann)
Baumann$pretest <- (Baumann$pretest.1 + Baumann$pretest.2) / 2

summary(Baumann$post.test.3)
```

## **2b.**

There are also three post-test measures but their correlations are mixed. You decide to treat them as three distinct measures of performance and examine the treatment effects on each separately. Estimate a separate OLS regression for each of the three post-treatment measures. Include the experimental treatment factor variable in the model (traditional method as baseline) and also include your new `pretest` variable as a control.

```{r}
levels(Baumann$group)
m_post1 <- lm(post.test.1 ~ group + pretest, data = Baumann)

m_post2 <- lm(post.test.2 ~ group + pretest, data = Baumann)

m_post3 <- lm(post.test.3 ~ group + pretest, data = Baumann)

aggregate(pretest_composite ~ group, data=Baumann, mean)
```

## 2c. 

Explain why including the `pretest` measure might be beneficial even though the experimental treatments are known to be exogenous via random assignment.

Including the pretest measure might be beneficial because (1) while randomization balances all pre-treatment covariates in expectation, covariates (observed and unobserved) on average, they might be imbalanced by chance which would skew internal validity and inflate variance of the estimator.

the analysis involves multiple treatment arms (Basal, DRTA, Strata) across multiple outcomes, and repeated independent trials have family-wise error rates that are elevated. This increases the rate of false positives. Formal adjustments (alpha) might reduce power in this small (n = 66) sample. Moreover, since folks' post-test scores will vary due to prior ability, and while we're not worried about endogeneity, we can reduce residual variance assuming the control accounts for individual variability (increase the model's explained variance/R\^2).

## 2d.

Discuss the statistical significance of the treatment effects across the three dependent variables. What issues might arise here with respect to using p-values in the regression output combined with $\alpha=0.05$ to test treatment effects for all three variables?

Assignment to the DRTA treatment instead of traditional teaching is associated with a 3.55 point higher score on the first test (highly statistically significant \>0.008), 3.1894 point higher score on the second (slightly less but still significant (0.0084), and 5.8372 on the third test (also significant at 0.00339).

Assignment to the Strat treatment condition instead of traditional teaching is associated with a 1.9 point higher scores on the first post test (significant at 0.0122), 3.0694 (highly significant at near 0) on the second test, and 3.5 point higher scores on the third, though that is not significant (0.07).

The pretest coefficient tells us that a one unit increase in pretest scores are associated with a 0.9735 point improvement in test 1 (not significant), a 0.2987 point improvement in test 2 (significant), and 0.3255 point better scores in test 3 (not significant).

However, given this is two different treatment contrasts per each three separate regression (6 hypothesis tests) the probability of at least one false positive at $\alpha = 0.05$ is 1-(1-0.05)\^6 \~ 0.26, this setup inflates the likelihood of type 1 errors and also, at the model-by-model level, sort of obscures the risk of error. Moreover, with only 22 students per treatment group, statistical power is pretty limited.

## 2e. Discuss the substantive meaning of each coefficient you decide is statistically significant.

While I can refer to max and min scores on the pre and outcome tests (1-15, 1-13, 1-57), one barrier to substantive interpretation is not knowing how many points these tests are out.

By the $\alpha = 0.05$ rule, statistically significant coefficients are discussed below.

-   3.55 coef on groupDRTA for model 1 – a 3.55 point improvement on a test where student scores range from 1 to 15 is seems substantively significant – like a meaningful reflection of student learning.

-   1.9095 coef on groupStrat for model 1 –

-   3.0694 coef on group Strat on model 2

-   5.8372 coef on group DRTA on model 3

```{r}
alpha <- 1 - (1-0.05)^(1/6)

coef()
```

**Statistical vs Substantive Significance:** We should clarify this important distinction for the student:

-   *Substantive (or practical) significance* asks: is the size of the effect big enough to matter in real terms – in the context of education, is a difference of that many points meaningful in students’ learning? For instance, Strat’s +3.1 on post2 was highly significant statistically and is also clearly substantive (assuming the test is on a scale where 3 points is a considerable jump, like going from a score of 5 to 8 out of 15). On the other hand, suppose we had a huge sample and we found a tiny effect (like +0.5 points with p \< 0.001) – that could be statistically significant but not educationally important. In our case, most effects we see, if significant, are a few points – which in these tests appears to be a notable difference given the scale (especially since each group only had \~22 students, a difference of 3-5 points is quite noticeable in group averages).

We should also interpret the meaning of Basal vs the others: Basal being the traditional curriculum, these results imply the new methods can produce higher comprehension scores. One might consider, for instance, if we wanted to recommend a method: DRTA seems consistently effective, Strat is effective for certain outcomes. One could also test DRTA vs Strat by comparing their coefficients (though that wasn’t explicitly asked, it’s a possible additional insight: e.g., on post-test1, DRTA outperforms Strat by \~1.7 points, etc.). However, since the question likely expects just Basal vs others, we stick to that.

a.  Create a plot that shows the estimated treatment effects of the innovative methods relative to the traditional one for each dependent variable. Include confidence bounds for each treatment effect. Make your own choices about how best to present these results (e.g., number of panels, labeling choices, reference lines, etc.). Pretend this is going into a paper to be submitted to a journal - make it look good!

Finally, we want to create a figure to professionally illustrate the effects of the treatments. A good approach is to plot the **estimated treatment effects with their confidence intervals** for each outcome. This could be a side-by-side coefficient plot specifically for the group comparisons.

One idea: Have a plot for each post-test outcome (maybe as separate facets or panels), showing the difference from Basal for DRTA and Strat with error bars. Alternatively, a single combined plot could use the x-axis for effect size and list each (Outcome, Group) combination on the y-axis.

We’ll demonstrate using **ggplot2** again. First, let’s compile the effect estimates and CIs from our three models:

```{r}

library(broom)
# Extract tidy results with conf. intervals for each model
tidy1 <- tidy(model_post1, conf.int=TRUE)
tidy2 <- tidy(model_post2, conf.int=TRUE)
tidy3 <- tidy(model_post3, conf.int=TRUE)

# Add an identifier for outcome
tidy1$outcome <- "Post-test 1"
tidy2$outcome <- "Post-test 2"
tidy3$outcome <- "Post-test 3"

# Combine into one data frame
tidy_all <- rbind(tidy1, tidy2, tidy3)

# Filter only the treatment effects (exclude intercept and pretest)
effects_plot <- tidy_all[tidy_all$term %in% c("groupDRTA", "groupStrat"), ]

# Rename terms for better labels
effects_plot$term <- factor(effects_plot$term, levels=c("groupDRTA","groupStrat"),
                            labels=c("DRTA vs Basal","Strat vs Basal"))


```

Now `effects_plot` contains rows for DRTA vs Basal and Strat vs Basal for each outcome, with columns for estimate, conf.low, conf.high, and outcome.

We can plot this using `geom_pointrange` or `geom_errorbar` + `geom_point`:

```{r}
ggplot(effects_plot, aes(x = estimate, y = term)) +
  geom_point(size=3) +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height=0.2) +
  facet_wrap(~ outcome) +
  geom_vline(xintercept=0, linetype="dashed", color="gray") +
  labs(title="Treatment Effects on Post-test Outcomes (95% CI)",
       x="Score Difference (Treatment - Basal)",
       y="Comparison") +
  theme_bw()

```

In this plot:

-   We use horizontal error bars (`geom_errorbarh`) since we put the estimate on the x-axis (effect size) and the comparison on the y-axis.

    We facet by outcome, so we get three panels (Post-test 1, 2, 3). In each panel, we’ll see two points: “DRTA vs Basal” and “Strat vs Basal”.

    The x=0 line helps see if an effect crosses zero (not significant).

    We labeled the x-axis in a self-explanatory way (“Score Difference (Treatment - Basal)”).

    We chose a clean theme (`theme_bw`) for a professional look. One can further tweak text size, etc., if needed.

The key learning points for the student from Problem 2 are:

-   How to create and use a composite control variable.

-   How to interpret regression coefficients in an experiment (where they have a causal meaning due to randomization).

-   Why controlling for baseline improves the analysis (precision, power).

-   The importance of distinguishing “significant” from “meaningful” results

-   How to effectively present regression findings (clear tables and especially a nice plot of effects).
