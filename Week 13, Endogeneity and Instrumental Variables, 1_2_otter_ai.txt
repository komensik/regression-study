Speaker 1  0:00  
Length squares as an alternative OLS as an estimator, which, under some conditions, at least, is more efficient than OLS, so potentially prefer. Okay, so we talked about leverage, because leverage is used directly in the HC estimators, HC two and HC three estimators for our standard errors, or robust standard error. Now we're going to think about leverage in the context of a concept called influence. And so an influential observation is one that if you remove it from the estimation, it changes the estimates a lot. So an influential observation is one where, if you compare your estimates, clearly, without that observation, you get a pretty big difference. Another way to put that would be inflation. Influential observation is one that matters. In that sense, impactful. Inclusion is impactful. High leverage points in particular, are often and again, it has to do with combination of things. The first is that, by definition, high leverage observations are outside, sort of the core of the independent variable space. If you look at the multivariate distribution of the independent variables, a variable with high leverage is one that is defined by being far away from the centroid of that distribution. And so that's one reason, right, as we talked about, like the analogy to torque, if you push on something far away from the center of rotation, you get more movement. And so high level points are influential because they pull the regression line towards them. But it's also, it's not only a function of that, it's also a function in the OLS context of the fact that you're minimizing the sum of squared errors. So think about what that does. Right? It means that, you know, as you get further away from the center of the space, the penalty for going further out is amplified. You have increasing marginal returns distance with square. So another way to put that is OLS is really concerned with things that are very far away because of that square loss function, and so that amplifies the problem of leverage. It means that when you're really far away, it's going to exert a lot of extra because of that quadratic loss so here's an example. I just created some data, and I the way I did this was I drew all the points that are open circles are drawn from a particular distribution, a normal distribution, and I sort of made up a bi variate normal distribution, so there's some relationship between x1 on the x axis and x2 on the Y axis. And all those observations are drawn from the same distribution, same binary. This one point up here, I just drew from a very I drew from a different distribution so that it would be an outlier. It's generated by a different process. So in this case, in particular, it's generated by a process that's very far away from this process. So this clearly has has high leverage in the sense that we can find it. And you can see what happens if you estimate the regression line both including it and excluding it. The blue line is including it and the red line is excluding it. And you can see what the leverage point is. High leverage point is doing is it's dragging the regression line towards it, right? So there's a huge difference in the estimate for beta when you include this than when you ask. So that's what I mean by influence, right? It's dragging the regression line. And it can do that because it's a high leverage point. So okay, so you might want some ways of trying to figure out if you've got points like this in your data, and if so, at least knowing that they're there. And then we can talk about what maybe you do with them. So one, one useful statistic is called Student size residual, which I talked briefly about before. And the idea of student types residuals is to estimate the residual for a given observation from a model that is estimated without that observation in the data today. Why would we want to do that? Well, the whole point right is that if it's a high leverage, if an influential point, it's dragging the regression line toward it and reducing the residual. So you want to see what the residual is without that observation. So that's what a student size residual is, right? Each of these residuals for each of the observations is the residual for the model without that observation. And so if we take a look at so Car, car has some nice functions here. If you use outlier test on your LM call, it'll give you, I think it just gives you, like the ones where you get significant P value here, but you can change that in the options, and you'll see that this 25th observation is that observation, and you see that the R student value is 6.38 and you can think of that like a t value. So that's a residual. That's the student size residual, but if it was drawn from the same distribution as the other observations, it would be a t value for that distribution. And so a t value of six, something is extremely high. We don't expect values of t for a typical t distribution to be much further than plus or minus two. That would be a typical day, that would be most of the distribution. So going out like six standard deviations is an extreme, extreme value. So this student size t value is very large, and what the p value is telling you is, is this T value so large that the probability we would have gotten that if it was a member of this data generating process is really small. Is this an outlier or generated by some different process in the sense that it would be really unlikely to be generated by the same distribution? So this is just a key value for a key distribution on the student size residual Bonferroni thing is, use the Bonferroni correction, which is if you if you ever heard anything about multiple hypothesis testing, the idea is that you're doing a hypothesis test for every one of these observations. And if you do like 100 hypothesis tests, you know you're going to randomly get one out of every 20 significant even going on. And so this just divides the p value by the number of observation. So this is one thing you could do, right? You could just run that wire test something and see if there's any particularly large student types.

Speaker 2  7:19  
Student size. That's not the right

Speaker 1  7:27  
way to think about it. Right you kind of calculate a student size residual for every observation box, and you're just going to get a P value that sees that basically tells you is this unlikely to have occurred if it's part of this distribution. So it can do it for every it does it for every observation of the data. The I think there's just a default option that only lists a certain number or uses some criteria to tell you which ones are worth looking at. But that can

Speaker 3  7:56  
be changed. So if you have, like, very highly variable data. There's a high variance with, would they cancel out and with the with the details, and not necessarily be high,

Speaker 1  8:13  
like, you get exactly that. Okay, good. Because, if you have right, this is telling you, like, given, you know, sort of, is it unexpected, given sort of the amount of data you have, it's not. It's, in that sense, adjusting to the variance. Okay, so that's, that's, that's fine. There's other measures of influence that are very commonly used, and these are the two most common, probably the most common. Both of these are measures that essentially estimate the model with and without an observation, and compare the predicted values and see to what extent there they differ. Like you get very different predictive values for an observation when you include that observation versus x, and they just normalize it in different ways, but they're both basically doing the same thing, slightly normalized differently. And so the only other thing to notice is that code D is always positive, and this DF fits function is both positive or negative, and so you just need to take the absolute value of the F fit. And what I'm giving you here are like sort of standard rules of thumb that people have offered as things that or as criteria you could use to figure out if something is worth looking into. Always be careful with that kind of thing, because they're kind of arbitrary. I mean, they are arbitrary. But for quick D, sometimes people say greater than one or greater than four over n, where n is the number of observations. For D outputs, it's two times the square root of k over m, k is the number of predictors. And so you can, you can use those if you want, or, you know, you can do whatever you want, but these are, you know, the sort of recommended values. And so what I'm just going to show you here is that these things are all super easy to calculate in R. Both cooks D and D, of bits are our base r functions. And so you could do something you create, like a really quick table for all your observations that basically says, like, according to these different criteria, is this a problematic observation or a highly influential point? And so what I'm doing is just calculating, for every observation, the cook's distance, cooks D, folks that distance on the model object, I'm calculating DF bits, which, again, is a base r function. And then I'm also calculating leverage values. So hat values are the leverage values, and as I talked about last time, I'm dividing that by what you would expect if everything is equal leverage, to give it a sense of whether it's more higher leverage than you would expect if everything was equal, and if you remember, there's sort of like different possible criteria here, like greater than two, greater than three, are things that people say and then The student has t values that we get from

Unknown Speaker  11:24  
this car

Speaker 1  11:28  
actually, our student is a base r function. Yeah. So the advantage of this outlier test is it gives you the significance test. But I think our student is just a base card package, so you don't even have to use card package at all. In any case, I'm throwing all that into a table. And, you know? And so what am I doing? Well, here's the value of code d, and this is zero or one that this meet? The rule of thumb, is it greater than four over m, here's the absolute value of d if this is it greater than threshold zero or one? Leverage over equality, we can look and see if it's greater than two or three, and then our student is a t value. We can see very large T value relative to t distribution. And if we do all that, let's go down to the 25th observation on the data. N indeed, it exceeds the threshold for Cook's distance, it exceeds the threshold for DF bits, it exceeds the three threshold for leverage over quality, and it's got a super large, super nice key value. And so it seems clear, right? These, all these different ways of thinking about influence very clearly identify the observation that I forced to be an outlier. And so that's good at the rate that makes sense. It also interestingly. So. So if you look here, observation number nine is the only other one that looks problematic. It passes the test for cooks, D it passes the test for DS. Fits like right on the border of three for leverage, and it's got a t value that's large, but not like crazy of like that could happen. So it's not definitive, and that's super interesting, because I didn't actually intend that at all. This is drawn from the same distribution, actually. So this is just a purely random happens dance that I didn't plan but it's a great example, because what the question then becomes, what do you do? Right? So this seems, this is obviously problematic, and the first thing you would think for that probably it's like, I coded it. There's some kind of coding error, but maybe not, right? But this one is, like, right on the border, and you have to really be worried about what you should do with that? There's some evidence that it's problematic or influential, but it's not obviously right outside the bounds of what would be reasonable. So that's a great example of how tricky this is. And so what do you do in these situations? Let's say that this is your data, and you've got these results, and you have to make some sort of decision. The first thing you might think is, well, I should just get rid of those two influential well, maybe, like in this case, if you got rid of that observation, you'd be getting rid of totally, I mean, you wouldn't know this, but you'd be getting rid of an observation that was drawn in the same data generated process, and this whole thing would be biased. That would be a really bad choice to make, probably, in this case, it's probably some kind of coding error, or at least it's a different data generating process. And it would be the right thing to do, probably, to get rid of that, or at least estimate it, both with and the now, but you can't know that. I don't really know that because I generate these data. So one thing you know you can always think about doing is getting rid of high leverage observations. If you do that, you typically need to make a pretty strong argument for it, and in almost all cases, it would be better off to estimate it, both with and without the observation or observation, and just be very transparent about the fact that you did that and the fact that there's some change. And then you make the best argument you can for whichever one you think is right, and say, like, I really think this is a problematic observation that shouldn't be in my data for these reasons, I'm going to defend that, but I understand there might be different opinion. Take a look at these other results with the observation, and then use you know, things fall where they may

Speaker 4  15:33  
where in the analysis process. Do we do exercise? Do we do it after we estimate the model? We have to do it

Speaker 1  15:41  
after you estimate the model, because all these statistics are constructed based on an estimated model,

Unknown Speaker  15:50  
except for leverage, leverage.

Speaker 4  15:53  
Then can you argue like, can someone look at that and argue that like, oh, you estimate a model and you didn't get what you were looking for, so you did all of these things, and you remove some observations to get

Speaker 1  16:08  
they can absolutely, they can absolutely say that. And the tricky part is how you can defend yourself against that, assuming you didn't do that. So don't do that, but assuming you didn't do that. You need to. That's why it's so important to be so transparent about it and to have a strong argument for your favorite. I mean, ideally you're doing something where you don't care what the answer is, and so you know, you just want to know, right? It's this or that. And so you're really just making the strong argument you can for whichever position you make right? And you don't have any skin in the game, that's ideal. In practice, that's not always the case. And if you end up with your favorite conclusion using this approach, you're going to get pushback, right. And so you could have a really strong argument. You need to be transparent, you need to be, I want to say humble, read about it. Say, like, you know, be very open about the fact that there is an alternative possibility here. There's just lots of uncertainty. The other thing you can potentially do is to integrate diagnostics into your pre registration, if you do that. And so, like, I'm saying lots of things, but one thing that people all the time do now in pre register experiments is because, like, the survey data quality can be quite bad, they pre register getting rid of certain observations based on some predefined criteria of like, what constitutes a good or bad observation. So like, if it takes like, 30 seconds to complete a 15 survey. I pre register, I'm going to remove this even that may have problems, but if you pre register it, they know you're not searching for the data to figure to get results that you want. You can do something similar with this. You could say I'm going to pre register that I'm going to get rid of observations with certain statistic values that indicate they're outliers, or I'm going to pre register that if I find any observations that have these characteristics, I'm going to estimate both with and without. You can do that as well, but almost no one does that. I haven't seen anyone ever do that. So that is definitely something you can do. But if you don't do that, you just have to, really have to defend it.

Speaker 5  18:31  
But in terms of pre registration, at least I'm quite unfamiliar, is that, just like a document that you're filling out on your own? Are you actively submitting it to an individual

Speaker 1  18:44  
person? It's kind of in between. So you are ideally, I mean, you can do whatever you want, but for it to have any kind of impact on the world, you need to submit it to a repository that basically guards against manipulation. So you're like, you know, you're odys, yes, here you're trying to change yourself to the math. And so you need some external institution to do that. And so the like osf.com or OSF that, or Open Science Framework, as is one of the most popular versions of this. And so what you do is, you go on OSF, start a new project, and there's a template that you used pre register your study, and has all these boxes that you fill in, like, you know, what's your randomization scheme? What are your independent variables? And you just provide as much detail as you can. I, since you know this is popular, I'm doing this. I upload my Qualtrics file, so I'll upload my survey design into the pre registration. And then right before you collect your data, you hit, you know, go, and it publishes it, and it freezes it, and then any changes to it are very prominent, and really pre register, and then it's final, and it's public. And so, anyone and so, well, you know, it's like you you're doing it on your own, but you're publishing it in a public way, and most of the time you're relying on a template that is standard. Okay, so, yeah. So what else you know this is inevitably going to be, you know, things probably gonna think about most often, and again, the best advice is just to say what you really think as clearly as you can. There are other options, and they're beyond the scope of this. Course. One is to change the loss function. So, as I said right one of the problems with high leverage transformations in OLS is that we use a quadratic loss function, and that amplifies the problem. And so there's various robust estimators. One is, is called least absolute deviation, instead of least squares. And so instead of using a quadratic loss, use an absolute value loss. So that's something you can consider. And again, the thing you would want to do is say, like, you know, I'm worried about this observation. I'm going to use a robust estimator, robust outliers, and I'm going to report both that and my original differ, then I'll make an argument from one to the other reader. Another kind of robust estimator is to change the distributional assumptions about the defect variable. So you can think of OLS in most cases as if we're using like a normal distribution. That's not exactly right, but if you think of it that way, a lot of times we're thinking about it as if the normal you could instead explicitly model the DV as a T distributed variable with low degrees of freedom. And if you do that, you have a fat tail distribution. Now, a fat tail distribution expects that there's going to be large deviations from the central 10. That's what it means to be fat tail. You get outliers a larger percent of the time, and because the T expects that, it doesn't give us high weight to those observations. So this is like something like this. That was a different approach, basically down waiting large deviations by making them more expected. There's also a whole protocol mixture modeling, where instead of saying that everything is identically and independently distributed, you say that there's at least two two or more data generating process, so, like A simple example of a mixture model, no,

Unknown Speaker  22:54  
it's okay.

Unknown Speaker  23:15  
I guess I have a

Unknown Speaker  23:18  
what feels like a silly question, which is like,

Unknown Speaker  23:20  
how

Speaker 6  23:22  
do you draw the line between dealing like, generally, like, formally dealing with outliers and cleaning data, or is this kind of,

Unknown Speaker  23:34  
like, I guess I'm just, yeah. Also, if this doesn't matter, then we can skip it.

Speaker 6  23:43  
Is there really a difference between, like, I generally have thought about cleaning data as some, to me, this feels also related to some like, there's not really a line between cleaning data and what we're doing here or there's,

Speaker 1  24:09  
I think your intuition is on to something important, which is that you know it can be the case, right, that these kind of outlying observations are due to a failure of faith, right? Really weird it's often the case

Speaker 6  24:24  
that something like it's is this I've been looking for satisficing, yeah,

Speaker 1  24:29  
so that's totally possible, but that's only one possibility. So one of the reasons why it's good to do these kind of checks is because you can find coding mistakes by doing these kind of checks, right? I mean, not always, like, if something, you know, if you code something crazy, like it should be like one to 10, and you accidentally put one observation a million, right? That mess things up. And you can probably find that stuff happens a lot so, so in that sense, they're related, but that doesn't have a big reason. Like in that other example, I had this one observation out here, that it was actually drawn from the same distribution that wasn't still had high leverage and and so like in the case of mixture modeling, another example, you know, let's say

Unknown Speaker  25:25  
no,

Speaker 1  25:35  
let's say that this is some variable, and you just Got one variable and you're trying to model it as, like a normal, let's say you've got a bunch of, I mean, you do, like a rough quad, like, you got a bunch of observations like this, and you got a bunch of observations like this, and, like, nothing in between. If you try to impose a single normal distribution as your model of these data, you know it's gonna it's gonna be whack rate. It's not gonna make any sense, because the expected value is like right at zero. And so you're gonna get a central tendency of zero. But the normal distribution obviously does not represent the data. It's by mode. And so instead, here what you really got are just two different data generating process. And so you would estimate a mixture model instead, which says that the data is generated by some mixture of, you know, two normal distributions, plus, like one minus alpha, normal, mu, Q, right? So the data is generated by some mixture of these two, and then you can try to estimate the proportion of the population that's in each of those two. So that's also possible, and that actually would make a ton of sense in the context of this, where that's actually the case, right? There's two data generating processes. This is a different model than that, and that's what a mixture model has to do. That's another possibility if you take Daniel's measurement modeling course, next fall, at some point, you'll talk about mixture models in the context of

Speaker 2  27:21  
I

Speaker 1  27:29  
mean, again, this is super unsatisfying, I'm sure, but I don't know, tell you, right? There's nothing obvious that you can do to solve these problems. And so as a general rule, right being as open as possible, and estimating with different procedures that you think are reasonable, and reporting all your results is and if you live in a better world, that would be enough. Fall in the world, and so it's hard to publish clear conclusions, and that's not unique. So the last topic for this section is generalized or weight of these squares. And so remember that our fifth OLS assumption is our assumption about the variant co range matrix of beta is not a problem of bias, not a bias of the beta estimator. It's a bias of the bias and standard error, and we can correct right for the standard error using the procedure. So we can deal with the bias and standard errors by correcting the robust standard errors or cluster standard errors, or whatever is appropriate. But there's another consequence of that assumption, and that said, OLS is no longer most efficient not It's not blue anymore. It's only the best Linear Unbiased Estimator when we have that simple form the variance covariance matrix of data. And so that obviously raises the question of what a more efficient estimator would be, and the alternative generalized squares under a certain assumption. Assumptions is the lower bound, the most efficient possible. And so if we have that assumption five violated, you know one, one thing to do is just correct the standard errors, but use OLS and correct the standard errors. You're giving up some efficiency. If you move to generalize these squares, you get a more efficient estimator under the assumption that model, and you also get correct standard error. So you get correct standard errors and a more efficient estimator. So the question that you're asking yourself is, well, why did we not just talk about this the whole time. And the answer is, because, you know, you never really know what you need to know. And so it's not entirely clear which of these places and you'll see that, okay, so I guess the first thing so you'll see weighted least squares. This is really, you know, people will call this weighted least squares. It's a special case of generalized least squares. So these two things are closely related. So I'm calling this GLS, but you call weight, weight of these squares? Yeah, you were

Speaker 5  30:34  
saying that we estimate the model to be able to identify the outliers. We estimate identify out files and then say, no, actually, GLS

Speaker 1  30:46  
would do that, no. So these are just two different kinds of things, right? You could estimate GLS and also do all those same exact diagnostics. Okay, so you, in fact, I probably should switch the order No,

Speaker 5  30:56  
so you don't need an ons model to identify the outliers in the way that we talked about it? Yeah, exactly yes.

Speaker 1  31:10  
Okay, so we call this weight of these squares, or it's a special case of generalized squares. But the basic idea is that instead of minimizing the sum of square errors, right, some of the square residuals, we minimize a slightly different function where we just divide each observation residual squared by some weight, w sub square. So again, right? This is basically OLS. This is OLS, right? And now this is with these square we're just dividing each observations residual square by that, and we're trying to minimize that overall sum. So sort of the slogan would be, don't you know, OS is minimize the sum of the squared residuals. Weight of these squares is minimize the sum of the weighted square residuals. And so the trick then, is just to find these weights. Okay, so that's weighted least squares. The more general form of this is generalize these squares. This is a special case of generalize these squares. This is the general case, general and so the difference between this and OLS is this omega matrix in the sandwich in between the other. You take out omega, you have x, transpose x, inverse x, transpose y, and just OLS. So another way to say that is that OLS is a special case of GLS, because if this was the identity matrix, this reduces to so we should think of omega as an n by n weighting matrix, where n is the sample size. And so basically, what we're doing is is, at least in the case that we're interested here, we're trying to create a matrix with a main diagonal that gives a weight for each observation. So, so this is the same as this with a diagonal matrix where the main diagonal is so, so in the case we're interested in, we're interested in a diagonal matrix with weights for each This is more general, because this doesn't have to be a diagonal

Speaker 1  33:31  
matrix. So the obvious question is, what is omega? The answer is, we don't know. And so they say that GLS is typically infeasible because we don't know what omega is. We knew omega, then we could estimate it. We had this estimator. We don't know omega. So there's an alternative. It's called feasible GLS, because it's actually not this hypothetical. You can do it. And so what we're going to do is try to is to estimate the weights. We're going to use as weights, some estimate of the variance of each observations error term. So this variance of UI, given x, I is the variance of the error term for observation I and what we're going to do is we're going to say that that's going to be a function of the independent variable. This is where you say there's a constant sigma squared that's for everybody. And we're going to multiply that constant by some factor that will be between zero and infinity always positive, and that factor is a function of the independent variance, which is just another way of saying that there's heteroscedasticity. That's the definition of heteroscedasticity. Is that the variance when this is between zero and one, there'll be less variance for this observation than everybody else. When this is above one, there'll be more variance than everybody else, but the independent variables are determining whether this factor is below one or above one, and by how much, again, that's just the definition. So what we're doing is we're going to model the variance explicitly using our independent variables, which we've decided is important because there's heteros gas. The only tricky part here is that, because the variance is always positive, we need to force that positivity by using a non linear function. And so this ends up being a non linear model, which is just a little

Unknown Speaker  35:43  
bit okay question so far. Yeah, what did you say that delta was here?

Speaker 1  35:49  
Yeah, good question. So, so this is just going to be, this is just a linear regression model. Oh, so this is just going to be the independent variables times, instead of beta, I'm just going to call it delta. So it's just a set of regression so underlying

Unknown Speaker  36:02  
this exercise is

Speaker 5  36:07  
the assumption that there is some like structure to the reason for variation.

Speaker 1  36:13  
Yes, yeah, and specifically that that structure is is related to the independent variable, because that's the violation, the assumption violation.

Speaker 5  36:24  
But then in terms of your theory, that's definitely something you should be thinking about, right? Because that's a crazy thing to do without at least explaining like substantively, because it would have very different implications for what your hypothesis

Speaker 1  36:44  
there are, I don't think it's quite that clear, but

Speaker 5  36:48  
there's some relationship between your independent variables and your

Speaker 1  36:51  
error, not quite okay, between your independent variables and the variance of your error. So the next section of the course is the relationship between the expected value of the error term variable, that is a much sort of fundamental problem that very relate to what I think you talking about. This can be that like, I think I gave an example in previous class where political psychologists, Alvarez and Brown did this study, where they model the variance of the error term as a function of ambivalence. With the idea being that you can think about the variance of the error term in public opinion as like a measure of variability of attitude from observation to observation. And so that might be substantively interesting, and that it might be the case, that's what actually what you want to do. You have a substantive interest in the variance. That's possible, but it might also be possible that you're just trying to get the efficiency gains associated with a better estimator, so that you're not you don't really care that much about how these things are related. You just want to that makes sense. And I don't think it's the case that you know, once you accept this, you have to think, well, you know, I have to, like, integrate that into my theory, or something like that. I think you can treat it as a nuisance, as kind of okay, but your intuition

Speaker 4  38:17  
so to plug this back into the GLS formula, it means that we are punishing, no, we're applying a penalty to the variables that are furthest away From our as from our like normal OLS maker.

Speaker 1  38:42  
Way. No, because, well,

Speaker 4  38:47  
because, yeah, this is how I understand it. So we, so the GLS thing is that for each of the observation, we divide it by a weight, yeah, and then we estimate, like, the distance, sorry, sorry, you estimate the distance, then we divide it by the weight, which we penalize it by the weight that we give to it. Right here we're estimating that weight. So essentially, yeah, it means that the more the variance that we estimate, the bigger the weight is like, the bigger the denominator is. And so essentially, what we're doing is we're penalizing the like, yeah, right, the observation furthest away from our lines of

Speaker 1  39:36  
things that we asked, yes, exactly yes. That is exactly right. So we're going to take the predicted value based on this estimation and get a predicted value of this and then we're going to use that as the weight. And we took the weight in the denominator, the larger the weight, which is just another way of saying, the larger the estimated variance, the smaller the contribution of that observation to the estimation. Again, because it's going like you said in the denominator this, you could just think of estimated variance of observation. I If this gets large, this whole thing gets is reduced in size, and so it has less of the contribution. I'm just restating what you said. Sorry. Did I just make it worse? Because you're exactly yeah, and I'll show you what that looks like. Okay, sorry, more, I should have just

Unknown Speaker  40:35  
said, I'm just thinking,

Speaker 1  40:38  
okay, so, so let's, let's look what this actually does. So again, right? The problem is like, if we knew delta, this would be easy. We don't want Delta, and so we need to estimate delta. And so what we're going to do is something very similar to what we were doing when we were doing log linear models. And so we're going to take the natural log over on this left hand side and model that as a function the natural log of the residual square as the dependent variable. And then we're going to estimate an OLS regression with all the independent variables on the right hand side. So delta is just going to be the you know, the beta regression. Why are we taking the natural log? Because we're estimating E to this. So, so u squared equals E to this. So we take the natural log, we get the linear function. It's squared because we want the variance. We don't want the the we don't we're not modeling the directional deviation of the residual. We're modeling the variance of the residual. And so this is just another way of writing down the variance of a variable, and this, by assumption, is expected value zero, so it just reduces to the expected value of u squared given x, which is exactly what we have. So it's u squared that we want, not U, not the directional deviation, but the variance. And then after we estimate this model, we'll get delta H, and we're just going to calculate the expected value of the weight as e to x delta hat. So we're going to estimate this model and then use delta hat to get the expected value by taking by exponentiating x times delta, that'll give us the weight, the estimated weight. Then we can just plug that into the main diagonal and estimate GLS. So let me, let me give you an actual example, and then feel free to ask questions. So here I've got my running British election Panel Study model. Say we want to do GLS here. So if I already estimated this model, I can, you know, estimate the original model and store it in m1 and I can take the residuals out of m1 based on my OLS estimation. And so here I'm taking m1 residual, swearing them, because I want the variance. And then taking the natural log of those square residuals and using that as my dependent variable, again, I estimated an OLS model that I'm actually interested in, and I took the residual from that model, squared them, and then took the natural log and put it on the left hand side of a new model. On the right hand side, I just have all the independent variables of the original model. Just re plugging those in. And here I'm just saying, you know, the first independent variable from my original model, the second, the third mean intercept as well. Once I've done that, I get predicted values for this thing, but I don't want this thing. I want the original thing, not the natural log thing. So I need to take e to that. So what I'm going to do then is to estimate weight of these squares with E to this as weights, one over e to that space. So here I'm just doing a new call to LM. I'm estimating the original model, but I have this additional option now for LM, this weight equals one over e to my fitted values from that first stage model. So LM has an option called weights that you can use to put in weights, any weights you might have, however you want to weight your observation. This will do it for generalized v square, for weight of these squares, we want one over e to the fitted values from this. And that's that's weird. So again, what did I do? I estimated an OLS model of interest. I took the residuals and squared them the natural log of those and used those as the dependent variable. Then I estimated OLS of those log squared residuals on all my independent variables, generated predicted values and then use one over each of those predicted values as weights in the original

Speaker 7  45:31  
model, You don't have to incorporate the interest.

Speaker 1  45:42  
Is there because whenever you you run LM, you just get an intercept by default.

Speaker 4  45:51  
Yeah, so essentially, what we're sorry to continue like the previous line of logic, just to try to understand why we're doing this. So essentially, GLS and feasible GLS is minimizing the leverage, oh, sorry, minimizing the impact of the observations with potential for highest leverage or influence, or I would

Speaker 1  46:27  
just say it's, it's, it's adjusting the influence based on the variance. So if you think about an observation with very high error variance has less information than ones with low error variance, and so you're basically just taking into account the fact that some observations have less information.

Speaker 4  46:48  
So that's how it relates back to the concept of leverage and influence.

Speaker 1  46:54  
Leverage is different, right? Because leverage is distance from the centroid of variable center of that distribution, the residual of an observation, like a high level, really high leverage observation, very small, because it's dragging regression towards that. I based on this conversation, I really need to switch the order of these two things, because I think it got confusing when I did outlier, influential points first and then GLS, because GLS is much more about problems with heteroscedasticity than it is about outlying observations. So yeah, it's not leverage necessarily, because it's often the case that leverage, high leverage observations have smaller error variance. So this is basically, you're you're assuming that you don't have outliers. And so it makes makes sense to be using the residuals as a measure of information, because you're assuming that everything's from the same data, data generating process. Sorry, is this? Does that make sense? Yeah.

Speaker 4  48:04  
Can I have a follow up question? So if what we're doing is we're adjusting the variance of each of the coefficient that we have here, wouldn't that have an impact on how we do hypothesis testing? How do we know that, like, the previous variance is too high? Now we implement, implement some ways to make it smaller so now our coefficient are significant, yay. But, but how do we know that that is like, I don't know how

Speaker 1  48:42  
to how do you know it's legitimate? Or, yeah, it depends on, maybe this will be answered when I get to the assumption today, sort of the properties in POS. But the the answer is, you know, general terms is, you know, GLS reduces, maybe let me, let me do the properties in a second, and then, and then, if you're you, you want to keep adding questions,

Unknown Speaker  49:16  
But the outcome of this are we just like, predicting?

Speaker 1  49:25  
Second stage, this is interpreted exactly the same way you interpret any OLS, COE services are the same. Everything's the same.

Unknown Speaker  49:34  
Yeah, everything's

Speaker 1  49:37  
for this first model. It's trigger interpretation, but you usually wouldn't be interpreting that all that much.

Speaker 3  49:46  
This is maybe related to what to was asking for. Like, what is the relationship between like, GLS and OLS? Because it seems to me, like, so if you want to do GLS, you first have to do OLS, right? Because you need to estimate the initial

Speaker 1  50:00  
error structure or feasible? Yeah, if you don't know, you don't know what the weights are,

Unknown Speaker  50:07  
you don't know. You never know about doing the normal OLS first, right?

Unknown Speaker  50:12  
Yeah. I mean, I get, I get the counter intuitive nature, but it does have desirable properties.

Speaker 3  50:22  
No, no, I get that. I was just thinking like, you could never do a feasible GLS without first starting with OLS or like a different model that estimates the error structure where you need it for the weights.

Speaker 1  50:37  
I mean, I wouldn't say never, just because I can imagine this. What people when I'm calling the feasible GLS, that's true, but like, let's say that you know you were like, I'm gonna ask experts what they think the variant of these observations are. That probably isn't a good strategy, but you could do it or flip a coin. Yeah.

Unknown Speaker  51:11  
Well, that's not true.

Speaker 1  51:19  
I think so they're blank on the paper now. But someone was for American politics who were estimating public opinion at the county level, I think, or something like that. And they did, they did weight at these squares. And they were, they were giving a weight to it was like aggregating counties or something, and they were giving an explicit weight to each county based on, you know, the functional form of the variance based on sample size. It's like, you know, the formula for the variance of a public opinion poll based on the sample size. And that case, you have a theoretically determined weight. You know exactly,

Speaker 2  51:58  
at least in principle, and so you would plug them in. That's actually the example.

Speaker 1  52:07  
The other case where this can come up is in survey weighting, where you know some observations receive larger weights than others. And so like, if you have survey weights, I should say it's closely related to that, right? It's like, if you have survey weights, you know the weights already, and there's a sense in which you can plug in weights here, but you shouldn't use LM weight or survey weights, because it doesn't there's an additional adjustment to the standard errors that's necessary. So you should use like survey

Speaker 5  52:51  
representative a higher percentage of a minority than using Could you use weight for observations of people from Southern

Speaker 1  53:04  
groups do address that? Yeah, exactly. That's exactly what survey would do. So if you Yeah, if you have, like, only, if you have, like, 5% of your samples from a minority group, if you know to be 20% of population, you would essentially give each of the people in that sample from that group a weight of four. That might not be quite right, but there would be some process by which they would be over weighted. By it, they would count like two to three, four times as much each of them and other groups members would count less than one. They would count less than they otherwise would. And so that is a form of weighting of weighted squares, essentially. But the problem is, is that you have to also adjust the standard errors with that. And so if you just use LM with survey weights, you'll get, I think in most cases, I can't say this correct. Sure, I think you get the same beta coefficients as survey Rea, but you get the wrong standard error. So there's an additional complexity in calculating the variance that stuff that sometimes

Speaker 1  54:21  
covers just one other option here. I can't tell you what the I think these are asymptotically equivalent. I don't know anything more about it than that, but you could include like the white version, the White adjustment version of that we talked about in the context of standard errors as your way of predicting the log residual. So here I'm just including the original fitted values, and the fitted value squared those are linear combinations of the x variable. So it's like including the X variables on the right hand side. And then you can use the same thing with this, but instead of including all the predictors, you're including the Fit value model and square

Speaker 7  55:05  
is the feasible GLS on bias.

Speaker 1  55:11  
That's next topic, next slide, I think correct me about so feasible generalizing squares is biased. It's a bias estimator, but it is consistent. It is only consistent. However, under this more stringent assumption, remember when we talked about exogeneity assumptions for OLS, we talked about two different versions of that. One is that each individual predictor is uncorrelated. This assumption means that any function of the independent variable,

Unknown Speaker  55:50  
stronger assumption,

Speaker 1  55:53  
so OLS is consistent under the weaker assumption, and it's unbiased. Fg, LS is only consistent under the stronger assumption.

Speaker 2  56:02  
Asymptotically,

Speaker 1  56:06  
it's more efficient than OS. So here, the distinction would be generalized least squares with a known Omega matrix, or known case would be finite sample work,

Unknown Speaker  56:21  
feasible general

Speaker 2  56:28  
when you're doing test sensor six. So this isn't a property

Speaker 1  56:36  
if you're doing like, that's like a restricted and unrestricted need to remember these things and getting use ex anyway, using

Speaker 1  56:53  
the same data. Okay? So, so it's bias, but consistent, and it's asymptotically more efficient than L's good. It corrects the standard errors. Do you get survey standard errors and t test? That's also good problem. The problem is that all those things are only true if you properly specify the variance function, where by variance function I mean, so then the question becomes, well, what if you miss specify that? And the good answer to the optimistic answer is that it doesn't seem to be that big of a problem. So the first thing is that if you miss specified that variance equation, your standard errors are now incorrect. But there's an easy fix for that, you just use robust standard errors. So this isn't a real problem. You can solve it just using HC three, robust. So if you miss specify the variance equation for GLS, feasible GLS, pretty much you should always just use HC three or some correction for if you do that, then you're back to correct standard error one if you miss specified variance model, variance equation. This is not no longer guaranteed to be more asymptotically efficient than OLS, but it seems to often be infectious. Yeah. So, yeah, this is one of these, like, depressing conclusions where it's like, well, but I want to know when, like, maybe often. So this is one of these worlds where we can't prove anything about this. But, you know, I guess the literature on doing this, beyond simulation, literature looking at, you know, how often it's the case that weight these squares is going to be better and more efficient than OS, even when you misspe variance equation seems to be high, so that it'll often be the case that you this will be better, even with a miss testified. So that's it.

Unknown Speaker  59:13  
Questions about properties.

Speaker 1  59:21  
Do you ever see people doing this? Not usually? Should you be doing that? Good argument. So I don't know exactly what. I'm not sure in terms of practical advice, like what to tell you to do there, I guess there's two ways to look at this discussion. On the one hand, right? There seems to be a reasonably good argument that using generalized least squares as sort of a more default approach would be a reasonable choice. Practically speaking, nobody does that, that I that I can speak because it's harder time, more confusing. That's not a very good argument. Very good argument. You might, on the other hand, take this problem very seriously and say, like, if this is the case, that, for me, just makes it a default OLS better. That's also unreasonable argument that, with correct and standard errors, especially if you have a very large sample size, where you're not as worried about small sample size, this starts to become much more important. So if you're not terribly worried about this and you have a reasonably small sample size, now, efficiency is a thing that you really care about, right model for the value estimates.

Speaker 1  1:00:50  
That's part of it, but the whole model includes the specification on the right hand side. So like if the true model has a quadratic term for one of the independent variable. But I guess the second thing I would say, if you're like frustrated with this, is that it does seem useful to me for pedagogical purposes to go through this, because thinking through what's going on with this, I think, expands your understanding of sort of estimation and what OLS is doing and what the more general form of OLS would look like, and so on. And you will often see, as we talked about, right, the idea of weight score is coming up in other contexts, in particular survey research. So having some sense of what's going on very useful, even if you don't use this by that logic

Unknown Speaker  1:01:48  
is with a GLS, it's

Speaker 1  1:01:56  
just, that's exactly right. So again, here's GLS, if you, if you make omega an identity matrix, this reduces to OLS. And an identity matrix would be ones on the main diagonal, which is equivalent to exactly right? So it's generalized these squares in exactly that sense, right? It is the more general form of the estimate of the estimator that we've been talking

Speaker 4  1:02:23  
about your slides on properties. Do you mean that the properties only apply for fgls?

Unknown Speaker  1:02:30  
Yes, other properties? Yeah, so regular gloss is unbiased,

Speaker 4  1:02:34  
right under the condition under the assumption that you know

Speaker 1  1:02:41  
the true Omega point. So if you know omega, then you're in the world of GLS, and it's an unbiased estimator, and it's maximally efficient. It's a lower bound on variance. Okay, that's provable. But when we don't know omega for sure, we go to feasible GLS. And feasible. GLS has this weaker property of consistency and a more tenuous claim to estimate to efficiency.

Speaker 4  1:03:10  
But coming back to your example of the group that use like the theoretically calculated waves, because I see that a lot in the surveys, like the household living standard surveys and stuff. We use that a lot in like paper set use that survey. So in that case, it's not SG OS, because you have, you don't estimate the weight based on feasible GLS. So then, is it still unbiased? Because there's a risk that the way that we theoretically estimate

Unknown Speaker  1:03:49  
is not actually the

Speaker 1  1:03:50  
real way. Yeah, that risk would be there, but under the assumption that you have the correct ways

Speaker 4  1:03:55  
unbiased, okay, so we just have to assume, yeah,

Speaker 1  1:03:59  
but yeah, exactly right. So, so yeah, if you don't know the weights, if you're uncertain about the weights, then you're uncertain. Yeah, you're definitely you're uncertain about the unbiased because I'm trying to think, I don't know what the answer would be, like you knew that, if you knew they were correct in expectation, I don't know what that would mean, but yeah, yeah, that's that's exactly you're uncertain about the ways. But if you do know theoretically given ways, then it's both unbiased and Maxim.

Unknown Speaker  1:04:39  
Questions

Unknown Speaker  1:04:53  
Well,

Speaker 1  1:04:58  
we will. So that's all I have for for the section, of course, so we'll move on to endogeneity on Thursday. So we've got endogeneity, we're never going to get through that Thursday next week. Then we have five parents trade off, which I think is an important topic for us to cover in this

Unknown Speaker  1:05:19  
class. And then we're going to extra day, probably

Speaker 1  1:05:23  
don't figure out it, it's not going to probably not be worth diving into,

Unknown Speaker  1:05:28  
like, time series.

Unknown Speaker  1:05:36  
The extra day.

Transcribed by https://otter.ai
