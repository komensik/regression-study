Speaker 1  0:04  
Okay, so what is the plan for today? We are going to, unless things go terribly wrong, finish up advanced strategies for inference finally, and then we will move on to qualitative variables, where there's not a lot of content to do there, so we'll see how far we get. Okay, so our last topic for advanced strategies is the delta method, which is potentially the most popular method for calculating uncertainty estimates in the literature, whenever you're not using, like, some straight forward analytic solution, like with regular OLS, it's the default in most of status post estimation routines, the default for marginal effects in R so it's extremely popular. You should know what it is, what it does, what complications are. The basic idea is pretty straight forward. Remember, sort of what we're interested in. We've gotten a set of coefficient estimates. We have an estimator, beta hat. But it turns out that in some cases, we're interested in some function of beta hat, rather than beta hat itself. Beta hat you get from summary of an LLM object. But sometimes you want, you're interested in a function of the difference between two betas, or you want some non linear transformation of the beta. And so those are the kind of situations we've talked about. I mean, I should have given a better example, right? It's like, right? It's like you're interested in the conditional marginal effect. That's a function of the betas. It's not the betas themselves. So we're interested in some function of the betas, and we want a variance estimate for that function. And so what the delta method relies on is the fact that if you have a differentiable function, which is, are the functions we're working and if the it's a function of a variable that is itself asymptotically normal, then the function is asymptotically normal as well. So that's, that's sort of the key idea is that if you're interested in some differentiable function of some random variable. In this case, it's our estimator. If that estimator is asymptotically normal, then the function of that estimator is asymptotically normal as well. And so, so that's the key. And it turns out that if you know, under those conditions, we can if we look at the difference between the function of the estimator and the function evaluated at the true value of beta, right that difference converges in distribution to a multivariate normal with mean zero and variance covariance matrix equal to this thing. So what we're saying is we have some function of our estimator that we're interested in, and we want to know what the distribution of that function of the estimator is around that function evaluated at the true value of theta, which is just saying, we want to know what the sampling distribution of the function of our estimator is. This is just saying, what is the sampling distribution of the function of beta hat? What this is saying is that that sampling distribution converges in distribution to a multivariate normal with mean zero and variance covariance matrix equal to this. So the sampling distribution for our function of our estimator has this form as n goes to infinity. That's going to be the key assumption. Just some notation here, because I sort of struggled writing this writing, all this stuff, out in a way that very intuitive. But here we're saying, right, a differentiable function of our estimator. This is saying that function evaluated at the true value of beta. This is saying so this is called the gradient. It's the first part for the vector of first partial derivatives of theta, the vector of first partial derivatives evaluated at theta, the true value of theta. So in this context, when I put the apostrophe after F, what I'm saying is the derivative of that function. But because it's a because we're trying to take the vector of first partial derivatives. What we're actually calling what we're actually doing here is the gradient grad. So this is the vector of first partial derivatives of this function evaluated at the true value. And you'll see, I'm going to give you a simple example where you'll see what I mean at the end of the day. This gets very complicated, very fast, and so that's why you're really always going to rely on software to do this for you. But you know? Okay, so the dump method is so popular because it's so flexible, right? If you can write down a function of beta hat, right? Then basically you can do this, write down a different So pretty much, anything you want, any quantity you're interested in, you can almost always use the delta method for it. You get standard errors. Constant sounds things like that. So it's super flexible. The down side of it is that it relies on this asymptotic normality to be valid. So you're, you know, you're doing a normal approximation. And so if your sample size is small, you're going to be worried about using worried about using it. Because you all you know is that the quality of this approximation goes, you know, get perfect as n goes to infinity. But at small samples, it's not going to be right. And again, it's all the same things we talked about, right? How many do you need? No, right? It converges, then goes to infinity. So, you know, you're sort of stuck in that same world of, you know, we know as asymptotic properties. We don't know it's finding sample properties. More, larger sample size is better. Most of the time, when we're doing stuff in social sciences, or at least in political science, we're probably good. If you have a particularly small sample, maybe think about, okay, so, so this is sort of the definition of the delta method, but the key part is over here, right? This is what we're trying to get, the variance covariance matrix for the function of our beta hat. And so where does that come from? Question, and it it comes from this. So what we're going to do is approximate this function. So what we're going to do is do a first order Taylor expansion. Did you do this last semester? Anything with Taylor series? No. So a Taylor expansion is a way of approximating a function using the derivative the different different orders and derivatives of that function evaluated around a particular point. And so what the delta method does is it uses a first order Taylor expansion of the function of our estimator about the true value. So it's going to approximate this function. It's not going to we don't know that. We don't know we're not going to know exactly what the function is, but we're going to approximate it using this is the first order Taylor expansion evaluate. By expanding this approximation about a particular point and the point, we're going to choose the true value of theta. That's the idea. This is an approximation of this function. Again, this is the vector of first partial derivatives, the gradient. And so we just write that down first, right? That's like a that's an identity, and that's the first order Taylor expansion. Now we can subtract this over here and take and square both sides. So if we subtract the function evaluated at the true value of the other side and square it, this should look like something of interest. This is what we want, essentially almost right. It looks like the variance of the function of our estimator, which is what we're looking for. That's what we're trying to get. So we're on the right track. Now. We've got this, this thing over here that we need to deal with, and we probably want to simplify that a bit, and we can simplify it. So the thing to notice for this next move is that this is actually a scalar quantity in here. So this is one by k. This is k by one. So this whole thing is one by one. So it's a it's a scalar. And if you have a scalar, you can take the transpose of it, and it's a sub case, we can take the transpose of the scalar, and it's just the same thing. Return to you. And so I'm going to do that, and if I take the transpose of this thing inside, I just reverse the positions of the two terms and then put their transposes. So this is going to be, instead of, you know, this transpose times this, it's going to be this transpose times this, and then I'm just multiplying it by itself. And so that's what I've got here, right? I've got the thing that's up here just rewritten, and then I've got the transpose of that thing over here. And now, when I multiply these two things, when I multiply that by itself, I get something interesting, because this is the outer product of that vector, which is a matrix. And if we get the expected value of both sides. It's just the variance covariance matrix of beta hat.

Speaker 1  10:06  
So this is the outer product. The expected value of that outer product is just the variance of beta hat, because it's beta hat minus the true value of beta that's exactly what the variance covariance matrix is. And so what we end up with when we take expectations on both sides is exactly what we're looking for. You get the variance of the function of our estimator on the left hand side and on the right hand side, we have an expression that includes two things that can be estimated from our data, the value of beta. We can just plug in an estimate, and we can plug in our estimate for the variance covariance matrix of beta hat as well. And so this is an approximation, right? We started off with an approximation. So this is an approximation, but it converges as n goes to so this is, that's where this content,

Speaker 1  11:06  
okay, so let me show you an example, and then we'll stop and take questions. So let's so this is an example that we've been working with, but let me write it down real quick, in short form, just forgotten the two. So we have, like, something like beta one, age plus beta two, age squared. I mean, there's other stuff there that doesn't matter. And what we're interested in, in this context is the conditional, marginal effect of age that we take the first partial derivative, can we get beta one plus two, theta two? First derivative of this gives us the conditional marginal effect of age, which is now a function of itself, because we have a quadratic term. And that's what that's let's say that that's the quantity we're interested in. And so notice right, that what we're talking about is a function of the beta hats. We're not looking at one particular beta hat. We are interested in a function of the beta hats, and that function is written like this, beta one plus two times beta two times eight. That's fine. We can easily calculate the conditional marginal effect for any particular value of age. But what we are going to want for any particular version, for any particular estimate of the conditional marginal effect is a standard error for that conditional marginal effect. Conditional marginal effect and some kind of confidence. So that's what we want, right? This is the puzzle we're trying to solve. Can we get a standard error for this function that the beta has? We're going to use the delta method to do it. So what do we need for the delta method? We need two things. We need this thing and we need this thing. This is easy. We can pull that right out of the LIM estimation right using B code. This is a little trickier. This first vector of first partial derivatives, the gradient of that function is the more complicated thing that so let's, in this very simple case, it's actually easy to show what this looks like. We need, right? This is, again, called the gradient. It's the vector of first partial derivatives. And a vector of first partial derivatives means that for every variable that's in there. So for each beta, we need to take the first partial derivative with respect to that beta, treating everything else as a constant, and then we plug that into the first entry. Then we go to the second one and take the first part of that. So there's two betas here. So we need two first partial derivatives. The first the first, first partial derivative is going to be with respect to beta one. And if we take the first derivative with respect to beta one, what do we end up with? One beta one, one times one, this goes to zero, that drops out. This whole thing is a constant because it doesn't include beta one, that drops out. We're just left with beta so the first entry of the gradient is going to be the first partial derivative with respect to one, and that's just one, but we need to do this for beta two as well. So now let's take the first partial derivative with respect to beta two. This is a constant. Now it drops out. This is one times two, and now this drops out, and we're left with two times eight. It's 22

Unknown Speaker  14:49  
sorry, so then we

Speaker 1  14:55  
just plug that into the second entry. So the only difference between what you're typically doing with derivatives and what we're doing here is that we're, we're taking the first partial derivative with respect to each of the betas, and every time we do it, we plug it into the vector that we were ultimately going to get. And so this is, this is the final gradient for this particular problem, and that's the one we're interested in. That's what we need, right? So now we've got both things we need. We've got this and we already knew this anyway. It's easy to get and so now we can do the delta method very simply. Normally you wouldn't do this out as like a formula, but I want, what I want to show you is that when you actually do this by hand, what you end up with is the analytic solution to the conditional marginal effect, very for quadratic model. So here's the whole formula for the variance covariance matrix. Here it is plugging in the gradient and the estimated variance covariance matrix from our LM output. And notice here I'm just subsetting that matrix to only the parts of it that are relevant. So variance covariance matrix, right? Is going to be k plus one by k plus one, but all we want is the portion of that matrix that includes the entries of beta one and beta two. That's all that's of interest to us. So we got the variance of beta one, variance of beta two, and the covariance, theta one. So we're just pulling out that that subset that's relevant to our I'm not going to do this multiplication, but if you do this multiplication through, this is what you get. And if you go back to earlier slides, you'll notice that this is exactly the analytic solution, or, for the conditional, the variance of a conditional, marginal, effective model. And so this is what, when you know, like a week and a half ago, when I said, in these simple cases, the delta method gives you the analytic solution. This is what I meant. So the delta method is a perfect approximation in a simple but that's not general. And I'm not going to go through this example because it's super complicated, but let's say that you have a more complicated model where it's only an approximation. Let's say here I've got log of salary on the right hand side, and I'm interested in, let's say, in this case, I'm interested in the average first difference moving years of years since PhD, from 10 to 20 on UN log salaries, right? So I've got a non linear model. I've got a first difference, and I want to take the average first difference, allowing every person to have their own values of all other independent variables, and I want to use the delta method. What I what I show you in here and in the code, and you can go through that on your own, if you're interested. Right is that you could do this as well. It's just annoying. It's super annoying to do, and you just have to be super careful about each step. But it's not, not terribly difficult. It's just annoying. And again, that's, you know, if you're interested in trying to really see sort of how the delta method is working in a more complicated example, you know, go through it, but almost in almost all cases, right? You're going to be using software to do this stuff, so it's not necessary, right? They can do that. And so here's a couple different ways to use the delta method. So in this case, I'm using a function from the car package, which is nice. It allows you to write a function of your coefficients in symbolic terms, and then it'll do all this for you. So in this case, right? I'm trying to calculate the same quantity up here, but instead of doing the average first difference, I'm holding all other variables at their mean value, because I don't think there's any way. Well, I don't know. I guess I have to explore it more, but there's no obvious way to do like, an average first difference for using this function. But here is a way to do it, if you hold everything else at its mean or central tendency, and all I'm doing is just writing the exact function that I'm interested in in symbolic terms in our package. And you'll see that it's quite easy to get your delta method standard error doing that, you might want to be doing that, right? So, like, this is a quantity that is not, you know, the other way to do this is marginal effects package. But I'm not sure you can easily do something like this in the marginal effects package, where you have, like, a complicated quantity of interest. It's possible. I'm just not sure the marginal effects package is really good when what you're interested in are, you know, the coefficients themselves, or simple functions of those coefficients. But if you got, like a log in kind of variable, and you got some complicated function of coefficients you're interested in, not sure Mark.

Speaker 1  20:20  
So it's good to have, like, another tool in your delta to be able to do this stuff. And so this is just one kind of convenient function in the delta method for you if you don't want to do this. All right, yeah. So what's the episode about this? Right? Delta method is super useful. It's the default in marginal effects. Marginal effects is a great default as a thing to be using for post estimation in linear regression. So you should know what the delta method is, right, because it's if you use marginal effects or use data, you're going to be getting delta method standard errors as a default in many, many applications. And you want to know what that is. You want to know what it is you're reporting in your paper, not just using the software blank, especially because it relies on this assumption you have a really small sample and someone says, you know, why are you using the delta method, and you don't have good answer for them, okay, let's stop for a second and just talk about questions or confusion or anything related to delta

Speaker 2  21:38  
method. Yeah. Can you explain again what it means to approximate a continuous function by this first derivative evaluate it at particular point?

Speaker 1  22:02  
So it's using what's called a Taylor series as a tool for approximation. And so I'm pregnant, but let's do it so you have some function of a variable. And so the idea is that that function of that variable can be approximated using the following infinite series, and again, I'm hoping correctly, but you can Internet if I do, you

Speaker 1  22:52  
I think that I'm pretty sure that I can make mistake there, but actually, so this is called a Taylor expansion, and it's a way of approximating a function using this pattern of derivative. So we've got so this is the function is of some variable x, and what we're going to do is try to, let's say that's, let's say that's the function. I can pick a point A and try to approximate the function by writing a different, a different, you know, expression that tries to approximate that function as best as possible without knowing sort of the explicit representation of that. So I'm picking a point and then trying to expand outwards to approximate that function as best I can. And the more terms you add. So this is an infinite series. The more of these terms you add, the better your approximation is going to be. So if you add more and more partial derivatives, they'll start to approximate the function better and better. If you went all the way to infinity, would be a perfect and so what we're doing and so in this general form, right? The function is of x, and we're picking a specific point, a and then expanding the expanding out around that point. And so in our case, right, we're saying we have a function of beta hat, which is our estimator, and we're going to do a Taylor expansion around the true value of beta, and then we're and at the end, we're actually just going to end up subbing in our estimate. But this is what we're trying to do. We're trying to approximate a complicated function with a simpler function. And the Taylor series is sort of proof that we can do do that in this particular way. And so and the pattern, hopefully you can see, right? We've got, you know, after this f of a, we've got the first derivative evaluated at a, over one factorial times x minus a to the one plus the second derivative evaluated at a, over two factorial times x minus a squared. It would just continue on my third derivative over three factorial, Q and so on.

Speaker 2  25:24  
And here we're only using the first exact term of that exactly. Which does it also mean that the more complicated the function we're trying to approximate, the less accurate approximation?

Speaker 1  25:41  
Yeah, if I don't want to say anything with the arm, just that, that is clearly true when you, when you move from, like the simple kind of function we just looked at in that example, to anything more non linear. So like I showed you, it's a perfect approximation of the simple linear quantity, but if it was a quadratic quantity, it would be slightly less. Yeah, and then, and like, and then, in the other example that I have on here, I didn't go through, we've got a log dependent variable and exponent, exponentiated terms, and that's just impressive. But I the question I can answer is that if you asked there wouldn't be able to answer would be, well, you know, why not use another term in the expansion? And I mean, the derivation only uses the first order Taylor expansion, and you get the variance on the left hand side. So that's kind of like the way I think about it, but how you would go about thinking about it if you started adding more terms. Okay, could you talk again about what we're using?

Speaker 1  26:53  
Yeah, so the idea is that we're starting with the idea. The idea is that we're going to start with an expression that assumes the true value of beta is the point around which we're going to expect, but we don't know that true value, And so when we sub in our our estimated value

Speaker 1  27:26  
and then and sort of the logic of picking the true value of beta is that what we want on the left hand side is the difference between our estimator and the true value, because that's the essence of The variance, which is what we're trying to derive. So

Speaker 1  27:48  
yeah, what we're interested in is getting the variance of this difference. We want to know how our estimator varies about the true value. More questions, also practical questions

Unknown Speaker  28:08  
on the screen,

Unknown Speaker  28:10  
if I can, that's cool.

Speaker 1  28:22  
So you know, I mean, one, one practical up shot here, right, is like, you know, you can delve a little bit into this and get a deeper understanding, maybe, but you know, you don't miss it. For most of practical applications, you can be able to use software. And so more important that you sort of know the fact that you're implicitly assuming a multi, very normal distribution, and so that's what you're using confidence intervals. And if that's an assumption you don't want to make, then the most important, practical, but again, super flexible method, and it's the default in many software that you will use. So nothing, nothing else you

Speaker 1  29:54  
correct, so we've Got a topic. So we've got a topic. So this is an interesting topic to plug in here, because, on the one hand, you've already done this stuff, so it's not like completely new, but there are some sort of interesting aspects of using nominal ordinal variables that are potentially worth talking about, and just making sure that we understand what we're doing when we have today. So what is, what do we mean by qualitative information? What we're not saying here is like in depth interview data in the space, as we often use the word qualitative science instead. What we're talking about is is sort of, you know, it's information that is not quantitative in the sense that, you know, the numbers that you would assign to categories are completely arbitrary. They have no quantitative meaning. They have no underlying significance. They're just labels. That's, that's what we're talking about. Qualitative information is variables that for which you can assign numbers if you want. But those numbers don't mean anything at all. They're just ways of saying, of referring to that same category right using a different base. So region right of the United States, south, north, east, west, you can give those variable, you can give those categories numbers, and then say, you know, I'm going to define west as equal to one. But that doesn't mean anything, right? It's just a label. It's just another way of saying West. That's what we mean by qualitative information. It's in contrast to quantitative information. When we include quantitative information in a regression, the numbers. It matters. What number you give something that means something this is, in this case, that's not. So how do we typically represent variables like this? Right? Here are just some typical ones. The most common way to do it is to, is to give these variables new numeric codes, um, but in a very particular way, and it's to, it's to create a set of binary variables, zero or one, where you get one binary variable for each of the categories where it's set to one if you're a member in that category, and zero otherwise, but then you exclude one of the categories from the model. So this is called the dummy variable approach, where, for a variable with K categories, or I should say j right, for a variable with J categories, you create j minus one binary variables and then include those j minus one dummies in your regression model. Okay, so you know, for example, if we had US Census region and we had four different regions, we could create three new variables where each variable takes on a value of zero or one, and we have a dummy variable for South whereas one if you're in the south and zero otherwise, zero for everybody else, for North East, one if you're in the northeast, zero for everybody else. Mid West, one if you're in the Midwest, zero for everyone else. And then you include these three new binary variables in your model, excluding, I guess in this case, would be West, excluding the West variable. Complete. So there's a couple things to say. The first is that the 01 coding is technically arbitrary. You could do anything you want here, as long as there's a difference. So you could do zero in a million or five and 7.3 or whatever. You can choose absolutely any coding scheme you want for the dummy variables, because there's no quantitative meaning to categories. However, that makes things much more complicated. And you don't want just use 01 coding, because that makes the coefficient readily interpretable without any additional work. If you choose zero and 573,000 it's going to be much harder to interpret your coefficients, so don't 01 coding technically arbitrary, but is the right choice for all purposes. Okay, so let's take a you know, the simplest example, and again, you've already done this before, so this isn't necessarily new, but we've got a simple regression here with right wing economic preferences. As the dependent variable, and we've got one independent variable, which is coded one for people who identified as male in the survey and zero for people who identify as female. And we've got a coefficient, and we've got it in yourself. So how would you interpret? Would interpret interpretation would you give

Unknown Speaker  35:12  
to these

Speaker 3  35:14  
results, the economic mean for a male, actually, yeah, the economic mean for male would be oh

Unknown Speaker  35:28  
four or plus the intercept.

Unknown Speaker  35:31  
But female,

Speaker 1  35:35  
yeah, exactly. Yeah, perfect. So the predicted value for a male is this intercept estimates, plus does the Predict value for female is just intercepted, right? There's one other thing you might want to say here.

Unknown Speaker  35:52  
Maybe, as we change from but, or like, the economic to a female changes by,

Unknown Speaker  36:02  
yeah, exactly. Increase,

Speaker 1  36:09  
yeah. So it gets tricky to say with variables that you don't really think about as like manipulating. So in this case, I would just say the mean, the difference in means between people who identified as male people identified as female is point report, so it's a mean difference. So yes. Hypothetically, if this were like not a quiz, the answer would have been Yes. So this is the mean difference. And I think to your question, it's because we coded male as one, right? It's the difference calculated male mind. And that's exactly what you should you can see that by saying, Well, what's it for females is point three, four. What's it for now, point three, four plus point 04, and the difference is such that the category code of one is getting this addition relative To the category. That makes sense. Yeah. This

Unknown Speaker  37:22  
is basically the average treatment effect.

Speaker 1  37:26  
If being male was one was a true, yeah, in that context, you could improve it that way. In a binary variable or learning

Speaker 3  37:33  
variable, does the f statistic matter down

Speaker 1  37:39  
here, it means the exact same thing, negative. Thing it always does. And what it always means is, if you do an F test comparing model fit or the model you estimated to a model with only an intercept, is it statistically significant? It's

Speaker 3  37:56  
not making like a It's not saying that. Now, it's basically but if it's just the intercept that isn't say that

Speaker 1  38:07  
male, but the gender. So if you only have an intercept in here, remember what that does. That means now that you're just calculating the average of the defense variable. So so the an intercept only model is equivalent to guessing the mean of the DV for every observation, it's it's only female. When you include the dummy variable for male, because now if male is zero, you've subset of the data only females, and it's the need for females. If you took out male, it would just be the it would be the weighted average of these, point three, 4.38 depending on how frequent male is relative,

Speaker 3  38:43  
because it's still being added in as a potential predictor.

Speaker 1  38:49  
I'm not, I'm not sure. I'm not sure about that last part, I guess. I guess my question is, like, if you put in zero from mail, then we get back to the intercept anyway. Yes, but the key difference, right is that the meaning of right, like when you only estimate a model, when you estimate a model with only the intercept, you can't set mail to zero, okay, so you're implicitly just averaging everyone together, right? It's only if you include male in there that you can set it to zero, and when you do, it gives the intercept a different meaning. So in this particular case, the X statistic tells you whether male matter is because the only variable model and so it's telling you is is knowing whether someone's male female contributing additional information to predicting female. Yeah, exactly. And the reason for that is because f is T square, given what you

Speaker 3  39:51  
just said, Would you always expect to have?

Speaker 1  40:00  
And when you say these kinds of models, you mean models with dummy variables. No, definitely not. To give you just an example, imagine that you put in a dummy variable for respondent. So you had one dummy so you create N new variables, and each one is coded one, if you're that respond to zero otherwise. And then you include n minus one of those in the model the R square include one, you just be including a point for every single person in the data. And so that would be a dummy variable model with an R square

Speaker 3  40:39  
one. Okay, now I think I might, because we asked where I decided how much better you an intercept only model is different.

Speaker 1  40:55  
I'm not sure. I'm not sure it doesn't seem different to me, in a sense that's relevant to thinking about R square right the intercept only model is always just, how well can you do predicting y if you just guess the mean of Y and your your model can be anything. It could be dummy variables, continuous variables, whatever you want. The question is just, how good are your variables at predicting y relative to only guess the mean of Y every time. So I don't, I don't really see why moving to the dummy variable model makes us Yeah,

Speaker 3  41:30  
I was just a little thrown, because the meaning of the stuff is different. Yeah, two situations, yes,

Speaker 1  41:39  
that was getting anything more questions? All right, let's, let's look at a slightly more complicated example now, right? So now I've still got that same dummy variable in there, but now I've got a bunch of other variables too. So now, what is the intercept?

Unknown Speaker  42:13  
The average value a woman was not educated

Speaker 1  42:22  
or exactly right. So still, because we have this dummy variable, it's still a prediction for females, but now prediction for females of a very particular set of characteristics, namely, they have zero on everything else. And in this case, because everything is zero to one, it's a female with a minimum value of age, minimum value of education, value so again, it's no different from anything we've ever done before, but I'm just sort of pointing that out explicitly. It's not that you're not making a prediction for females. You are. It's just there's additional conditions now the other and what about the book for male? If you thought if you're just changing from

Unknown Speaker  43:18  
female to male, but everything else

Unknown Speaker  43:21  
is interesting. However, exactly.

Speaker 1  43:24  
That's exactly, right, yeah. So, so this is the same interpretation, but holding everything else constant, it's like our typical interpretation of a regression coefficient. It's still, in this case, the mean difference between males and females, but in now control variables, it's holding everything

Speaker 1  43:53  
else. So let's try to get more complicated. Let's take a different kind of example that will come up with you all, and start making a little bit more complicated. So this is an experiment that I ran with brand bar tells in 2016 you'll use these data as well for your assignment this week. The basic idea we were trying to figure out, which has become, unfortunately, more timely, is whether, you know, people are more willing to, you know, attack the institutional integrity of the Supreme Court when decisions are handed down they don't like and under what conditions, that's more or less the case. And so we there's two experimental factors here. The first is, so basically, people are getting a decision. They're reading about a decision that the Supreme Court made. That decision can be either consistent or inconsistent with their own preferences. So they either like the decision or they don't like it based on their issue preferences. And in addition, right? So that's randomly assigned, and then they also get a condition where there's either no elite cues at all, or the parties are saying things like, Democrats really dislike this, and Republicans really like this, but it's not very polarized, or getting those same party cues, and the parties are sharply polarized, right? Everyone on the right disagree. Sort of you know does disagree with the decision make you want to curve, curve the Supreme Court, attack the institutional integrity of the court, and does the existence of pardon rhetoric surrounding the decision matter? More or less likely, basic idea. So you can see that the experiment here, there's three few conditions, and people are randomly assigned within that in terms of the direct I can't remember exactly why they're unbalanced here, but for our purposes, okay, so that's basic idea, and we're going to try to analyze this dependent variable to zero to one, which is willingness to curve attack. Okay, so I'm going to just start by looking at whether any sort of polar is being any sort of part of cues or polarization hurts the court in and of itself. And so here the defense variable is willingness to attack the court, and I've got this Q's variable right again, that has three values, no Q's unpolarized queues, polarized queues, and I'm just including as dot factor for the queues, because I want dummy variables. And so the easiest way to do that with LM is to if this is not already a factor variable. If Q's was defined as a factor variable in your data, it'll do this automatically. If it's not, you can say as that factor, and LM will treat it as a factor variable. So this was coded numeric, but I'm converting it to a factor variable on the fly regression. And so summarize that it's dummi Doubt two of the engine for me, and what it's going to do by default, is pick like the lowest and treat that as the one that you exclude. So this variable is coded 012, and so when I treat it as a factor variable, LM picks out the first zero value, and then includes Dum use for one and two. So, and that's exactly what I want, because nobody uses kind of like a control condition. So I've excluded the control and I've got now my two treatment conditions as dummy variables in the model. And so that's I'll stop there, and like somebody give an interpretation of this. You know, if you looked at the if you were, forget about significance for a second. Just focus on the focus. How

Unknown Speaker  47:50  
would you interpret the

Speaker 4  48:02  
results? I mean, sorry, the means of the outcome when the cues was only equal to zero is being resurrected, and the difference between the Q equal to a one versus equal to zero,

Speaker 4  48:26  
regarding on the outcome is the point 01 and the difference of The outcome between cubes holding to two two versus two. Zero is point spell Street,

Speaker 1  48:44  
exactly, and so and so, you know, in the context of the project that we were interested in, right? You know, we would think, Okay, are there treatment effects of our two kind of treatments of interest? You know, party cues, or polarized protein cues. And so the nice thing about having the dummy variables for the treatment conditions is that often what we're interested in is the difference between those treatment conditions and the control condition. And so if the control is the excluded condition, these are now the treatment effect estimates of interest. So this is the treatment effect for that first use condition, because it's the difference in means, comparing unpolarized card et cues to the control, the change in that dependent variable, an increase in willingness to curve the court by point 02, tiny comparing this treatment condition to control. The second one is the treatment effect for the polar ice protein used the difference in means comparing that condition to the control. So these are the in an experimental context. Those, those are now the quantities of interest, and that's very convenient right now, I've got the treatment effect estimates and significance tests on those with no other work to do. So in this context, neither is statistically significant, though you might be intrigued by the coin, oh, a p value that Making sense here. This is also an value there. Na,

Speaker 1  50:26  
okay, good. So well, so in this case, but the F value is also not significant. So in this context, how would we think about that? So if you've got a, you know an experiment. You know, if your model is an experimental sort of design, and you don't have any other control variables, you're just including, you know, your treatments in its predictors, then the s statistic is a significance test on whether your treatments work in some very broad sense. It's what we would call an omnibus treatment, an omnibus test of the treatment efficacy. Right? Again, as always, it's testing whether your model is better than an intercept only model. But in this context, right? That means, do I do better predicting the dependent variable, knowing what groups people were randomly assigned to, than not know. That's just another way of saying, did my experiment have any effect on people? And so in this case, the f statistic is insignificant. So I can, I would say I have no evidence that random assignment to these three conditions had any impact on the people. Third, now I was actually kind of hoping that this would be statistically significant. That's another example of the fact that the omnibus has to be significant without any given condition,

Unknown Speaker  51:57  
any good

Speaker 1  52:10  
No, not necessarily, because you could have so many degrees of freedom even up that you would end up it would be true if you only had one predictor, But I've got, like, 150 predictors and only one is significant the omnibus test might not be and so for those of you who've had like psych background, this is, this is just an ANOVA. If you've ever done an analysis of variants, you know, you came up in psychology. You were trained to run experiments and do analysis to variance to see if your treatment had an effect, and then if it did, you would go and look at means. And that's what this is. The F statistic is just an ANOVA, exactly the same, like, literally. So this is an ANOVA of this experimental factor. Um, so what? So one thing you might think is like, well, what if I just, well, maybe even think this, but what if I remove the intercept? So as a reminder, you can always remove the intercept from a regression model by just including zero plus in your LM call. So I did that here, so it's the exact same model, except I included zero plus up here. And now, what do we get? Well, it doesn't kick out anything. It doesn't kick out any of the dummy variables, all 3n and now, what do these mean? Now I

Unknown Speaker  53:50  
What are those quantities?

Unknown Speaker  53:57  
What are these estimates telling you?

Speaker 1  54:20  
Category. Yeah, they're they're equivalently, they're either the predicted value of the dependent or they're the mean of that variable in that category. These are just the means, the predicted values of y, or anyone with that particular value of that category, mean for people in the control, that is the same from the other model. That's the intercept. The Intercept in this model is predicted value for the control, so that doesn't change. This is just the same as having the intercept. But now these two coefficients are no longer the difference between the control and that condition. They're just the means this minus. This is this minus, this is that I hope? Yeah, this minus. This is just that mean difference. So if you kick out the intercept in a simple model like this, you just get the means of the conditions that you're talking about, but right there, it's still going to give you significance test. Because LM is not not that smart. It's kind of smart, but not that smart. And almost surely, you don't care about the significance test. When you care about the significance class. Well, are you interested in seeing whether you know a zero to one coded variable is different from zero you know in terms of its mean and a particular condition? Very unlikely, right? Maybe it's possible, I suppose. But even if you did want to do that, this wouldn't be very good. So once you pick out the intercept, this becomes

Unknown Speaker  56:07  
much less interesting,

Speaker 1  56:12  
if you want to just if you wanted the mean values. So let me give you a practical context where actually this is what like I typically do. So you might have seen conjoint experiments before. So one way to do conjoint experiments is to do dummy variables for every sort of set of attributes that you have. So maybe it's like there's two candidates, and the candidates get randomly assign ages and party. You can include dummy variables for each of those attributes, where, for each one, you kick out one of the levels and all the dummies just compare to that base line. The other thing you can do, and those are, those are the average treatment of average marginal component effects. But the other thing you can do is calculate what are called marginal means, which is just getting the mean value for each of those attribute values. Like, how often did you choose a profile when that profile has this particular value of this attribute? And to do that, you just do exactly what, zero plus has that factor of the first attribute, second attribute, and then it would just give you much that would be a practical and that's what I typically do. But if you just wanted to see what the means were, very easily using a call to LM, that's another here's another way to get those means, the aggregate

Unknown Speaker  57:53  
function, Oh, and so, the R squared m6, eight,

Speaker 1  58:06  
and so LM is still trying to do its normal thing. And so it's saying, Well, you know, I, you know the R square is defined as the difference in how well you're predicting to become a variable based on your model compared to the intercept only model. The problem is, you got rid of the intercept So, but you didn't really get rid of it. You just set it to zero. And if you set the intercept to zero, now it's going to use that as the as the base model prediction. So it's going to get for that model. It's going to get zero for every single observation of wine. And if you do that, you're going to do very poorly. And guessing the minimum value for every observation is a bad guessing strategy. And so your model is going to do way better than that. And so you get a really high R squared, but that's it's totally deceiving. You have a high R square, but the only reason you have higher square is because you pick a really crappy model as a baseline to compare it to. I look at the other way of estimating. I adjust our square. So this is all an illusion here, based on picking a really bad base line comparison. If I

Unknown Speaker  59:22  
asked because of the stochastic variation.

Speaker 1  59:30  
Yeah, like there's, there are lots of observations that are close to zero, and some of the, for some of the many of those, you will actually do a better job guess the zero than you would for yes, maybe expected. But on the whole, looks bad to guess. And to put even a finer point on it, right? We know from this previous model, right, that these this condition, these conditions, aren't doing very much. But that doesn't really matter, because now that I've just got the means of the conditions in there, it's almost like I'm just guessing the grand mean observation. My model just says guess something close to point four every time, and because the overall mean of the given variable is close to point four, that's going to do you know, as well as the intercept over here. But guessing zero is going to do much worse than just guessing. So this is just to say, if you ever do include zero from there, right? Just be very careful not to interpret dr square, as you

Speaker 1  1:00:41  
already talked about that. Okay, so let's include, let's make this more complicated. So there was two different experimental factors in this. In this experiment, there was the cues factor, which I just talked about, but there was also this decision factor, did the decision agree with what you wanted that's coded one, or did you disagree with the decision? Not like yeah, and and so now I'm just going to include both of these in the same model. I don't need to do as dot factor necessarily for decision, because it's already just zero to 101, and so it's already coded the way I want I can just include it as its own down period. So now, right?

Speaker 1  1:01:33  
So now we get something very similar. It should be very, very similar for the factor we just looked at, but for the decision factor. Now we get a negative coefficient of about point one four, statistically significant. And so how to interpret that? Again, it's a mean difference the treatment effect saying, If we compare people who got a decision they like to people who got a decision they disliked, the people who got the decision they liked are about 14 percentage points less likely to attack institutional integrity of the court than people got people who get a disliked decision are more likely to attack the court. And now the intercept is different. Again, it's now something completely different, right? It's not just the control for the keys condition. It's it's a person who's in the no keys condition who got a disliked decision. Again, it's just the same logic. It's when everything else is set to zero, but setting everything else to zero now is disliked decision and no cues again, just remember intercept is always

Speaker 1  1:02:54  
so let's make it more complicated. You might think that there's an interaction between your treatment conditions. That would be a typical thing to do in multi factor experiment, and that's easy enough to do by time between the two variables, it'll automate enough again. Ellen is smart enough to do this. It'll give you both the dummy doubt versions of the variables, and it will interact my decision variable with both the dummy variables for the other factor. So now I've got my decision variable constituent term, I've got my 2q variable constituent terms, and I've got the interaction of the decision variable with each of the other constituent terms. So now what we're trying to look at is whether there's any evidence that the impact of the decision getting a dislike decision or a like decision depends on what party elites are saying about this, and you might expect that if party leads are really divided on the issue, that would amplify the effect of a dislike decision in this experiment, we don't find that. And so you can see that by looking at the interaction term. First interaction term is decision by factor one cues one. So this is how the effect of the decision changes when you get unpolarized particles relative to the control. See the effect of the decision is actually going the opposite direction, so it's getting smaller, but it's very close to zero and not this is how the effect of the decision is changing when you get polarized versus control. And here we see it's in the decision the direction you might expect is negative, which means amplifying that negative effect, but it's not significant, and it's not terribly

Unknown Speaker  1:04:53  
Different.

Speaker 1  1:04:59  
Question, the party cues are aligned such that you know if, if you if the decision was conservative, then liberals dislike it, and Democrats are saying they dislike it. That makes sense.

Speaker 3  1:05:15  
Then that an individual says that a Democrat, again, that individual preference is in line with the body cube always. In

Speaker 1  1:05:24  
this case, that's basically what's happening. We run different models sometimes. We run partisanship sometimes, but yeah, that's exactly right. So there's going to be some slippage between those two that's going to drive down that effect. There's the interpretation of that interaction make sense? And as always right? You can look at it the other way. You can say, you know, does the effect of getting polarized particles change as a function of whether you like the decision dislike it right? The effect of getting polarized part of cues when you dislike the decision is positive. Point 05 and it goes to zero when you get when you get a decision you like. So if you dislike the decision, this is zero. So this is zero, this goes away. We're just looking at this getting polarized. Party fees. When you dislike the decision increases your willingness to attack the court by five percentage points relative to not getting any views. But if you like the decision that goes again, it's not significant, but that would be a subs verification that makes sense.

Unknown Speaker  1:06:53  
Is there any maybe the

Unknown Speaker  1:06:57  
radical

Speaker 4  1:06:59  
explanation regarding the effect of the Q is equal to one versus Q is equal to two. That motivate effect is actually larger.

Speaker 1  1:07:11  
That's what we expected theoretically, is that if the parties are not sharply divided, then people are less invested in the outcome. There's, like, a motivational reason for it. It's like, if your team is sort of competing sharply against the other team, like everyone's very divided about it, you're more motivated to turn your preference into either an attack or defense

Unknown Speaker  1:07:40  
or, like, a social identity,

Unknown Speaker  1:07:46  
that would be the aspect that's what we find in the other experiment.

Speaker 1  1:08:02  
I Okay, just as the last thing right, you can add covariance to experiment, to experimental designs like this. So here I'm adding a bunch of control, of controls and quotes, because the first thing you might say is, like you randomly assign these things. Why are you using controls? And that's what you get with reviewers all kind. Literally just got an R and R like, Stop putting control. You randomly sign the fact, why are you doing it? And and the answer, which I, I push for a long time, even when I, when I was doing it for tenure, I got, like, a letter, a letter, one of my letters, includes control. I And so yes, it's true, right? If you randomly signed everything, and you're not worried about whether the random assignment works, you don't need to control for anything, because these are exogenous by definition. But right? What happens when you include additional controls? If there's decent control if they're decent, if they're variables that predict the outcome is that you reduce the unexplained error in the dependent variable, and that increases the efficiency of your treatment effect. So the more variance you can explain in the dependent variable, the more error variance you can suck out like a vacuum, the sharper your your estimates will be, or the treatment effect. Even though they're they're exogenous,

Speaker 3  1:09:30  
like something with random assignment, and then you don't have any of the controls you get, not clear differences between your two groups. Can you start then adding in controls to see if that explains or is that like, should you start with all of them and then you can kind of start filtering out.

Transcribed by https://otter.ai
