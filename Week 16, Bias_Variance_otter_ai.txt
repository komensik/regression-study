Speaker 1  0:00  
Over and over again, the square bias. So how directionally speaking, how off are we in expectation squared, and what we call the irreducible error, which is the portion of the dependent variable right, or the or the variable y, that is independent of the model itself, right? That's unexplainable based on the model. So we have those three different things. And the idea is, like, you know, this is fixed. We can't explain that, but we can potentially reduce this overall mean squared error by either reducing the variance or reducing the bias. And the issue that we're facing, and that we spent last time talking about right is that these two are actually often in opposition to one another, and so we want to reduce both of them, and it's great if we could do that as much as possible, but the problem is that the more we reduce the bias, or at least, if we push further and further and trying to reduce the bias, we end up inflating the variance. And so that's, that's kind of the issue that we're trying to solve. And we have this running example right where we've got a true model that looks like it's maybe qubit, or, you know, slightly higher, and we can fit a really low, very unflexible, linear model, if we want to, that's going to be biased, right? Because it's not capturing the non linearity of the actual data generating process. But it's very sparse, right? It's very parsimonious, and so it will have very low variance, right? It's not going to change much from sample to sample, but it is bias, right? We could try to reduce that bias by fitting a more flexible model, and that will indeed reduce the bias. And eventually, the more flexibility we use, it will reduce the bias to zero. But at the expense of variance, the variance will go up. And eventually, if we hit, if we use an incredibly flexible model, right, the bias will be zero. It'll it'll have gone to zero, but we will have a very high variance estimate. So what we're trying to do right is not just reduce or the in sample mean square or we kind of find the point at which we minimize the out of sample mean square root, that's going to be the point at which bias is is quite low, but variance has not increased substantially. So another way to see this right is that you know, if we if we have the.in line, the vertical.in line here, is the truth, right, of the complexity of the true underlying data generating process. As we increase the number of parameters to that point, the bias eventually hits zero. And the bias hit zero at exactly the point where our model parameters are equal to the true number of parameters in the model where the flexibility of our model matches the reality. That's when bias hit zero. After that, bias stays zero. Obviously can't go down anymore, but the more flexible model now starts increasing the variance at two to a point right where we're doing worse than we would if we had a less flexible model. Right. Once the bias is at zero, we don't do any better by increasing flexibility. We want to, we want to stop right when we hit some optimal point. And so that is the point right at which the mean squared error will be minimized. Anything after that is just going to increase variance with no compensating gain in bias reduction. Now, of course, we don't ever know where that true point is, so that's kind of the puzzle, man, we're trying to minimize this, but we don't know the truth. If we did, then we wouldn't have this would be an issue. So that's what we're trying to do, right? We're trying to we're trying to minimize this thing, which is the point at which bias is minimized, but variance is still small, but we don't actually know what the true point is. Since we're trying to figure it out. But the bigger point that we've been talking about right is that there is this trade off. There is this trade off if we reduce bias, we increase variance, necessary. There's another issue here that is more practical, perhaps, but it's important for most purposes that, you know, most things we're engaged in is that, you know, if we increase flexibility, we don't only increase variance, we also decrease interpretability. And so the more parameters you have in the model, the harder it is to make sense of in many cases. And we've, you know, had examples of that, right? It's, it's much harder to interpret even a quadratic polynomial model relative to a linear model. And as you add orders of polynomials, right? You know, an order five polynomial gets really hard to interpret. A three way interaction is very hard to interpret, and so on and so forth. Yeah. And so the more variables we have, the more parameters, the more complexity, and the more difficult it's going to be to understand what our model is doing and relate insights to other people. We're in social science, we're more often than not, trying to understand some underlying process in terms that a human like us can understand. And if we have a very high dimensional, high parameter model, it's gonna be hard to do. And so, you know, think about that in the context of like deep learning models that we're dealing with now, right? No one really understands chat GPT, this is an extremely high parameter model, like trillions of parameters, and we don't understand it. It's essentially an alien of intelligence for real. And so, you know, at the extremes, you can easily see what the problem is. Right? It might do really good at various things, but it might be a very successful model at making predictions or doing tasks that you're interested in but you won't, you don't understand it at all. And so if you're if your task is to understand something, then usually a more parsimonious model is better. Okay? So, all right, the upshot of all this, right, is that we're trying to figure out, you know, whether our task is prediction, whether our task is understanding. We're trying to come up with a useful model, and usefulness now, after this discussion, doesn't necessarily mean minimizing bias. It means, you know, it means something broad, right? And that could be that we're trying to optimize across bias and variance. It could mean that we're taking into account interpretability. It could be both of those. Okay, so, so if we're thinking about this in the context of choosing among different possible models that we might have, what are we, what are we going to do? And so the task in front of us, then, is to try to come up with a principled way to choose among different possible models that we might be considering using these sort of broader criteria that we're talking the problem is that, you know, to date, we've been using measures of model fit that are essentially in that are in sample models measures of fit that are measuring fit using the data we use to fit the model itself. So we can think about that as measures of fit that use the training data to assess how well the model fits fit to current data, right? So we have some sample we use o, l, s to estimate our parameters, and then we measure how well the model fits that exact same data that was used to fit the model to begin with. What we really want right? I think what this discussion is, is sort of telling us right is that what we want is to see how well our model fits to new data that that were not used to fit the model. The problem, right? The essence of the problem with this, this variance situation, right? Even easier to see here is that we're, you know, when we over fit the model and we have a high variance estimator, the problem is that we're, we're capitalizing on randomness that's not part of the true data generating process, right? This like squiggly line is picking out, is fitting randomness, and by definition, then it's not going to replicate a new data. And so our in sample model of bit right is going to go is going to be really low, but that's only because we're basically connecting the dots that can that way of connecting the dots is not going to replicate the new data. And so we're overestimating how well our model fits the data in new data, it's going to be much worse than in the data we use. And so we want to choose, you know, a better or worse model based on how well it fits in you to get it, you know. And one thing we've talked about so far is like a, you know, one worth while correction to sort of crude measures within the sample, fit is to use adjusted R squared, and that that is better, right, then using R square, but it's still not, not what we want. Ultimately, it's still a measure that's that's fit to to the data that was used to train the model itself. Okay? So we want something different. Now, obviously, like the sort of most straight forward idea would be to figure model on the data you have, and then you have some other data that you use to go out and test how well it fits right. So you estimate the model on the sample you have, and then you predict observations of new test data that the model hasn't hasn't seen

Speaker 1  9:58  
yet. We usually don't have that. Because if we had it, we would want it to include it in the estimation and so we need to sort thing about this problem, right? It's put this another way, right? If you had test data, it would, you would get better model estimates by including that test data in the estimation procedure. But then you no longer have test data anymore. So we have this kind of you know problem, where, on the one hand, we want data to be available to measure model fifth that we hold out rather than using to estimate, but on the other hand, we want to use all the available data to estimate our model so we get a low variance estimator. And so that's kind of the essence of the problem that we face. And so what's you know, what should we do about this? Well, what we're going to do is to hold out some of the data that we use to estimate the model. And so the simplest way to think about this would be, you know, we have an entire sample, and we're going to set some threshold, right? Let's say we're going to use 70% of the data to estimate the model, and then pulled out 30% as a way of testing how well that model fits new data. And so here this is just saying, you know, what's my sample size, multiply that by point seven, and then the floor function just says, find the largest integer that's less than or equal to that number. And so it's just going to say, you know, estimate the model on 70% of the data, defining a training sample, and then I'm using that to estimate my model parameters, and then I'm using the other observations for prediction purposes. This is just kind of like a simple procedure you could use if you wanted to do something like chat by and choose what percent you want to use to estimate, hold out the rest, then you can use the held at observations to predict. That's fine, right? Yeah, in a theoretical sense, dropping

Speaker 2  11:51  
like example or separate or instead of sampling it, it's just running from all form

Unknown Speaker  12:02  
you it also fair.

Speaker 1  12:11  
It's it's similar in the sense that we're taking samples from the roads we're just in bootstrapping. You would sample. You would take a sample that's equal to the sample size with replacement. Here we're taking Sam a sample of the rows. We're going to sample only 70% of the sample size, and we're not going to replace and the reason why we're not replacing is because we want to hold out some set of observations that are not going to go into the estimation, and then we can use those observations to predict based on the model estimates. So I guess what I'm showing but, but it's similar, I guess, in the sense that we're taking samples from the rows I'm just trying to show you is sort of, sort of like, you know, this is something kind of straight forward that you could do, but this is not going to be sort of exactly what you want. So there's couple different issues that you might immediately think of. The first is, well, how much data should you pull out here? I chose point seven to use for estimation and point three to pull down. But what I read like is that a principal choice, or is that just something that sounded good, not odd. The other problem, right? Is that here I'm randomly sampling a set of observations to use for estimation, the set to hold out, but I would get different answers if I if I got, if I took that sample again, right? Each time I sample, I'll get a different sample. And so each time I would do this, I would get a different answer, and that also seems problematic. So the two big issues are, well, what percent should I hold out, and what percent should I use for estimation? And how do I deal with this fact that I'll get a different answer each time I do it, and that's what the general sort of approach of cross validation tries to deal with. Let's think about sort of the trade offs here that are going to be that we're facing. So when we're thinking about how big of a sample to use for estimation, how big of a sample to hold out for prediction, we face a trade off. Right? The trade off is that the larger our sample size for estimation. And so if we use, say, 90% of our sample or 95% of our sample to estimate, and then hold out only a smaller percentage for prediction, the that's good in the sense that it, it's, it is, you know, sort of minimizing the variance in our estimator, because we're getting a bigger sample size. So if we maximize the sample size used for estimation, we minimize the variance of the estimator itself, which is good. We'd ideally like to use the entire sample to estimate, but we want to hold out something different. The problem. Problem is that the fewer the observations we hold out for prediction, the worse our estimates of the mean square error going to be. I think about in the extreme. If we used n minus one observations for estimation, we would minimize the variance of our estimator, and we would get, sort of, you know, the minimum variance estimates that we could get while still holding out some data for prediction, but we would only be holding out one observation for prediction, and our ability to predict that one observation is partly due to the goodness of the model and partly due to random factors associated with that idiosyncratic observation. So we're not going to get a very good estimate of the test mean squared error using only one held out observation, but we do want to minimize the variance of our estimator, so we're facing this trade off right. The larger the sample size we use to train the data, the worse our estimates are going to be for mean square error, but the larger our held out sample size, the more inefficient our estimator is going to be. So that does that make sense?

Unknown Speaker  16:06  
Yeah,

Speaker 1  16:09  
and just to just put a finer point on it, right, anytime you use less than a sample size, the sample size that you actually have to estimate your model you're actually going to be over estimating the test mean squared error, because your true estimator uses sample size of N, and you're only estimating your parameters on sample size of n minus something depending on how many observations you hold out. So using less than your entire sample size biases your estimate the test mean squared error upwards, it creates a pessimistic portrait of how well your estimator would actually do given your sample size. But there's a fundamental trade off here, right? Because you need to hold out some observations in order to do the prediction questions about about that point. Does that make sense?

Speaker 1  17:11  
Okay, um, so, so keep that in mind. Okay, the other problem that we're facing is that we, you know, if we just do this one time, we're going to end up with a particular set of model estimates and a particular testing squared error, but if we took another random sample, we'd end up with something different. And so we want to deal with that in some kind of principled way. And so that's what, what's called K fold cross validation. He tries to deal with this problem by doing this multiple times on different subsets of the data. So we're going to do this k times, where the K fold cross validation comes in. And each time we do this process, we're going to hold out a different sub portion of the data, right? So the first time through, maybe we hold we hold out. I guess we're doing this five times, so we hold out. So for five fold cross elevation, we're going to do this five times, and each time, we're going to hold out 20% of the data, and then we're going to estimate our model parameters on the other 80% and then the next time we go and do it on iteration two, we're going to hold out the second 20% of the data and estimate it on the other 80% third time, we hold out a third, 20% fourth, 20% good, 20% so so for five fold cross validation, right, we estimate our model five times. Each time we estimate it, we only use 80% of the data, and we hold out a 20% block, and use that 20% block for prediction. So we estimate the test mean squared error using that held out 20% we estimate the model parameters using the other 80% but each time, it's a different 20% that's being held down, and then we just average over these to get an average estimate of the test means square here. Okay, I'm sure there's some point of confusion here so far. Yeah, feel free to ask questions.

Speaker 1  19:15  
Okay, so this is five fold cross validation, but we can do any full any number of folds, right? If we did 10 fold cross validation, we would do this 10 times, and each time we hold out 10% of the data. And if we did 20% we would hold out 5% of the data. And again, we're keeping in mind this idea that we have this trade off between the larger the held out block, the more variable our estimator is, but the better our predictions are going to be about test means for error, because we're facing this, this trade off,

Unknown Speaker  19:48  
yeah, this will tell you like, the difference between like, different thoughts, with telling you about the portion that you you.

Speaker 3  20:05  
What happens by 20% and I hold out for testing, and you just take a different 20% but it would tell you about 20 versus 40% No, that's correct. Yeah. So yeah, exactly.

Speaker 1  20:26  
So, so, so doing this multiple times, right? Allows us to average over the different the different estimation folds, and then get you know, instead of relying on only you know, if you only did this, this, I guess you know, if we only did it like using this this procedure, right? So to hold out the first 20% use the latter 80% that's going to give us a different estimate of the test mean squared error compared to this one. And so by doing it all five times and averaging over, we get a better estimate overall than we would only doing one. But you're right. It doesn't tell you whether it's better to hold up 20% to 10% percent, 10% 40% that's a separate choice you have to make. I'll talk it a little bit. Other Other questions.

Speaker 1  21:18  
How is this study different? It's not machine learning is just the application of this. So machine, I mean, machine learning is a very sort of vague term that doesn't necessarily have a straight word definition. Eight fold cross validation comes out of the you know, is more has been more popular historically within this sort of statistical learning machine learning tradition, because that tradition has been more focused on prediction than theory building historically, right? That doesn't have to be true, but because the if you think about everything we've discussed so far, right, it's very much centered so far, at least, around the idea that we're going to try to get really good predictions of new data, and we don't care so much. We introduce some additional bias so long as we get good prediction today, that's very much like historically, that's been a task that machine people working within the machine learning tradition have been more focused on in the econometrics tradition, where it's much more about theory testing, so seeing if one variable is related to another to another variable. So saying this is machine learning is not wrong, but it's it's also not very informative, because this is just a procedure for trying to see if a model predicts a deep, you know, predicts an outcome well. And so you might want to do that regardless of whether you're working in like econometrics or machine learning.

Speaker 1  22:57  
I guess my, maybe my broad point would be, forget about sort of, you know, the distinction between machine learning and econometrics. It's just not like a useful distinct thing, except to say, like, you know, we have a class called machine learning. You'll do sub topics that you won't do in this class. But we're, you know, I'm just not sure it's very useful way to organize things. Yeah,

Speaker 2  23:24  
so when you're trying to figure out how many narratives you should be doing, if you are getting like, a huge difference between the data that you're doing, let's say this section one, section five. Is it an indication that you need to have more folds to get a filter estimate, or is it just an indication that there might be some outliers in the data? I guess very

Speaker 1  23:56  
Yeah. So, so this is this issue will come up in a second again. As you increase the size of the folds, so as you reduce the number of held out observations, you increase the correlation across the folds, because they share more data, right? So the amount of shared data across these two right, is a lot more, is a lot less than it would be if you were only holding out one observation per estimation. So there's two different things that are happening. When you're increasing the size of the folds, right, you're reducing the variance of the estimator. You're reducing the variance of the estimate of a sense of an increase in sample size, but you're also increasing the correlation of bolts with each other, and so, so that that is a trade off that people have talked about in literature, but I get to that in a second if you have more questions. Okay, so. Again, feel free to jump in at any time, but I'll make sure we move forward so we can figure out what's confusing. So here's just an example. So is there our long running regression of opposition to European integration on a set of independent variables? Here I'm going to use this useful package in our parrot to do this basic cross validation procedure, right? So I estimate a model using LM. Then what I've got down here is I'm going to use this package parrot to estimate the cross validation mean squared error, r squared something down there. So the first thing you need to do is to this train control function is basically defining, for this package what it is you want to do, right? It's telling you what you want to do. And so what I'm telling it is that I want to use cross validation method equals CV, and the number of folds I want is so this is just saying defining a set of control parameters that are going to tell the function what to do, and what I'm telling it to do is 10 fold cross validation the second command train. So this is the control set of control parameters for this function. So I'm going to train on the formula for this model up here, I'm just taking the formula for this model, plugging it in. Here. You could write the formula out if you wanted to, but telling it what data I want to use to estimate the model, telling it what method I want to use, LM, and then I'm just telling it what to do, right? And when I tell it what to do. It just wants an object from train control. So I told in train control, I said, Do 10 full cross validation. And then down here, I just tell it that that's the thing I want to do. You could define multiple ones of these. You could define five whole, 10 full, 20 full, and then run this command multiple times if you wanted to. But for this, for these purposes, I'm just doing it once with 10 fold, but again, formula, data, method, and then the control. So what is this going to give me? Well, it's going to do exactly this, and then it's going to spit out my fifth statistics that's going to use that model, but it's going to do it 10 different times, holding out 10% of the data each time, estimating the model on 90% of the data and predicting the remaining 10% and then after it's done that, all 10 times, it's just going to average these fit statistics across the 10 soles. So here's the root mean squared error, the mean squared error that we might be interested in. But now it's not just the insight. It's not the in sample one, right? It's the estimated out of sample or test mean square error, right? It's not it's no longer subject to this problem of over fitting the data used to estimate the model, because it's always generated by predicting data that wasn't used in the estimation procedure. We can do the same thing for r squared. So we can calculate an in sample R square for each of those folds. I'm sorry, not an incident. We can calculate a test R square for each of those folds and then average it. And now we've got an explained variance that is for new data that was not used to estimate the parameters the model. This is mean, absolute error, I guess,

Unknown Speaker  28:31  
double check with what that is. That's what it is,

Speaker 1  28:37  
right? So, so basically, what we've done right is it's the same thing we're always doing, but instead of just pulling out the mean squared error and the R squared from our summary. LM, right, we're using a, you know, a second set of procedures to get a set of fifth statistics that are not subject to this problem of over fitting the dates. Use desk. I

Speaker 1  29:10  
S. So you can extend this right in any way that you want, right? And so, you know, K is a parameter that's something that you have to choose. We can extend it all the way up to k equals n, such that we have n fold cross validation, where n is the sample size. If that was the case, we would be estimating the model n different times, and each time we'd be holding out one observation. So we would estimate the model n times, each time on n minus one data points, and we would just go one by one through the data, holding out one observation at a time, and then averaging over those n different estimations. The only difference between that and what we typically do is that we would end up we would still end up with n different predictions, right a prediction for each observation, but that. Prediction would be based on a model where that observation was not part of the estimation process. This should sound very familiar, because this is just leave one out regression. We've already done this. And if you do exactly this, if you estimate the test mean squared error where k using K fold cross validation, where k equals n, then you've just done what's called leave one out cross validation. Leave one out cross validation is just eight fold cross validation, where you only leave out one observation at a time. And so to do if you want to do that right, it's very easy. The only difference here using this parrot package is you just say method equals L, O, C, B for lead one out cross validation, everything else is the same, and you'll get the statistics for that approach as well. One nice thing about lead one out cross validation when you're doing o, l, s regression, is that there's actually an analytic solution for it. So I mean, I just showed you how to do it, so the character package will literally go through and estimate the model n times. But it turns out that with OLS, the one out cross validation test mean squared error has an analytic solution, which is very surprising, but turns out to be true, and this should also look very familiar, right? Because it's just an HC three adjustment. And if you, if you remember when we talked about leave one out bootstrapping, it's essentially, it's the Jackknife estimator, right? We said that it's essentially the same as a heterosity consistent estimator. And so you can see this again in this formula for the test mean square error. So quite literally, right? If you just plug in, you know, the residuals from your OLS regression and the hat values that we talked about before, into this formula, you get an estimate for the test mean squared error that is identical, like, literally identical to what you would get if you went through and did the process of leave one out cross validation, estimating the model. And you don't need to do that. You can. It's not like it takes very long to run on this either, but leave one out cross

Speaker 2  32:21  
validation is unbiased, but the formation

Unknown Speaker  32:26  
always has sufferable

Speaker 1  32:31  
Yeah, so this is the this is the difficult situation, because there's no good answer to this question. So, yeah, so, now we've assuming maybe I can stop for just a second and see if there's any confusion about the basic idea of tables validation itself. Do we understand that conceptually? Okay, so yeah, then the exact question that we asked, well, how to choose 10? You could choose 510, or N. You know, it's like a lot of people choose five, a lot of people choose 10, and a lot of people do leave one out cross validation. And if you have, you know, a sample of 10,000 that you're like 510, or 10,000 that seems like a big difference. And so, what do we do? And so, yeah, there's exactly this issue right, that we face, if, because whenever we do this kind of cross validation, by holding out some of the data right for prediction purposes, we are creating a bias in our estimate of the test speed square error, and it's an upward bias. We're over estimating the test speed square error because we're using an estimator with a sample size that's actually smaller than our true sample size, so it's higher variance, so we have a higher SV squared error. So any time we use, let you know, use the sample, we hold out any of our data, we have an upward bias and speed error, however, right? So that tells us to push K towards n, so we minimize that bias. However, as k goes to n, that bias goes to zero, but the variance of the of the estimator for the testing square error also goes up. And why does that go up? It goes up because there's a higher correlation across the poles. Okay, when you have when you increase k towards N, each of the folds that you use, each of those k folds now shares a larger percentage of its data, and that additional correlation creates a higher variance estimator, just like multi co linearity, like a regression complex and so this higher correlation across folds creates A higher variance estimator of the test speed square error. And so our bias goes down, but our variance goes up or estimating the test speed squared error. But again, bias variance trade off even in the context of trying to correct for bias. Variance trade off because of this issue. It's sort of become a common part. Practice to use 10 folds of your data. So hold out 10% of data at a time, and that's the default from much software. That's what you'll see quite often in the literature. When people do this, you'll see 10 fold cross validation. But there is a debate about this, and it's exactly for this reason, like the common wisdom. Well, I mean, there's two reasons. One reason is exactly this. There's this trade off between variance and bias. But the second reason is that, you know, once you're outside the realm of linear models, estimating your model n times can get very computationally expensive and and so you know, if you have a really complicated model. Let's say it takes an hour to estimate, then estimating it 1000 times is not feasible. Most of the time not feasible. So using a smaller number of folds allows you to do this without, you know, spending a year trying to estimate your model. So 10 folds has just kind of become accepted practice. But there is a debate about this that I don't have an opinion on one way or the other, about whether, in practice, it's actually the case that this increase in variance is enough to offset the bias problem, or whether leave one out process. Sorry, I chose that the other way. The debate is whether it really is the case that we should be so worried about this increase in variance that we're willing to sacrifice the upward bias in order to reduce the variance. And so the argument would be that in most practical circumstances, leave one out. Cross validation is actually the best strategy to use, because the increase in variance is not enough to offset the loss and bias. I don't have a worse than that fight. I don't really know what the right answer is, but if you try to dig into it, you'll find people arguing about this. So I don't know, I don't know what to tell you, right? You know, if you use 10, that would be something that people would generally accept as reasonable. If you leave one out cross validation, I imagine it would also be seen as acceptable do anything in between, probably also accept. You also see five quite a bit. And five is sometimes the default as well. But I would say probably go up to 10. That's not very satisfying.

Speaker 1  37:29  
Let me try to say something more useful to you. If you use 10 fold cross validation as a default, that's probably good. You're probably fine. So if you want a default and you want to just want to just do something like until you learn something you know, something better, or go take, you know, de Marquis machine learning class or something 10, full cross validation to do. One last point that's you know, maybe worth noting is that when you when you have a pay that's very far away from your n. So let's say it's 10 fold. If k equals 10 the final estimate, right? Partly we're trying to correct for the fact that any given fold is going to be different from any other. But the folds that you choose are also partly arbitrary. If you take the first 10% second, 10% third, 10% Well, why did you break it up that way? You could have rearranged your data and then the first 10 second 10, that would be different. And so potentially, there's lots of different ways to break up your data into 10 folds. And so the question is, shouldn't you take that into account? STEM now? And so there's repeated cross validation, where you can do the cross validation procedure at M different times, where each of those m times, you do a different set of 10 goals, and then you average over everything. You average over the k, and then you average over the pm versions of k cross validation. I've never seen anyone do that, but that is a thing. I do things about getting the

Unknown Speaker  39:12  
number of

Unknown Speaker  39:22  
predictions. Predictions, but what does it do? Co

Speaker 1  39:30  
efficient so, so so far, it does nothing for co efficient estimate. So let me just go back to this example. So in this example, right we've estimated a model, and we've got a set of coefficient estimates, and we report those coefficient estimates, and we should report those coefficient estimates from the simple OLS regression. What we're doing here is just trying to get better model measures of model fit. That's it. So if we. Summarize this L, M, object, m1, we'll get a root mean squared error and an r squared both of those will be calculated on in sample fit, which is data used estimate the model. And we know from our discussion over the last few days, right? That that's over fitting the data, right? It's not a good it's not a good estimate of what we would do if you predicted for new data, because we're capitalizing that chance variation in the in sample data. So all of this cross validation step is just getting us better measures of model fit that don't give us overestimates of how well we would do predicting new data. And so this root mean square error from the cross validation is telling us the expected squared error? Well, in this case, it's the square root of the expected square error, right for predicting new data. If we used our model up here to predict newly sampled data, what would the model fit look like? What would the R square look like? What would the average square error look like? Does that make sense?

Speaker 4  41:07  
For each poll, we will get a different estimate. Just want to

Speaker 1  41:15  
read like, yes, yes, yeah, that makes sense. And the answer is that you don't want to use those coefficients as your estimated coefficients. You want to use the coefficients that come from the overall sample, because your estimator is uses the full sample. The best estimator you can use is the full sample. But to get these predictions of out of sample, predictions of new observations, you need to hold at least somehow, but you wouldn't want to use those held out data parameter estimates as you reported us. Yeah. So So, so far, our task has been to deal with the problem of, you know, saying something about model fit, given that we know that model fit is a combination of truth and randomness when we're dealing with only in sample model fit. So we're trying to develop a procedure that has the advantages of predicting new data, given that we don't really have new data. We only have the data we have. So we're pretending that we've got sort of, you know, training data and test data by remedy, you know, by breaking our data apart and and using that, using that to generate this sort of pseudo added sample prediction. Okay, but so that's just for model fit, and we would, it's totally it would be a good idea to report cross validated measures of model fit as your measures of model fit. That's not very common, actually, still in political science, but it's better in the sense that it's hard to imagine that it wouldn't be better given that you know that your in sample measures of fit are, you know, inflated in some sense. So it would be good practice to use your test mean squared error and your test R square through cross validation as your reported measures of fit. But we might want to do more than just report better measures of fit. We might be interested in trying to choose between models right or choose among models. And if we're trying to choose which model is better, we want some criterion. And the idea would be that if we're just using R squared or something like r squared, we're using a criterion that's inflated by this problem over fitting the data used to estimate the model. And So one possibility is to not only just use test mean square error, or cross validated error as a way of, you know, as a better measure of fit, but to use that to choose among models that you might be considering. So if you estimate a series of models with different sets of parameters, different variables in the model, some, you know, with an interaction without an interaction with a polynomial, without different subsets of parameters. You can estimate all the models you're considering, calculate the K fold test mean square error for each and choose the one with the lowest mean square error test means per error. And so this would be a reasonably principled way of choosing among a large set of models where you don't have a strong sense of which one is the best one to choose, but you want a principal way of choosing. And again, this is better than just using, say, in sample r squared, because a lot of those models are going to be over fitting the data the in sample data, right? You want to use cross validation, so that you're choosing based on how well it predicts new data not used to estimate. Okay, let me pause again and take questions. Well.

Unknown Speaker  45:00  
Choose about models using the criteria the prediction accuracy as material. Does that mean that the most committed model is the one best reflecting the true relationship we're

Speaker 1  45:15  
measuring? Yeah, not necessarily, right. So, so this gets back to this kind of different traditions, one of sort of the machine learning tradition versus sort of the more econometrics or into tradition. It's not necessarily the case that the model that has the best predictive fit is the model that you always want to choose right, because you might have other purposes beyond just predicting the kind of variable you might be trying to test a particular theory, right? I think about if you run an experiment to try to test whether a drug works or not, your goal isn't to predict, you know, to put in a set of variables that best predict the outcome, right? Your goal is to see whether giving a drug to someone works. So, so there are plenty of cases where you know using test mean square errors as a model choice procedure is not relevant because you're just not trying to do that. You have a model you're trying to estimate that's theoretically driven and and you're just going to estimate that model no matter what. Now, having said that, right, it still makes sense, right, to use cross validation to report your measures, model bit. But you might not use those models, measures, model fit, to choose among models. You might just, you know, just like you would typically report an R square to say, like, you know, my experimental treatment explains 10% of the variance in the dependent variable, right? You might just use cross validated R square to get a more accurate picture of explain variance. But you might not use that to choose to say, like, I'm going to estimate this model rather than that one. It would just be a standard report database model fit. So, yeah, so there's going to be some cases where you really are trying to choose among models based on model fit, and there's going to be maybe even more, plenty more scenarios, maybe the vast majority of scenarios you deal with, where that's not your goal. Your goal is to estimate a theoretically driven model, and really what you're interested in is just reporting accurate measures of model. Does that help at all? So it depends on the task. It's not always going to be the case. Maybe even for our purpose, is often rare. There's going to be a case where you're just kind of agnostic about, you know, what variables go into a model, and you're just trying to find the one that best predicts. That's a case that happens sometimes, but not necessarily the most common social science. Okay? Yeah. So two more selection criteria that are sort of somewhat related, but it's different in terms of what they do that you'll see quite often, so I'll just mention them, but it's a different class to go into depth in these you'll often see the AIC and the Bic. So this is the AP information criterion and the Bayesian information criterion. They're very, very similar, as you can see by the formulas. But the basic idea is to penalize model fit based on how many parameters are estimated. And so this k plus one is the number of estimated parameters in both of these formulas. The only the thing that's going to be confusing for you over here is that is the log of this L with a hat on it, and that's the log likelihood at the maximized likelihood value. Since this is not maximum likelihood class for basing class, we're not going to get into that. But if you went on and took an advanced regression class, that would make more sense. But basically what this is doing is just, this is a measure. This is sort of like the sum of the square residuals in OS. And this is a penalty parameter for the number of parameter, penalty value for the number of parameters estimated. The only difference, real difference between the two is whether you're you're multiplying the number of parameters estimated by two or the log of n. And so, except in really small sample sizes, the Bic ends up exerting a higher penalty for additions, for additional parameters, but you'll see these reported quite a bit. In terms of model choice, you want to choose the model with the small value of AIC or BIC, so small values indicate better fit. The other thing to notice is that because you're using the likelihood you actually need, there needs to be a likelihood model present for you to calculate these things with ordinary least squares if you assume normal errors, the model becomes equivalent to a maximum likelihood linear regression with a normally distributed dependent variable and. So you can calculate a, i, c and b, I, C, or O, L, S, regression models, but it's making this assumption of normal errors, because you can only get a likelihood if you have a probability model. So some assumption there. So you'll see, you'll see these quite often. The key thing for again, you'll you would go into this in more depth, if you take like Daniel space, even course, or something or a maximum likelihood course. But the key point for our purposes is, if you see these right, they're measures of model fit. You can compare models using these values, and lower values mean better fitting models, and they penalize for additional parameters. If you want to get these in R It's super easy. There's built in functions for a CIC. The actual value doesn't mean anything really, you know, useful. It's just you're comparing different values for different models.

Speaker 1  51:00  
So, so to this point, I maybe just summarize, again, our key problem that we're facing right is that when you add a bunch more parameters to a model, it fits the data better, because you're losing degrees of freedom that we've we sort of talked about, sort of the nature of that problem, and the fact that, because of that, right, if we report standard measures of model fit, or we use those standard measures of model fit to choose among models, it's possible that we're sort of exaggerating the extent to which our model is actually explaining, you know, is actually doing a good job predicting the data, because if we applied it to new data, it would do a bad job. And so these procedures across validation are ways of trying to get more accurate estimates about of sample prediction, instead of using our standard r square. So that's totally reasonable. That may be one thing we want to do a second related problem is that we might not just want to have better measures of model fit. We might actually want to integrate this idea of bias, variance trade off into the estimation procedure itself, such that we want to, for example, get rid of variables that are not doing a good job predicting new data. It might be that, let's say we've got like, 1000 different independent variables that could go into a model, and we know that that's way too many, but we don't really know which ones are good and which ones are bad. And we want a principled way to select which variables to retain in the model, and we want to do it based on how well we can predict the dependent variable in new data. Procedures related to what's called regularization are our methods for our principled methods for doing that. So what we're going to do is, instead of just kind of after the fact trying to adjust for over fitting in the data, we're actually going to integrate into the estimation procedure itself, a way of penalizing models for over fitting. We're going to try to reduce over fitting in the process of estimation itself, and so we're going to do this by placing different kinds of constraints on on on the parameter space during estimation itself, and by their very nature, these constraints are going to reduce the flexibility of the model. By reducing the flexibility of the model, it's going to reduce the variance of the estimator, but it's going to come at the expense of increased bias. So what we're doing in regularization is constraining the parameter space. We're saying we're only going to allow these parameter values to be acceptable because of that, right? That, I mean, that's by definition, introducing bias, right? We're constraining what, what you know, what's possible to observe in the data, but right that very the very nature of that constrain, right by reducing flexibility, also reduces the variance of the estimator. And so what we're trying to do with regularization is introduce some bias in order to reduce variance and hopefully do better overall in terms of predicting new data. By doing so, okay, and you know, so that's one reason you might want to use this. The other reason is that you might be worried, right? Even if you could include, say, 1000 different predictors in a model, you might be worried that there's no way to interpret that model, right? It would just be too complicated. And so you want to create a model that has some subset of the overall variables that would be more interpretable. And so sometimes these methods are also used to increase interpret ability, right? You increase bias in estimation, but what you get in return is increased interpretability of the output. Yeah. Um, so let me go through sort of the basic idea, and then we'll pause for questions. So a standard way to to do this is to do a penalized version of least squares. And penalized least squares introduces an addition to the loss function that analyzes the model for some some function of the parameters. This is confusing at first, but hopefully, think about OLS, and what is OLS doing? The OLS, the OLS estimates, right are the estimates of beta that minimize the sum of the square residuals. That's just the definition of O, L, S. So the loss function, right is saying, you know, the loss function grows as the residuals get larger, right? So we're trying to find the set of estimates that minimize the sum of squared residual what we're going to do, forget about that in just a second, we're going to minimize a new loss function that starts with the sum of the square residuals, but adds this additional thing over here, and this is a penalty term, and that penalty term has two key components right? The first component is some function of the coefficients themselves. And the second component is a term we'll call lambda. That's a tuning parameter called the tuning parameter. And that tuning parameter is going to be the size of the penalty that we exert for beta coefficients that increase this function. Now the question is, well, what's that function? And we'll get to that in a second. But for now, the idea is that we're augmenting the OLS loss function. We're starting with the same thing, but then we're adding a penalty for some function of the coefficients. And the question is, well, what kind of penalty do we want? That's what we'll talk about. So lambda is a tuning parameter that has to be chosen by the researcher, and we'll talk about that tricky problem, but it's going to be basically, it's a weight, right? It's how much of a penalty Do you want to exert? In addition to the standard OLS, larger lambda means larger penalties, right? We're trying to minimize this overall loss function, and so the larger lambda is, the larger this penalty term will be, and the more that model will be, we'll have a bad fit for estimation purposes. The other thing you might wonder about, which is reasonable is why we're dividing the sum for residuals by two over n. That's actually not necessary, but it's the state. It's standard for the software you'll probably end up using. The reason you divide by n is because that standardizes the sum of the squared residuals for sample size. So if you didn't divide by n, SSR just goes up every time you add a new observation. Okay. And the problem is that makes the tuning parameter conditional on the sample size. And usually you want to be able to compare tuning parameters across different estimations, regardless of the sample size. And so dividing by n essentially standardizes the thing that the tuning parameter is comparable across different models, different estimations. Dividing by two is definitely just a convenience norm, because when you take the derivative of this function, this is a square term, and so you bring the two down, and that cancels that two. And so that's just, that's totally a norm. It means nothing. Doesn't matter, but it's convenient mathematically. And so you'll see that again in a second too. So this is not necessary. And in fact, when I show you ridge regression in the mass package, it does not divide by 2n so that it's actually not comparable to the other software, without sort of taking into account the tuning parameter has a different scale. But in the package I'll show you to focus on the standard. The default is to divide by 2n and that's one, right, standardizing by n, and then it's using this norm of dividing by two to get rid of the square when you take the derivative, okay. One other thing to note right is that in this loss function, the scaling of the betas is going to matter, right? So if, let's say you have a model where you have income measured in dollars, and your coefficients are like, you know, 100 man, if you divide you know income by, you know, 1000 to get into 1000s of dollars, your coefficients will get smaller, obviously, right? Yeah, the coefficients will get smaller, right? But nothing about your model has changed. It's relevant. And so the scaling of your VA, your variables, matters for these regularization procedures. And so you need to be very careful about that if you change the scale of your variable, so you can end up with a very different set of estimates, because that scaling affects the penalty term. And so usually what you want to do is standardize all your variables before doing these procedures. Okay, so that's all very introduction probably doesn't make lot of sense yet. So let's talk about the two main examples of this that you'll see quite often, I guess three. Okay, so the first that you probably already heard of that are coming across at some point is called lasso, which stands for least absolute shrinkage and selection operator. Why people call so? And you'll see that it's just an instance of this, right? It's got the same form. But what we're doing is we're making an assumption about what the function is. So it's the sum of a function of the betas, and the function we're going to choose is the absolute value

Speaker 1  1:01:03  
the function we're choosing. This is the general penalized regression format for lasso. The only thing we're doing is just choosing a function, and the function we choose is the absolute value of the betas. Now, why does this make sense? Like hopefully this will actually, actually make sense of what we were just doing. Now, the penalty term says if, as the betas get larger in absolute value, this penalty term goes up, which means, again, since we're trying to minimize this function, makes the function fit worse. This loss function gets larger as the size of the betas grows, because it's an absolute value. And so what lasso is saying is that we're going to penalize models with large values of beta and so the other way to think about that is that model all else equal, right? Models with smaller betas in the sense of being closer to zero, will be better fitting models. So that's introducing bias, because an unbiased estimator, OLS only uses SSR, we're introducing bias by saying that even if SSR goes down, we're still going to penalize the model if it has a larger value of beta, all else equal. So models with smaller values of beta closer to zero, all else equal, will be better fitting for lasso than other kinds of model. And so in practice, what's going to happen is that all of your coefficient estimates will be shrunk towards zero, and some of them for lasso will be shrunk all the way to zero and just eliminated from the model estimation altogether. And I'll show you why that's true in a second. So the lasso is a particularly good choice when you're trying to reduce model complexity, because in addition to shrinking some coefficient towards zero, it just gets rid of some altogether by shrinking them all the way to zero. Let me show you another one. So the other most common version of analyze regression is ridge regression, and the only difference between Ridge and lasso is that the function we choose is the square of the betas. So again, we're penalizing coefficients that are larger, further away from zero. But instead of using an absolute value in the sum of the betas, we're using a square. We're squaring the betas the surprising difference between lasso and Ridge is that while lasso not only shrinks some coefficients to zero, it also eliminates some entirely by shrinking them all the way to zero, Ridge doesn't do that. Ridge is this is a function that smoothly shrinks towards zero. And so if you run ridge regression, you'll end up with all the same number of variables. They'll just be shrunk towards zero in terms of their coefficients. If you run lasso most of the time, you'll get both shrinking towards zero and elimination of some subset of the variables. That's super surprising to me, at least. So why is that? And the explanation is actually really interesting. So you can visualize this in two dimensional space. So if you have a model with only two predictors, and you're estimating beta one and beta two, you can think of this point up here as the OLS estimate. So here's the OLS estimate for beta one and beta two. What are we doing when we're doing either lasso or Ridge is that we're putting in place a set of constraints on the parameter space. So what lasso effectively does is impose a box constraint where the final parameter estimate for lasso has to be somewhere in this this region. So here the OLS estimate, here is where it has to be. These are in difference curves. So these are curves of equal amount of dip. And so what Laso is doing is it's saying, find the value of beta, right? That is not a. Only the smallest, you know, mean square, not only the smallest s error right of prediction, but also falls within the constraint. And so it's going to be the first place that an in difference curse curve touches that box constrain. And because it's a box, it has points sticking out. And so the first place it touches is almost always going to be on an axis, and an axis is just a place where one of the coefficients is zero. Rich regression places a spherical constraint on the parameter space, and because it's a smooth shape, the place where the first place where an indifference curve touches, that constrain will not usually be on access. And so Ridge ends up giving you, keeps all the variables but shrinks them towards zero based on this constraint, while OLS not only shrinks towards zero, but also eliminates some variables entirely because of its sort of pointy structure boxes I again, that I find that very surprising, but it's an interesting sort of difference between the two. And so the upshot of that right is that if you're if you're trying to eliminate variable from the model, you're trying to use subset selection to get a more parsimonious model than last that was your choice, because rich is not going to eliminate variables from the model. It's just going to shrink them. Fix either. OK, so let's, let's pause and sort of see if we've got basic logic Here. What's confusing about this?

Speaker 1  1:06:42  
So we can combine them, and that's called an elastic net progression. So you might say, one might say, Why do either one right when we can do goal and again, and it's called elastic net. And so here the the additional complexities that we're adding another parameter. So again, we start with our LS, and now instead of having either lasso or Ridge, we use both penalty terms and we mix them together. We use a mixture of those two penalty terms with the amount, with with the mixing determined by a second parameter, alpha. We've got two tuning parameters now, lambda and alpha. Lambda determines the overall size of the penalty that we're using, and alpha determines what proportion of that penalty goes towards Ridge, what proportion goes towards plastic. So, you know, you might think this is like kind of crazy, maybe, but notice, right that, like lasso and Ridge are the extreme ends of the continuum. If you choose alpha equals one, this entire thing drops out, and you're left with lasso regression. If you choose alpha equals zero, this drops out. You're stuck with ridge regression. And so the question, yeah, I mean, the reasonable response to this is crazy would be, well, isn't it crazy to choose one of the extremes? Why not choose something that's not at the extreme of those two possibilities? Convincing that is, but that would be the argument. And so, you know, that makes it more complicated, because now we have another parameter to choose. And again, the issue with all of these methods is that neither lambda nor alpha is something that you can that you know for theoretical reasons, it's something you have to choose. So we're going to get to that in a second. We have to go to five more minutes. So keep going. We're going we're going to get to that problem of choosing these parameters in a minute. So it's not like I just won't leave you hanging on that. But let me just show you real quick, some examples. So if you want to do ridge regression, the mask package in R has a function for ridge regression, LM, dot, Ridge. So here I'm just estimating, you know, I'm estimating that sort of standard OLS model, but I scaled all my deep, all my independent variables, to have mean of zero, standard deviation of one, and then I'm just going to use that exact same formula down here for bridge regression. The only difference, well, function is L on dot ridge. But the only other difference is that I have to specify what lambda is. And again, ridge regression has a tuning parameter lambda. You have to specify it. It's not given. And so here I'm just choosing 100 don't worry about that killer value for now. But something has to go in there, right? And. And what do I get? So here I'm giving you the OLS estimates and the ridge estimates. And so these are our standard OLS and here's the ridge. And you'll notice that in most cases, right, we're getting a shrinking of the coefficient towards zero, so movement towards zero, with the exception of economic condition household, which is about the same as it was previously.

Speaker 1  1:10:30  
So simple enough to do. And you might have questions for now. I'm just kind of giving you an example of how this might look to practice. One other thing we might think about is, well, what happens as you change the values of lambda, so here I put in 100 but what if I chose different values of lambda? But let's, let's just look at the coefficient for age and see what happens as we go from lambda equals zero, which would be no penalty, all the way to something large, like 10,000 and what you see is a smooth decrease in the size of the coefficient towards zero. All I'm doing here is increasing the size of the penalty that's exerted for having a value of the coefficient that deviates from zero. And so inevitably, what happens is I increase lambda. As I increase the size of that penalty, the coefficient shrinks towards zero, and this will ask until it goes all the way to zero as as this goes to infinity, but it's A smooth decline.

Speaker 1  1:11:37  
A second. Any questions over, yeah, small okay.

Unknown Speaker  1:12:01  
So since then, one more time, sorry, backwards, or is it something

Unknown Speaker  1:12:13  
necessary and

Speaker 1  1:12:19  
later, like other concepts, so lasso Ridge, elastic net. Those are all methods of regularization. So we're thinking, Yes, so the term regularization, and again, you might hear this, you might hear that term more or less in the context of lasso or elastic net, but the idea is that these are different methods of doing regularization, and so the broad topic is regularization, and the different methods we might use to do that are lasso Ridge, elastic net, or even basing bridges.

Unknown Speaker  1:12:59  
Good I United States,

Unknown Speaker  1:13:03  
but it means that we almost always have to standardize

Speaker 1  1:13:09  
our data. Yeah, exactly yes, because, and again, the reason is, is because, right, the penalty is just a function is just like the absolute value, for example, of the size of the coefficient. And so if, for example, you know, you have one variable that's coded on a zero to a million scale and one variable that's coded on a zero to one scale, the penalty on the zero to a million is going to be a lot larger, all else equal. And so, but it's artificial, right? So you want them to be on comparable scales. That's a significant limitation.

Speaker 1  1:13:51  
Okay, we're out of time, so we will pick this up on Thursday. And yeah, so we'll probably do we'll have about half the class on Thursday finishing up regularization, and then we will turn the last class and a half to discussion panel stuff. Yeah, okay, and for the quiz on Thursday, it'll just be the readings

Speaker 1  1:14:27  
for panel. I can't remember the date, but it's this. It's the final for it's the scheduled final for the class. So shoot. I

Unknown Speaker  2:44:29  
Oh, P First you

Unknown Speaker  3:21:56  
What do You Want?

Unknown Speaker  3:38:56  
Hello, Hello,

Unknown Speaker  3:43:20  
SW, Free, SW,

Transcribed by https://otter.ai
