Speaker 1  0:00  
About modeling non linear relationships. And there's three different kind of versions of this that we're going to talk about. The first, which is very common in economics and political science, is our logarithmic transformations of variables, either the dependent variable, the independent variable, or both. You don't see this as much in say, like, you know, political psychology as you might be political economy. But this does appear in even in political psych, like reaction time, data is often logarithmically transformed. You might see it in other places as well. We'll talk a lot about that today. We'll spend a little bit of time on polynomial transformations where you might have like a quadratic relationship. So you include both x and x squared in your model, potentially even x x squared and x cubed. Very rare. You would go beyond a cubic polynomial. Cubic polynomials can capture most of the things that you're trying to capture pretty well. We'll talk briefly about this, because polynomials and interactions have similar implications so well, these two are, in some ways, the same thing. And think of a quadratic term as an interaction of a variable with itself. It's just the product of the variable itself. And so it's really a kind of interaction, in some sense. But then interactions are situations where the effect of one variable depends on another. Effect of one variable depends on itself. Effect of a variable depends upon another variable. So that's another kind of non linearity that we're very often interested in in the social sciences. And so the issue, right? You know, conceptually, these things are pretty easy to understand. The tricky part is, is dealing with what happens after you estimate the model. Estimation is the same, right? It's not hard to estimate any of these things just to involve a little variable recoding, or sometimes not even. What's more difficult is that the interpretation of the coefficients and trying to say things about what the relationships of independent to dependent variables are when you have these non linearity. So that's what we're going to do this week, and then starting and then next week, you'll see on the syllabus, it's advanced strategies for inference. Those advanced strategies that we're going to talk about largely, are largely needed for situations like this, where it's not, you know, it's not the case that you can just use the out the direct output of your call to LM, right? Like you can't just look at the standard errors that you get in summer. And when you summarize an LM object and immediately know the confidence bound, say, for something you're interested in, it's going to be the case that we're going to do more work to do significance tests or confidence bounds and things like that. You'll see why that's true. And so we'll spend pretty much next week doing stuff like that. Okay, we're gonna start by talking about logarithmic transformations. And so first, use a few useful rules to remember as we're going through these things. So there's two different kinds of notation for for eulers Number, for e in R, you use exp, the exponential function. Sometimes it's written like that as well in like a textbook. I'm going to stick with just the regular E. So you'll almost always see me write it this way, maybe occasionally like that, but almost always like this. And so a very important rule to know writes that if you exponentiate a sum that's equal to the product of e to each of the components of that sum, so e to the a plus b is equal to e to the a times e to the b. Similarly, if you see e to the a times e to the b, it's equal to e to the a plus b. And it works the same with with sub traction, except it's it translates into division. So e to the a minus b is equal to e to the A divided by E. So that's a useful rule to remember, and another sort of the inverse of e to the something is the natural log of So we also want to remember some useful rules for logs. The log of A plus the log of b is equal to the log. So again, inverse here, right into the a plus b. Of B, log of A plus log of b is log of a times b, and again, same for minus, except it's division. So we're going to use that a bunch in slides. These both. I OK, so why? Let's think a little bit about what logs do and why we might want to use them. So the first thing to note is that if we have a constant linear effect, right? If we have a constant effect in the log of x, we have a non linear effect in x itself. So let's take a simple example. Let's say we have this model, and we're going to call this the log level model. In a second. It's log level because the defense variable is is natural log, but the right hand side is in its quote, unquote level form, where level just means it's not transformed. So x is not transformed here, only the Depend variable is so the first thing to notice is, so let's say we estimated this model and we got an intercept of 10 and a beta coefficient of one, so I have an implicit beta coefficient of one there, and let's say that's a true either that's a true model or it's our estimated model. Doesn't really matter for our purposes here. What we want to think about is how to interpret this. So the first thing to note is that we can interpret this model as it is in the same way we would for any other line of regression, right? So we can, if we want to say that for every unit increase in X, there's a one unit increase in the log of y. That's that's just comes out of everything we've done to this point, right? The depend variable is the log of y. The beta coefficient is one. So a unit increase in X means a one unit increase in the log of y. That's fine, right? That's totally reasonable. That's correct. But, of course, right? The immediate thing that you would say to yourself is, I don't really understand what the log of y means. Like, what are those units? What does it mean? And so well, and so if you interpret it that way, I guess the other thing to note right is that this is a linear model, right in the log of y, so we might not really get what these units mean yet, but Right, every one unit increase in X has a one unit increase. It's associated with a one unit increase in the log of y, and that's constant, no matter what other values of it. You know, regardless of what values x, constant, linear effect, just like every other linear model we talked about, so but right because right, this is log y, right, a one unit increase in this in the dependent variable up here implies a non constant effect in y itself. So if you want to understand what a one unit change in x implies for Y, not log of y, it's going to be non linear, because the relationship between y and log of y is non linear. So it has to be the case that the relationship between x is going to be non linear in y itself, rather than in log of y. And in what way is it non linear? Well, it's non linear in the sense that an order of magnitude increase in y is associated with constant increases in the log of y. So an order of magnitude increase in in the level y, the original units of y is associated with a constant increase in the natural log of y. And let me just show you a simple example that that's true. So if we take the log of 100 and subtract from it the log of 10, we get 2.3 if we take the log of 1000 minus the log of 100 we get 2.3 log of 10,000 minus log of 1000 is 2.3 why is this all true? Right? Because, right. What we're doing with the log from the rule I just showed you, is that in each of these cases, what we're implicitly doing is the log of the first min over the log of the second. So log of 100 minus log of 10 is equal to the log of 100 divided by 10, which is equal to 10. But that's true of all of these, 1000 divided by 110,000 divided by 1000 each of those is an order of magnitude increase. And so each of those is equal to each of these is equal to the log, and the log of 10 is 2.3 we see that right when we take a difference in logs, we're really taking the log of the ratio of those two things, and because the ratio is constant across all it's the same it's the exact same value. And so every order of magnitude increase is associated with a constant increase in the log of y, moving from 10 to 100 moving from 100 to 1000 1000 to 10,000 they all increase the

Speaker 1  10:02  
log of y. Makes sense. So it's in that sense, non linear. A constant increase in the log means something different, depending on where you're starting in the levels of life. I

Speaker 1  10:23  
So, so that's good. That's potentially useful for us in some situations, right? So let's take a concrete example. So there's a set of a small set of data, and I don't know the details behind this collection. It's just useful as an illustration, but it's in the car data package in R called salaries, and it purports to be a data set of Professor salaries, I don't even know when, as a function of things like what discipline you're in, and this is, I think, empirical versus theoretical, or two categories of disciplines here, years since pH, D, years of service, a variable, female, male, and then, and then the salary variable that we might be interested in and rank. So this is a good so let's say we're trying to model salary, and let's say we're particularly interested in how salary changes as a function of years since PhD. So you know, you might be interested, for example, in what an additional year since PhD does for one salary, right? How much does salary increase as each year opposed to PhD? Now, what you should probably think, right, is that, you know, it'd be, it would be at least a little strange, right, if that was a constant effect in salary itself, right? Like we wouldn't expect every year since PhD to just increase your salary by, say, $2,000 because there's going to be wide differences in how much someone's making when they're saying, 20 years out of PhD versus 10 years and, you know, like, you know, if you get, like, a merit raise, right? It's usually like, you know, one or 2% merit raise. It's not a $2,000 merit raise. So what's happening, right? Is that years of certain, you know, years since Ph D is having a percent you know, it has an effect on salary as a percentage increase of your existing salary. So maybe you get a 2% raise for every year since Ph D, you don't get a $2,000 raise. But that, that's just another way of saying that the effect is non linear. A year an additional year post Ph D has a bigger effect on nominal dollars of salary when you're further out from PhD than when you're close from getting your PhD. That makes sense conceptually and so. So one way we could model that right is by taking the natural log of salary on the left hand side with the dependent variable, because that's exactly what this is doing. Over here, we're modeling a non linear relationship by saying that an additional unit of x has a percentage effect on the dependent variable, rather than an effective nominal scale. And

Speaker 1  13:25  
okay, so are the other questions so far, though, does that basic idea make sense that we're having multi multiplicative of that? Let me just, let me just show you an actual example here, right? So let's say that right. So let's say that the log of someone's salary right is equal to 11 or 12. One person's salary is log salary is 11 and one person's log salary is 12. Each of that is the inverse, right? That gets us back onto the scale, the nominal scale of y. So each of the 11 is a salary of about $60,000 per year. Each of the 12 is a salary of about Now, let's say that for every two years of service, someone gets a point one increase in the log of income. So let's say that that we get that coefficient from our model, so that for every two years of service, you get a raise of point one and log of income. So what does that mean in nominal dollars? Well, we can do for the first person, we can do e to the 11.1, minus e to the 11. So they got the raise of point one. And that's a $6,300 increase in solid the exact same Log In Log income increase for the person with 12, right? Log 12 income is e to the 12.1, minus e to the 12. But that's a $17,000 raise, so it's the same raise in log income, but it's a much bigger raise in nominal dollars in level income for the person who starts at a higher level again. That makes sense if you're getting like a percentage raise based on the income. But what happened? What would happen if we took the ratio? Well, the ratio is actually a constant, right? So e to the 11.1, divided by e to the 11, instead of minus is 1.1 1e to the 12.1. Divided by e to the 12 is also 1.1 and the reason is because this division is equivalent to e to the 11.1 minus 11, which is just e to the point one. Similarly, e to the 12.1 divided by e to the 12 is e to the 12.1 minus 12, or point one. So both of these are equal to e to the point one, and that's 1.11 both of these people are getting an 11% rate as a function of two years of service, which is more okay questions about so far, yeah. So in terms of interpretation generally, when we're looking at one unit increases vaso this

Unknown Speaker  16:22  
X amount, linear effect. So when would we be using the ratio? Is that to show, like, a percentage

Speaker 1  16:30  
of Yeah, I'm going to show you some time. I mean, good question, and that's what we'll get to other questions. I needs.

Speaker 1  16:46  
So hopefully we see at least we're going to keep doing this, but hopefully at least are starting to get the sense of why we might be doing this first place and what kinds of things are going on behind the scenes. OK, so we've been talking about, implicitly, talking about what we're going to call the log level model, and in its generic form, it looks like this. Again, it's log level because log on the left hand side, level on the right hand side. On the left hand side, we have the log of an even variable of interest, and on the right hand side, we have our other independent variables in level form. So this is just our generic version of the model. Just note right, this is a vector right of independent variables for unit I and a vector of coefficients. So this is written at the unit level, but with potentially multiple independent variable. So what we're going to do is, what I want to start by doing is kind of expanding on what what we were doing here, which is to say, let's think about what happens, or let's see what happens. If we just take the expected value of the left hand side and the right hand side, because that's what we might be interested in. That a lot of times what we're interested in is understanding the expected value of our dependent variable as a function of our independent variable. So let's just do that. Let's take the expected sorry, let me take one more thing in many cases, when we're dealing with a model like this, we are, as we were here, interested in how x relates to this variable itself, rather than the log, just like here, we didn't really understand what 11.1 meant, and what we wanted was something on the original units of the variable. We want something in dollars at log models, so we try to transform back to nominal values. So let's say we want to do that again, right? We want to understand how our x variables are related to y, not log of y. To do that, the first thing we want to do is raise both sides to is take e to both sides of the equation. So if you take e to the left hand side, that's going to give us back y, because natural log and E are inverses of each other. But if we do that to the left hand side, we have to do it to the right hand side too. So this is what I'm doing here. If I raise log of y. If I raise e to the log of y, I get y back, but then I do it to the other side as well. And so I'm taking E to the X beta plus U on the right hand side. From that first slide where I showed you rules for E right E to the X beta plus U is equal to e to the x beta times e to the u, that's just the same. So E to the X beta plus U is equal to e to the x beta times e to the u, good so far. So now that I'm back on the original units that I'm interested in right now, y is on its original scale, and that's what I want. Now I want to know what the expected value of Y is as a function of the right hand side. So I take expectations on both sides, and I get this expression, maybe expected value of Y is equal to the expected value of e to the x beta times e to the u. E, because we're treating x as fixed. The expected value of e to the x beta is just e to the x beta, this is a random variable u. So expected value of e to the u is something, and we are interested in what that might be. And so the question is, well, what is that? If the error term is normally distributed, we're going to we're going to ditch this assumption in a minute, but for now, right? Our classic linear model assumption is that the that u is normally distributed. If that's true, then the expected value of e to the u has a nice, simple form. It's e to the sigma squared divided by two, where sigma squared is the error variance of the model. OK, so what did we do? Let's just stop for a second and see what happened. We started with our log level model. We said we really care about the original scale of y. So let's take e to the volt. Let's do e to the both of these sides to get back onto the original scale that gave us this expression, we then took expected values of both sides, because that's what we're interested in, predicting what y is going to be as a function of x. That left us with this. We said, OK, we saw an expectation of a random variable here, we need to know what that is. We assumed that the error term was normally distributed with a variance of sigma square. And it turns out that the expected value of e to the u under these conditions is just equal to e to the sigma squared divided by two,

Unknown Speaker  21:54  
which

Unknown Speaker  21:58  
is nice and simple. Why? Because,

Speaker 2  22:02  
in this case, the expected value of u would be zero, right? So why is it not to do

Speaker 1  22:06  
so you could show this to yourself in R with a simple simulation. But when, even though the original variable is normally distributed, when you, when you the expectation of e2 that normally distributed variable does not have a mean of zero, but you can show that you can just shut up yourself. But that's that's the problem. So this is the problem. So if we understand where we where we are now, right? The problem is that this is where we end up. The expected value of Y is equal to e to the x beta, that's fine, times e to the sigma squared divided by two. That means that the mean in the case of our simple linear model right whenever we take the expectation of the left hand side, the expected value of u is zero, as that's our typical assumption. And everything is good with the world, because we can just focus on x beta, once we enter this realm, that's no longer true. And so now the expected value of Y depends not only on x beta, but also on the expected value of the error. So this is something you need to remember when you're talking when you're going from log of y back to y, is that there is this hanging, you know, hanging on term from the error that doesn't go away like it does in the symbol, normal. Okay, so let's stop for a second. Are we? Are we sort of seeing what's happening here? Are there questions so far,

Speaker 3  23:47  
yeah, not sure if you're going to go there, but what kind of consequences does it have on how we define if the estimator is bias or not, it is consistent or not? So in this case, we would say that it is, for example, it is unbiased to log of y, but not unbiased we're

Speaker 1  24:12  
going to be it's going to be consistent, right? So the expected value of Y is going to be consistent when we when we start estimating, instead of

Speaker 1  24:34  
being unbiased. So okay, so this is sort of your first taste of why things get more complicated, right? Because we sort of have this additional weirdness that we have to deal with and that, you know, I mean, this isn't too hard to manage, as we're going to see, but when we start thinking about confidence intervals and stuff like that, it's going to get it gets more complicated, because we don't have a simple way to sort of represent we don't have a very easy way to say get confidence intervals for things that are non linear functional form. And so we have to use alternative methods, which we'll talk about. Okay, so if this seems reasonable so far, let's see an example of how we might actually implement this, situation. So let's take our our data for Professor salaries. Here, I'm going to estimate log of salary on years since pH, D, the sex variable and the discipline variable, both of which are binary and less moved OLS, we get what looks like a normal, straight forward OLS table. The thing to remember again, right? Is that when you estimate the log level model and you summarize it from LM object, everything is in terms of the scale of log of y, not y. So the output you get is telling you about changes in log of y, not y interpretation, the way we go about interpreting it is the same if you're interested in the log of y, right? So, for example, years since PhD, for each additional year of PhD, we get about a point 01, increase in the log of salary y, Point 01123, this rounds to point one, so this rounds to about point o1, okay, and so we have a one year increase in years since Ph D predicting a point 01 increase in the log of salary, the natural log. But again, that's for most people. That's not intuitive, that we're usually going to want to get back to the original scale somehow, in one way or another, but that's the interpretation in terms of the Law of One. But let's say we want to get back to the original scale, and so let's say we want to make a prediction for a particular person. Let's say that you want to calculate the expected value of y for a male professor 10 years from PhD, from quote, unquote, theoretical departments which I think our department equals A whatever that means. And so to do that, we're going to need to use this formula. So for now, at least for a second, we're going to assume normality. We're going to assume that our error term is normal. We'll relax that in a minute. So what do we need to do? Well, we need two things now, instead of just one, we need the expected value, right? We need e to the x beta. We need E to our typical prediction, systematic component of our model. And then we need this other term here, e to the sigma squared divided by two. So the first part is easy to get. Oh, sorry. Oh, sorry. So the first part is easy to get, just using our typical predict function right, which is just going to give us x beta. So if we do predict on the model we just estimated, and we just say to do it for someone with 10 years since pH, D, sex equals male, discipline equals A which is a typical call to predict, it gives us exactly what we expect, which is an expected value, the log of y, 11.42 is on the log scale. And again, that's just another way of saying that we're using this these coefficients, these coefficients tell us about the log of salary. If you just throw predict, throw this into predict, we get an expected value on the log of y at the scale. If we want the expected value back on Y, we need to raise this. We need to take e to this and then multiply it by that error related term. So how do we do that? Well, exponent of this prediction we just made times e to the What am I doing over here? I'm just pulling out the error standard deviation and squaring it right. It's just sigma squared from the model divided by 2e, to the sigma squared divided by two. And that's exactly what we have in our form. This is again, getting the prediction on the log of Y scale, taking e to that and then multiplying it by e to the sigma squared divided by two. What do we get? We get a prediction on the original scale of y. So we can say now that the expected value of salary in dollars for a male from discipline, a with 10 years since Ph D is about 93,000

Speaker 2  29:52  
Yeah. Why would you ever do this, if you are interested in the underlying value of why? Why would you ever take the log

Speaker 1  29:59  
so if this would, if I regress salary on the is, then I have a linear, simple linear model where the relationship between your since pH money has, yeah. So just to say that, you know, for everyone, it's a good question, but the answer is, you know, I think it's non linear. And if I don't do the log of salary, and I just have salary, then it'll be a constant effect of every year since Ph D, instead of a non linear

Speaker 2  30:25  
effect, yeah, obviously, we also actually get a different value for the 10 year for debt. Yeah, you

Speaker 1  30:34  
actually see that in second Q when I do the level. But yeah, you should try for yourself, right? You know, as an exercise, plug in salary instead of log of salary, get these estimates and then generate a predicted value for these values, and you'll get something new, more questions. That's not to say you couldn't model that in other ways. Right? There's other possible, possible ways to get at that non linearity, but this is a very common way to

Speaker 1  31:09  
do it. Okay? So sort of the obvious question is, well, you know, we don't, we spend all this time saying, you know, this is a tenuous assumption, that the error term is distributed normally. What happens when we don't want to make that? And the answer is that we have a pretty straight forward way to get around it. It's called the smearing estimator. You know, I think originally, when I first heard that, I was like, yeah, it must. It must be named after, like, some person named smearing. And that's not true, and they don't say in the paper why. They call it the smearing estimator, so you often figure it out for yourself, but that's what's called. So if you ever hear someone talk about the smearing estimator, this is what they're talking about. And so it's a solution to the problem of non normality, right? We can't assume normal errors. So instead, what we do is use a consistent estimator of the expected value of e to the u instead of the the sigma squared divided by two estimator. So if we can't assume normal errors, we have a consistent estimator of the expected value of e to the u in the mean of E to of this in the mean of e to the UI. So basically, what we're doing, we're taking every residual that we have in the data, right? So we get a residual for every observation. Those are the estimated error terms for each observation. We take e to each of those, we sum them all up and divide it by n, divide by n. So all we're doing right is we're substituting the average value of e to the residuals for the expected value of e to the error term, which is a consistent estimator. And so we can use that instead of sigma squared over two, of e to the sigma squared over two, if we're concerned about the normality assumption. And so here, I'm just doing that same calculation. But over here, you'll see sum of e to the residuals and then divided by the number of rows in the model matrix, which is just the number of observations. Make sense. So it's just the app, the average value substituted for the expectation. So your question is almost an actual question to you, that you should actually ask a question. So for saying basically what we think is like the need asking your

Speaker 4  33:40  
for saying basically what we think is like the new estimator, in a sense of the model, then are we not missing potential

Unknown Speaker  33:50  
variance in if we were And then would it like over predict of the model? Like for this week, then we know the model fit is good, but then this is just assuming that the model, the error

Speaker 1  34:15  
variance is normal. This is not assuming error variance is normal, so sorry, but it's, it's forcing the residuals to all vehicle across the observation, yeah, I'm not 100% sure what question is, but the let me, let me say, say This, and then so this approach assumes normality, errors. This estimator does not assume anything about the distribution of the error. So it doesn't the errors do not have to be normal for this s for the smearing estimator, but it is. It's consistent. So you know, it gets better as n goes to infinity. But it doesn't make any assumption about the

Speaker 4  35:07  
distribution error. So maybe my question is, is if we're dividing it by n, then is it, is it just

Speaker 1  35:19  
assigning the same error? Oh, yes, yes, yeah, OK, I get your question. OK, so it is using, so if you think about let's go back to our formula, right? So we're getting the expected value for a particular observation, and that's based on their particular value to the independent variable. It is indeed giving the exact same adjustment term for the error to every observation. So each observation gets a different value of this and the same value of this. And that's even true with this, this consistent estimator here, because we're summing over the observations and dividing by n, so this is constant across observation. There's one estimate for every observation, and it's equal to that, okay, so I the first part of my question. It's if we're doing that and we're assuming

Unknown Speaker  36:06  
that it's the same or

Unknown Speaker  36:09  
running that calculation, then

Speaker 1  36:15  
could we be missing potential outliers? Yeah, so that's a really good question, and the answer for this is that we're assuming identically and independently distributed observations. And so an assumption of our model is that the error term has the same form for every observation. That is an assumption. If that's not true, then you need to you need to do something more complicated in terms of your modeling. You terms of your model, and, you know, like headers to get your standard. So that's right, yeah, that's totally right. Sorry, it took me a while to get there, but, but yes, the assumption is that the errors are independently and identically distribute. Okay. So one question you might ask is, Well, okay, we have two different choices here. We can either assume normality, or we can use this, this consistent estimator. What are the trade offs? And the trade off is essentially consistency versus, you know, bias that comes about through departures from normality. So if you're thinking about, well, if I if I assume normality, that might be wrong, and if it's wrong, my estimate is going to be bias in some way. On the other hand, you have consistent estimator that doesn't rely on that assumption, but you might be worried about sort of efficiency issues as n this goes to infinity, right? It's like, how much is enough, and how good is my estimator with a small n and things like that. And as with all these things, there's no single answer in the original paper on smearing estimator, Juan looks at this, and, you know, I think my interpretation of the upshot of this paper is that this should be your default, because it's relatively efficient in terms of comparisons when normality holds like you know, you're not losing a lot of efficiency when normality is a good assumption. But if normality is violated, there can be quite a bit of bias in that having the estimator that assumes So, again, it's a trade off, and you don't know what the right answer is in any given situation. But as a default, it seems like the smearing estimator is probably a good choice because it's relatively efficient under normality, and deviation from normality can be quite severe. Take that for what it's worth. But here's the original paper too.

Speaker 1  38:57  
OK, so just to summarize, when we have a model like that, and we want to say something about the original scale, we can go through a typical process of taking expected values, but we end up with this nuisance term associated with the error. We have to deal with that if we want to get consistent estimates of the expected value of Y on its original scale. And to do that, we can either assume normality or not, and probably not assuming normality is a decent baseline. And it's very simple to do for any given model, generate the predictions on the log of Y scale, and then use this simple formula to get predictions On the original scale. Questions. Before we move on, I

Speaker 1  40:07  
All right, so let's go beyond thinking about making predictions, about why, and think about how we interpret the beta coefficient. I mean, obviously these things are related, but let's think about what the coefficients mean and how we might use them to say things about the relationship between X and Y, instead of just making predictions about y. So let's start again by taking the expected value of both sides after transforming so take e to both sides and then take the expected values just like we did before. The only difference here is that I'm writing this now more explicitly. Instead of consider writing it as e to the x beta, I'm explicitly writing out the fact that this is the product of e to the intercept times E to beta one x1 times e, the beta 2x two times the expected value of the error term. So that was implicit previously. Now I'm just making it explicit because I want to, because it's going to be useful for us, but all I've done again is take e to both sides and then take expectations. So now we want to ask, right we're interested in the relationship between, say, x1, and Y on the original scale of y. So that's equivalent to saying something like, what is the expected change in y, or some delta unit change in x1 so if x1 changes by delta, what happens to the expected value of Y? How does it change? So we want to ask, what happens to the expected value of Y when this changes to this? What is this? All it is is, is the exact same things here, except I've increased I've incremented x1 by delta x1 so there's x1 and then I'm adding some some change in x1 to x1 Yeah, x1

Speaker 5  42:10  
does. You would hold the other coefficient constant, yes, zero, yeah, which would be a problem if we will not increase be a problem if

Speaker 1  42:23  
we will modify this, right? Yeah, so that's the tricky part, right? Is that the sort of the tricky part of all of this is that the effect of changing x1, by delta units, has a different effect on y in nominal terms, depending on what all the other variables are at so it's intrinsically conditional in some sense. So that's true, and we're going to deal with that. The first way we're going to deal with that is by trying to get rid of it. And so I'll show you how we're going to get rid of it, and then later, we're going to deal with it by explicitly taking it into account. We could be

Unknown Speaker  43:01  
an interaction

Speaker 1  43:02  
again, you don't need an interaction term, because it intrinsically interactive in the sense that you'll see. But for now, we're going to try to treat that as a nuisance and get rid of it. So let me show you how to do that. So we've got two things here. We've got the base line expected value of Y, and we've got the expected value of Y when x1 increases by delta, and we want to know how they're different. And so one thing we might do is to take the ratio of the two. So here is expected value of Y when X is increased by delta, and here's the expected value of Y in the original at the original value of x1 everything else is held the same, and as long as everything else is held the same, you'll notice that it's all dropping out. When we divide e to the beta zero drops out, e to the beta 2x two drops out. Expected value of e to the u drops out. If we're just dividing by the same thing, they're all gone. And so what are we left with? We're left with e to the beta one x1 plus delta divided by the original e to the beta one x1 and if we use our rule for E right, this is equal to e to the beta one right, times this minus e to the beta 1x and if we put that in the same parentheses, what we're doing is essentially subtracting this out, and all we're left with is the delta x and beta one. So this we just make that explicit. We have e to the beta one times x plus delta divided by e to the beta 1x that's equal to e to the beta 1x plus delta minus beta 1x

Unknown Speaker  44:59  
and then that's equal to E, beta 1x plus delta minus x, so this cancels, and We're left with e to the beta 1x delta x.

Speaker 1  45:20  
Right? So things have simplified a lot now we've got a representation now that only includes the thing we're interested in for now. And so the question is, just, how do we interpret that thing at the end? And so if you see what's going on here, right? We're taking the ratio of the expected value when when x goes from x1 when x goes from one value to itself plus delta. So it's a multiplicative effect, right? We're multiplying the ratio of these two things is equivalent to multiplying the expected value of Y by some factor. So a one unit change. So the interpretation of this is that a one unit change in x1 multiplies the expected value of Y by e to the beta one. If we plug in one for delta x1 and a one unit if you're interested in a one unit change in x1 then this is just e to the beta one. And what is that thing? Right? It's the ratio of two expected values. It's what happens is what we multiply the expected value of Y by if we if x1 changes by one unit. We can call that. We can also call that a factor change in y, because we're multiplying y by a factor, and what that factor is equal to is e to the beta, one times delta x1 and if delta x1 is one, we're thinking about one unit change, then it's just e to the x, e to the beta. So one up shot, we get a bunch of coefficients out in your regression table. If you just exponentiate those regression coefficients, you get the factor change in the expected value of y for a one unit change in each of those variables. So one easy way to get around this problem that all the variables are kind of conditional on each other, is to think about it in terms of factor changes or percentage changes instead of changes in the original units. That still tells you something about the original units, but it doesn't require dealing with all that difficulty, and it allows us, it gives us an easy way to sort of interpret how the variables are having an effect without going through some of the more difficult calculations, we can just quickly exponentiate all the coefficients, and then we get the factor changes in Y for one unit changes. So that's a factor change in the sense that it's a multiplicative change is what we multiply the expected value by. If we want percentage changes, we have something. We have just an easy transformation of that. If we subtract one from that thing, right, we get a proportionate change. And if we multiply that by 100 we get a percentage change. So you know, you can do factor change, and I'll show you an example in a second. But exponentiate the coefficients, you get a factor change easy. If you just do 100 times the exponentiated value minus one, you get percentage change, which people might find more intuitive. So, so let me give you let me give you an example here. So again, what we're working with here is this model. And so we're interested, for example, in how salary changes as years since Ph D increases by some amount. So that's and this time we're going to we're going to do it using this method. Because, okay, so the first thing I might do right is just exponentiate the coefficient for years since Ph D right, years since Ph D is the first predictor, we have the intercept, remember, and then we have the first predictor. So the second coefficient in that coefficient vector is the coefficient for years since Ph D. If I exponentiate it, I get the factor change in the expected value of y for a one unit change in years since Ph D. And that's one point, let's say it's 1.01 so what does that mean? It means that for every one year increase in years since pH, D, I multiply salary by 1.01 the factor change in the sense that I multiply salary by 1.01 and remember that, or realize right, that that's exactly that's closely related to this, this idea that we started with, which is that the relationship is non linear, right? If I start with $100,000 in salary and I multiply that by 1.01 that's a bigger change than if I start with $60,000 so a factor change has a different effect in nominal dollars depending on where you start in nominal dollars. But right? That's a very intuitive way to think about it. If I want to understand the effect of years since Ph D saying that I'm going to multiply whatever starting salary is by 1.01 is a fairly intuitive way to think about the effect of years since Ph D. Even more intuitive probably, is use percent change. So if I do e to the coefficient minus one times 100 What am I doing? I'm subtracting 1.01 minus 1.01 times 100 is one. So it's a 1% increase. So that's even more intuitive, right? Now I can say for every year increase in years since Ph D, I expect a 1% increase in salary. So that is a particularly effective way to convey these results, right? That's readily understandable. You might not have to do anything else. People will understand that it makes sense. That would be a totally reasonable way to report these

Speaker 1  51:33  
results. What you don't want to want you to change. Let's go back to our formula. It's e to the beta one times delta x. So if you want to do something for a bigger change in x1 just plug that in instead of e to the beta minus one times 100 do e to the five times theta minus 100 and that's a five unit change in x. So now I say for every five year increase in years since Ph D, I expect a 4.9% increase in salary, slightly less than 5%

Speaker 1  52:18  
so notice that this is A very maybe. We started out with something pretty complex, but because of sort of the way we went about trying to think about the expected values, we ended up with a very simple way to interpret the coefficients that requires very little additional work after you've estimated them, and gives you substantively meaningful quantities that you can use to interpret change.

Unknown Speaker  52:47  
You could also just

Speaker 1  52:51  
multiply that by the change in x, right? Like, no, unfortunately, I mean, that would be give you a rough estimate. So, like you can see here, one 1% times this. That'll give you a rough estimate. And the reason why it's not exact, right, is because the multiplication is happening in happening inside the yes.

Speaker 1  53:22  
I mean, the other thing to say, I guess, right, is a rough, like, you're just looking at the output, right? This is about, I mean, I wish this wasn't. This is round, but this is about point o1 right? And that's that's giving you a rough estimate of the proportionate change. And so multiply that by 100 and you get one, which is the, you know, the estimate we just have here, 1% so the coefficients themselves can give you a rough estimate of the proportionate change. But the quality of that approximation depends on it on the change being close to one, right? You know it'll get worse and worse as you multiply by larger amount of dollars. And given how easy it is to discuss, I don't seem to reason to the bottom of the approximation. Okay?

Speaker 1  54:31  
More questions. All right, so that, like I said, that's a totally reasonable way to go about interpreting this model. Maybe that's all you want to do. That's totally fine, but let's, let's think a bit more about things that we might be thinking about doing. So, what about marginal effects? Well, what's the marginal effect? Marginal effect is the first derivative with respect to some variable. The marginal effect of x1 is the first derivative of that expected value equation with respect to x, first partial derivative. So we need to remember some simple rules to get into this, the first derivative of the log of x. We not even really going to use this to guess that is one over x and the first derivative of e to the a x, where a is a constant and x is the variable of interest is a times e to the x, so the derivative of e to the x, with respect to x is e to the x, which is weird. Okay, I saw there's like a three blue, one brown video on why that's true. I think that's interesting. But, yeah, that's weird, right? But if you take the derivative of the inside, you end up having to multiply that constant. So derivative of e to the a x, where a is a constant, is a times e to the a x. Derivative of e to the x, with respect to x, is just e to the x, OK, so let's figure out what the marginal effect is of some variable. So and again, what we're interested in is in the original scale of y, is the log of y. This is easy. You can just do exactly what you did for the last five weeks. If we're interested in the original scale of y, it's trickier. So again, we're going to start with the expected value on the original scale, which is what we derived at the beginning. Now the issue, as you noted a second ago, right is that the marginal effect of x1 is no longer a constant when we were dealing with a simple linear model, y, let's say beta zero plus beta one x1 plus beta 2x two, right? If we take the first partial derivative with respect to x1 we this drops out and this drops out, and multiply those, we end up with great one. So this is a constant in a simple linear model, but now you can see that that's not going to be true when we have this multiplicative situation going on. So what do we get? Well, we use those rules we just talked about, the first partial derivative of this with respect to x1 this part right is e to the x beta. It's that same rule we just talked about, but we have to pull out the equivalent of the constant here. And since it's with respect to x1 the constant is beta one, beta one times the exact same thing. Beta one times e to the x beta. This term is just, it has no beta in it, right? So it stays the same. I'm sorry, it has no X in it, so it stays the same. It's effectively treated as a constant to the partial derivative. But because it's each of the something, it stays the same. Doesn't go anywhere. And so what we're left with, if you take the first partial derivative, is sort of is strange and counter intuitive. We just pull out the coefficient for the variable of interest, and then we're left with the exact same thing we started with, which is just the expected value of Y. So what this means is that the first partial derivative of the expected value of Y with respect to x1 is equal to beta, one times the expected value of Y. Now what does that mean? It means that let's think again about what this means, right? The marginal effect. The marginal effect is telling us how this function is changing instantaneously as a function of some variable, x1, and what we're saying is how that function is changing is a function of its coefficient, which makes sense, but it's also a function of where you are on y to start with. And so depending on what the expected value of Y is, that the way the function is changing as a function of x1, is going to be different. So and this I, one other thing to note right is that this is index by I, the expected value of Y for any given observation is going to is going to vary, right? It's going to be different for each observation. And so this means, right? One way to put this is that the marginal effect of x1 is conditional on our predict our other predictions on all the other values of x and their coefficients. And because each observation has different values of x, that means that each observation has a different marginal effect of x1 that's so I was just gonna say that, but I'll say that again in a second. But do we sort of see this so far? I mean, the up shot so far is that we, when we take the derivative with respect to x1 we do not get a constant. We get our coefficient, but it's multiplied by this expected value y term. So it's a non linear function. It's not

Speaker 1  1:00:17  
so so the up shot right is the marginal effect of x1 is conditional on x beta. And since right, each observation has its own set of independent variable values. And since that's a part of the marginal effect equation, the marginal effect is different for every observation in the data. All right, it's different, potentially different for every observation data. Observations with the exact same values of x obviously have the same for every combination of independent variable values, we have a different marginal effect. And again, that just comes out of the fact that expected value of Y i is in the equation for the margin alpha of x1,

Speaker 1  1:01:13  
OK, sorry, I went through that pretty quick, so let's just stop and take Questions for a second. There are pieces of confusion

Speaker 6  1:01:22  
here. Other

Speaker 5  1:01:29  
regression has two very different interpretations, right? Because fact that changes by definition assuming some

Speaker 1  1:01:41  
kind of constant change. Yeah, exactly, so. So if you express the relationship between x1, and y in terms of percentage changes, then, then it's constant, sort of by definition, exactly, because this is not how it's not how y is changing in percentage terms, this is how it's changing in terms of raw values. And its raw values are going are going to depend on where you start. So another way to say that, so again, think about the example where a, let's say, a one year increase in years since Ph D has a increases your salary by 1% right? That's the same whether I start with 100,000 or whether I start with 50,000 but if, instead of proportionate changes in my salary, I'm interested in the raw dollar amounts, then it's going to be $1,000 for the first person, and $500 per second. And so now we're interested when we when we talk about the marginal effect, we're talking about how nominal dollars are changing. So this is a different strategy for reporting your results. Yeah, I think I was just working

Unknown Speaker  1:02:57  
with the older

Unknown Speaker  1:02:59  
idea that a coefficient was in kind of being an info, but like COVID, yo, it is, but in very different ways,

Speaker 1  1:03:06  
exactly. That's exactly the right insight. And that's where all of this stuff starts getting complicated, is because we go first from a situation, like you said, where we can look at a coefficient and know the relationship between x and y. Now we're in a world where that's not true anymore, right? The coefficient tells us something, but it doesn't tell us the how the function is changing. As the expected value of the y, I predict the value of y

Unknown Speaker  1:03:33  
by the model divided

Speaker 1  1:03:40  
by the N. I area. This here is the same. Is exactly the same quantity as we started with, right?

Speaker 1  1:04:00  
It's here you could use the smearing estimate too, but the expected value of Y is just the same thing that we derived earlier, same thing. And so you can either use the normal assumption or the smearing estimate, but it's the same quantity we talked about before. So so the other complication is that that also appears now.

Speaker 1  1:04:29  
So I wonder why I went back to the other side. Is right here. So the expected value of Y just is this part, it's e to the x beta times the expected value of e to the u, and expected value of e to the u we can get using this query.

Speaker 3  1:05:01  
More questions. Yeah, perfect. So one way to interpret this would be a margin of change in x1, leads to the expected value of Y changing by theta, one times the expected value.

Speaker 1  1:05:19  
And that's remember what that that is, right? We're in the world of non linear functions now, and so that's the slope of the tangent line to the function. So that's the instant. That's how the expected value of y is changing instantaneously, as a function of changes in x1 the slope of the tangent line to that curve. And that's, that's the general definition of the marginal effect right? When we're in the world of simple linear models, that tangent line to the curve is just equal to what happens to Y for one unit change in x, because we're plotting a line, right? But now we've got a situation where that's not true anymore. We've got maybe a curve that looks like that. And so the slope of the tangent length of the curve, okay, is not the same thing as what happens when x changes by one unit. It's the instantaneous rate of change. And so, you know, a follow up question would be, well, I don't like that. I don't want it. I you know, I want to know what happens when x changed by one unit. And the answer is, the response would be, that's really reasonable. And you can use percentage changes in y. That's an easy way to do it, like we showed you. Just exponentiate the coefficient, or if you want it on the raw values of y, like dollars, you can use first differences, which I'm going to show you

Speaker 3  1:06:42  
next. So a follow up question to that would be, in slide 12, you said that a one unit change, change expected value of y by E, to the power of theta one times, is also, it's also like a factor of of expected value of Y, whereas later on we establish that we take the derivative, the factor change is now theta one, and not e to the power of theta one anymore. So I'm just kind of thinking of, what's the difference between this and the other result that we allowed that?

Speaker 1  1:07:20  
Okay, okay, that's gonna work there. So does this make sense? Why this makes sense?

Speaker 3  1:07:28  
Okay, they both make sense that I don't, I don't get why it leads to different stuff. So

Speaker 1  1:07:34  
I think the thing to do is forget about that for a second and just focus on this, right? This isn't different, right? So look at this, right? So this is beta one times e to the all those things times the expected value of e to the u. Let's go back to slide 12, right? That's also what we've got, you know, in one of these things, we've got e to the x, beta times the expected value of e to the u. The only difference, as you know, right? Which is important, is that we also have beta one out in front, not exponential. So why is that? And the answer is, because of the rules for taking the first derivative of e to a constant times a variable. It's e to the constant times a variable, but we bring the constant out front. So let's actually just do that for a second here, right? So let's say we have e to the beta zero plus beta one x1 plus beta 2x two, and we take the first derivative with respect to x1 first partial derivative with respect to x1 so the first thing we know is that, because it's e to the something, we're going to reproduce e to that something. So

Speaker 1  1:08:58  
that's going to be our first term by the chain rule. Now we have to take the first derivative with respect to the inside of the function. So that's the derivative of e to the x is e to the x. So we reproduce that. Then we take the derivative of the thing inside the function, which is up here, and the first partial derivative of this linear function with respect to x1 is just beta one, and so we just multiply this whole thing by beta one. So that help

Speaker 3  1:09:30  
at all get why there's a beta one in France, I just don't get what's the difference between like? Is it like? Why does we Why is the factor of change sometimes e to the power of theta one, and the factor of change other times is beta one? Is it the difference in them? Is it like the previously, we're talking about a one unit change in x, and now we're talking about like infinitesimal change.

Speaker 1  1:10:05  
So, I mean that is one is

Unknown Speaker  1:10:08  
effective change in E y, still

Speaker 1  1:10:19  
safely. It is still I'm seeing, I'm seeing where you're getting so, yeah, the reason why you use it because this is also has the form of a factor change in the expected value of Y, yeah, okay, that makes sense. Good question. So what's the intuition of that? I mean, one difference is, as you noted, right? They are different quantities. There are differences in one is one? Is this factor change in the expected value of y for a one unit change in x, and this is a change in the expected value. Well, okay, so that's actually yes, that's exactly that right after change in

Unknown Speaker  1:11:04  
continuous times, as opposed to discrete

Speaker 1  1:11:09  
times. But yeah, that's definitely, definitely a difference, yeah, but it's also just yeah, the, I mean, the other difference here is that this is not I mean, maybe that's just the best way to put it. I'm going to try to say something else, but I don't think I'm going to say better than that. But the deep intuition as to why these have similar forms and the difference I get what you're saying now, though, think more about it, to give you a better intuition. Yeah, how do we

Speaker 7  1:11:47  
repress this formula in terms of the gradient? How do we show it on the background? To press this

Speaker 1  1:12:02  
gradient so do so on this curve? Yes. And if this curve is telling you the expected value of Y, then we're interested in how that's changing. How that's changing depends on where you are on the curve itself. So here you've got potentially a value of how that function is changing as a function of x, but it can be a different, you know, it can be a different value at other

Speaker 7  1:12:29  
places on that function is when you have a y major change. That's the horizontal value, right? If the vertical value would be the v1 multiplied by if you get effect or y. I, how do we actually understand, I mean, interpret this.

Speaker 1  1:12:45  
So the interpretation is tricky, because what you always what people want to do in situations like this, which is totally reasonable, is to ask, what happens to the expected value of y if x moves from some value to one unit different value. That's not what the marginal effect. The marginal effect is what it is, and it's the instantaneous rate of change in the function as x1 is changing by an infinite amount. If that's not useful to you, which is totally reasonable, then you know, you're moving away from marginal effects to first differences. When we were dealing with a simple linear model, it just happens, because it's a linear function, that the first difference and the marginal effect are the same. They give you the same answer. But once we move into the world of non linear functions, that's no longer true. And so there's a difference in what the marginal effect is and what the first difference is trying to tell you, maybe I'm not getting at your question.

Speaker 7  1:13:58  
Now I understand this marginal effect is changing continuously across this curve, because of the because given one huge change of x, the vertical distance, it changed across the different x value and y value, and given that vy, the constant so now the expected matter about what of why is changing. So how do we understand data on the diaper?

Speaker 1  1:14:34  
I don't think it's I don't think we should try to understand it on the diagram. I think the better. The way I understand it is just that what we would, what we're going to do in practice, right, is we're just going to calculate this thing. We're going to say we know the coefficient, we know the values of x that we're going to for some unit. We can plug all the all that stuff into this equation, and we'll get, we'll get the commercial effect for that mean? I mean, on the diagram, I'm not sure that's kind of, I'm not sure I can draw it in the way that would be enlightening at all, right, beyond just saying right, that this is the analogy is to a simple non linear function, where the first derivative is the tangent line to the curve. But conceptually, what we're doing right is calculating a different marginal effect for every unit conditional on what their x values are, and that raises its own issues. Right? That'll be true for first differences as well. If you have an effect that you're interested in that changes for every unit. You now have as many effect sizes as you do units in your data. And the question is, well, what do you report? You report 10,000 you know, marginal effects. And there's a couple different strategies that we use for dealing with that. We'll get into this in the next class. But one strategy is to fix all the independent variables at central tendencies. So if you fix all the variables at their mean, you now just have one marginal effect. The other is to actually go through, and this is probably the preferred one. Is to go through and calculate the marginal effect, or the first difference, whatever it is, for every single unit in the data, and then average over the data to get an estimator for the average marginal effect for the average risk difference both. This is becoming increase. This has become more common. This is probably still the one that people default to, much easier, much easier, somewhat easier. But that's, that's the implication, right? As soon as we have this reality, subscript is i that means that every unit has a different marginal effect or first difference. And you have to, we have to make, we have to use some additional strategy to figure out what to report. If we have, you know, 500,000 10,000 different values of the quantity of interest. Now again, if this, if this is something that you're thinking like, I would be it's too complicated. Why would I want to do that? You can always go back to this strategy with logs, and you have this nice, easy interpretation. Of course, that will not be the case for other kinds of models, right? So for like interactions, that's not, not necessarily the case when you get to like logit and probit models, other kinds of non linear models, the generalized linear model, you're going to want to talk about predicted probabilities, and this exact same problem arises, and there's no simple block solution. So this kind of issue comes up again and again in social science, use. Okay, so, so, that's a start. We'll pick it up again on Thursday. Yeah, I mean, Tracker because there were some of this.

Transcribed by https://otter.ai
