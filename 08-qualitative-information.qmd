---
title: "Qualitative Information"
subtitle: "POLSCI 630: Probability and Basic Regression"
format: clean-revealjs
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
date: 3/18/2025
embed-resources: true
---

```{r}
setwd("/Users/kristinamensik/Documents/Duke/2024-2025/Regression")

```

## 8. Qualitative information

Information about differences in kind rather than quantity

-   Gender identification

-   Religious identification

-   Marital status

-   Region

## Representation

We can represent qualitative information using numeric codes that have no quantitative meaning;

-   Binary variables, e.g., gender (male or female), coded 0 or 1

-   Multicategory discrete variables, e.g., US region (South, West, Northeast, Midwest), coded 1, 2, 3, or 4

## Dummy variables

It is typical in regression applications to represent a categorical variable using a series of $J-1$ "dummy" variables, where $J$ is the \# of categories, each of which represents membership (or not) in a particular category of the variable

-   For US census region, we could create 3 dummies:
    -   1=South
    -   1=Northeast
    -   1=Midwest
-   0/1 coding is technically arbitrary (could be 5 and 7.3, 1 and 1k, etc), but 0-1 easy to interpret

## Example, gender

```{r}
# load Lucid pub opinion data
setwd("/Users/kristinamensik/Documents/Duke/2024-2025/Regression")
lucid <- read.csv("/Users/kristinamensik/Documents/Duke/2024-2025/Regression/Lucid_Data.csv", stringsAsFactors = F)

# estimate regression of left-right economic policy prefs on IVs
m1a <- lm(econ_mean ~ male, data = lucid)
summary(m1a)
```

. . .

What's the intercept here?

-   the avg value for the 0 of all of thhe variables (woman who is not educated, etc)

-   prediction for females under additional conditions (0)

-   interpretation for male – just changed female to male, but holding everything else constant

## Example, gender

```{r}
# estimate regression of left-right economic policy prefs on IVs
m1b <- lm(econ_mean ~ age_scale + I(age_scale^2) + male + educ_scale + income_scale, data = lucid)
summary(m1b)
```

## Example, multiarm experiment

-   does decision of supreme court make you more likely to attack integrity of SC

-   does party context (party cues, polarized or unpolarized)

In this experiment, there are 2 factors:

1.  `cues`: 0=No party cues, 1=Unpolarized party cues, 2=Polarized party cues

2.  `decision`: 0=respondent disagrees w/SCOTUS decision, 1=r agrees w/SCOTUS decision

```{r}
# load data
yougov <- read.csv("/Users/kristinamensik/Documents/Duke/2024-2025/630 - Spring 25/yougov.csv", header=T)

# label conditions
cue_names <- c("No party cues","Unpolarized cues","Polarized cues")
con_dec_names <- c("Disagree w/ decision","Agree w/ decision")

# cue conditions
yougov$cues <- yougov$polcond
yougov$cues <- factor(ifelse(yougov$cues == 0, NA, yougov$cues), levels = 1:3, labels = cue_names)

# decision (like/dislike)
yougov$decision <- factor(ifelse(yougov$ideotreat == 0, NA, 
                                 ifelse(yougov$ideotreat == -1 & yougov$ideo %in% c(1,2), 1, 
                                        ifelse(yougov$ideotreat == 1 & yougov$ideo %in% c(4,5), 1, 0))), 
                          levels = c(0,1), 
                          labels = con_dec_names)

# make factor variables numeric
yougov$cues <- ifelse(yougov$cues == "No party cues", 0, 
                      ifelse(yougov$cues == "Unpolarized cues", 1, 2))
yougov$decision <- ifelse(yougov$decision == "Disagree w/ decision", 0, 1)

# summarize conditions
table(yougov$cues, yougov$decision, dnn = c("Cues condition", "Decision direction"))
```

The outcome, `narcurb01`, represents attitudes toward reforming the Supreme Court.

## Example, multiarm experiment

```{r echo=TRUE}
# estimate model
m2a <- lm(narcurb01 ~ as.factor(cues), data = yougov)
summary(m2a)
```

-   F-val also not significant, and p val 0.08

    -   if model is an experimental design and don't have othher controls, then F stat is significance test if your treatement work ("omnibus" test of treatment efficacy) – do you do better predicting dependent variable than not knowing

        -   ie: did my treatment have any effect

        -   --\> no evidence that random assignment to treatment conditions had any effect

        -   worth noting that omnibus test can be significant without

        -   f stat literally just ANOVA of the experimental factor

## What if we remove intercept? {.smaller}

-   doesn't kick out anything

-   what intercepts tell – predicted val of y, or mean, for anyone with that cat (mean of anyone in control, vs predicted val for control)

-   same as intercept, but no longer difference, just means

-   but, lm will still give significance tests

-   t becomes much lest interesting

-   r\^2 .68 : lm still trying to do it's normal thing (comparing your model to intercept only model); you set your intercept to 0, so for intercept only model will get 0 for every observation of y

-   aggregate function also gets means

:::::: columns
::: {.column width="60%"}
```{r echo=TRUE}
# estimate model
m2b <- lm(narcurb01 ~ 0 + as.factor(cues), 
          data = yougov)
summary(m2b)
```
:::

:::: {.column width="40%"}
::: fragment
```{r echo = TRUE}
library(dplyr)

yougov %>%
  select(narcurb01, cues) %>%
  na.omit() %>%
  group_by(cues) %>%
  summarise(mean_curb = mean(narcurb01)) %>%
  mutate(curb_rel_control = mean_curb - mean_curb[1])
```
:::
::::
::::::

Conjoint:

-   

-   can calc marginal means

    -   how often did you choose a profile when this profile had x attribute

. . .

What does $p \approx 0$ mean in this context?

## Multiple factors

-   don't need to do as.factor for decision bc already coded 0-1, can include as dummy

-   

```{r echo=TRUE}
# estimate model
m2c <- lm(narcurb01 ~ decision + as.factor(cues), data = yougov)
summary(m2c)
```

## Multiple factors w/ interaction

-   

```{r echo=TRUE}
# estimate model
m2d <- lm(narcurb01 ~ decision*as.factor(cues), data = yougov)
summary(m2d)
```

## With covariates

```{r}
# estimate model
m2e <- lm(narcurb01 ~ decision + as.factor(cues) +
                      age01 + female + black + hisp + educ + incomei01 + know01 + relig01, 
          data = yougov)
summary(m2e)
```

## Ordinal variables

Educational attainment in the previous example is ordinal, but we treated it as interval

-- ordered, but distance between adjacent values not necessarily the same across the range of vars

-   question becomes how to treat + think abt – case by case

```{r}
table(yougov$educ)
```

. . .

Partisan identity another common example of this

-   1 = Strong Democrat $\rightarrow$ 7 = Strong Republican
-   pseudo-interval measure, but goint from strong D at 1 to weak at 2 /= weak dem to weak rep

## Ordinal w/ no constraints

most straightforward thing to do w/ordinals is to dummy it out

Technically more precise to treat it as a category, but costly in degrees of freedom

-   change below is treating education as a ffactor to dummy it automatically (will exclude lowest cat, ed = 0, then dummy for the rest of

-   looking at ed below, how to characterize the whole thing?

    -   all p values are insignificant (none are statistically significant)

    -   no evidence education hhas any impact

    -   p strong evidence that plugging in as interval variable probably not correct

```{r echo=TRUE}
# estimate model
m2f <- lm(narcurb01 ~ decision + as.factor(cues) 
          + age01 + female + black + hisp + as.factor(educ) 
          + incomei01 + know01 + relig01, data = yougov)
summary(m2f)
```

Example

```         


![Trexler (2025)](images/trexler_diss.png)
```

-   non monotonic relationship in middle \*\* ask Andrew

## F test

We can use an F test to see whether the unconstrained model for `educ` fits better than the interval model

-   2 mult by 2 bc moving 2 units up from baseline

-   nested within more unrestricted model

-   model 1 treats education as interval, gets 1 coefficient, constrain effect to be equal across thhe categories (restricted model)

-   model 2 treats as nominal dummies out education, gives each coefficient (loose degrees of freedom, here can't reject the null /decide this provides better fit... Q: is model a better fit or are you just cutting degrees of freedom)

-   can test w F test – basically test off residual sum of squares

\*usually, you'll want to think about this more theoretically beforehand

```{r echo=TRUE}
anova(m2e, m2f)
```

-   also, in between step you can take

    -   strain coefficients to be ordered in a particular way

-   General take: usually need to commit beforehand to treat variable in a particular way

## Interactions, continuous x categorical

Interacting a continuous with a categorical variable implies distinct slope for each categorical group

-   Number of unique slopes = number of groups

-   If \> 2 categories (ex, multiple dummy variables 1-4 for education) and want to interact with something that is interval, need to interact with *each* category

. . .

ALT: Can also estimate separate models for each group

-   This effectively interacts every variable with the group
-   implicitly interacting education with every variable in the models

## Example

-   looks at weather if you get decision from court you like, more resistant to restricting court power

-   here: interaction between knowledge variable and decision (o if get disliked 1 if get liked decision)

    -   decision: constituent term

    -   interaction: as knowledge goes from min to max, impact by treatment increases by 7 pp

        -   change in effect of decision variable for 1 unit change in knowledge (coded 0 to 1, so min/max)

        -   max is decision + interaction term coefficients

```{r echo=TRUE}
m3 <- lm(narcurb01 ~ decision*know01 +
                      age01 + black + hisp + educ + incomei01 + relig01, 
            data = yougov)
summary(m3)
```

## Example, using `sim`

-   using simulation to get conditional marginal effects

Predicted values w/ confidence bounds, across `know01`, for each value of `decision`, holding other vars at their central tendencies

```{r echo=TRUE}
library(arm)

# get sims
m3_sims <- sim(m3, n.sims = 10000)

# new data for predictions
nd_3 <- data.frame(decision = c(rep(0,11),rep(1,11)), know01 = seq(0,1,0.1), 
                       age01 = mean(yougov$age01, na.rm = T), 
                       black = 0, hisp = 0, educ = median(yougov$educ, na.rm = T), 
                       incomei01 = mean(yougov$incomei01, na.rm = T), 
                       relig01 = mean(yougov$relig01, na.rm = T))
nd_3 <- cbind(1, nd_3, nd_3$decision*nd_3$know01)

# calculate slopes for each sim
know_sims <- t(sapply(1:10000, function(x) as.matrix(nd_3) %*% m3_sims@coef[x, ]))
```

## Example

```{r fig.align='center'}
par(mar=c(5,4,1,1))
plot(1:11, seq(0,1,0.1), type="n", ylim=c(0,0.6), xlab="Political knowledge", ylab="Expected value of Court curbing",
     axes=F)
axis(1, at=1:11, labels=seq(0,1,0.1))
axis(2, at=seq(0,0.6,0.1))
lines(1:11, predict(m3, newdata = nd_3)[1:11], col="blue")
lines(1:11, predict(m3, newdata = nd_3)[12:22], col="red")
lines(1:11, apply(know_sims[,1:11], 2, function(x) quantile(x, 0.025)), lty=3, col="blue")
lines(1:11, apply(know_sims[,1:11], 2, function(x) quantile(x, 0.975)), lty=3, col="blue")
lines(1:11, apply(know_sims[,12:22], 2, function(x) quantile(x, 0.025)), lty=3, col="red")
lines(1:11, apply(know_sims[,12:22], 2, function(x) quantile(x, 0.975)), lty=3, col="red")
legend("topright", c("Disliked decision","Liked decision"), col=c("blue","red"), lty=1, bty="n")
```

-   politically knowledgable people who get liked decision are decreasing support for court curbing

-   politically knowledgeable ppl always less willing to curb court, but less so when get disliked decision

## Example, using `marginaleffects`

```{r echo=TRUE}
library(marginaleffects)

# effect of decision at values of know01
slopes(m3, 
       variables = c("decision"), 
       newdata = datagrid(know01 = seq(0, 1, 0.1)))
```

## Chow test

Reasonable to wonder whether this model reflects the DGP for everyone, or whether the DGP varies by group.

-   does data generating process of interest vary across some set of groups that I can define in my data

-   can (a) estimate different model for each, or (b) test if relaxation of constraints stats signifficant

    -   f test

. . .

A special form of F test - called a [Chow test]{.yellow} - can be used to test the null hypothesis that a linear model is identical across groups

## Chow test

-   m4 model with everyone combined

-   d for ppl who got disliked decision

-   then subset for liked decision

-   if subset somethihng out...if subset data into mutually exclusive categories and estimate separately, you can no longer include un-subsetted in

    -   question is does relaxing constraint that all these are equal do good or do bad

    -   think i need clarity on how why thhis different than 3 dummies in 1 model

    -   

```{r echo=TRUE}
# model for all
m4 <- lm(narcurb01 ~ know01 +
                      age01 + black + hisp + educ + incomei01 + relig01, 
            data = yougov)

# model for disliked decisions
m4_d <- lm(narcurb01 ~ know01 +
                      age01 + black + hisp + educ + incomei01 + relig01, 
            data = subset(yougov, decision == 0))

# model for liked decisions
m4_l <- lm(narcurb01 ~ know01 +
                      age01 + black + hisp + educ + incomei01 + relig01, 
            data = subset(yougov, decision == 1))
```

## Chow test

estimate, then plug ssr into chow f test

-   then test compares fit of the m4d and m41 combined with m4

The formula requires estimation of restricted and *both* unrestricted models

$$
F = \frac{SSR_R - (SSR_{UR1} + SSR_{UR2})}{SSR_{UR1} + SSR_{UR2}} \cdot \frac{N - 2(K + 1)}{K + 1}
$$

## Chow test

```{r echo=TRUE}
SSR_R <- sum(m4$residuals^2)
SSR_D <- sum(m4_d$residuals^2)
SSR_L <- sum(m4_l$residuals^2)

Fstat <- ( (SSR_R  - (SSR_D + SSR_L)) / (SSR_D + SSR_L) ) * (nrow(m4$model) - 2*(length(coef(m4)))) / length(coef(m4))

p <- 1 - pf(Fstat, length(coef(m4)), nrow(m4$model) - 2*(length(coef(m4))))

cbind(Fstat, p)
```
