Speaker 1  0:00  
Extra year, six weeks E on the log of salary, when discipline and when discipline is at zero and when sex is at zero. So I guess that's

Speaker 2  0:16  
Yeah, exactly. That's exactly right. You don't know these things, but yeah, that's exactly right. It's the it's the effect of years since Ph D on log salary, when the two moderators are both set to zero. And in this case, that means you're in Ph D discipline A and you identify, so you might say something like a one year increase in years since Ph D for someone in discipline, a who identifies as female, is associated with a point 02, increase in mod salary. Now again, right? You know, if we were doing this for real, you might want to do something slightly different in terms of getting something more substantially meaningful. But for our purposes, we're focused here on trying to understand the interactions, right? So we're not so much worried about the fact that this is log accelerated, right now, okay, that makes sense, right? So the constituent term right is when both moderators are zero, but that's also true for the other two variables now, because they're both in the interactions too. So the effect this constituent term discipline B is now the difference in the average lot of salary between discipline B and discipline a when years since Ph D is zero. The constituent term for discipline B is the effect of discipline B when the other variable is zero, which means years since Ph D is zero. So that's the predicted difference between the two disciplines and log salary when you're some PhDs here. I guess in this case, that does make sense, actually. So that's, you know, we might think of that as like, as you're merging out of your Ph D, what's the starting salary for people in discipline B versus discipline A? So that's actually, that is actually, sorry, I was thinking it was thinking age for a second. And the same thing will be true of the variable set too, right? So this is going to be the difference between males and females, and starting loss salary also easily interpretable. And the interactions tell us how these differences change as years since PhD increase, right? So this interaction tells us how the difference between the two disciplines in lot of salary changes as years since PhD increases. So it's basically saying is, you know, here's the baseline difference in starting salary, and then for each additional year, we're getting an increase of point 003, in the difference between those two disciplines. So the difference is expanding over time between the two disciplines. The opposite is true of sex, male right here there's a baseline, moderately sized starting salary difference between males and females, that decreases as a function of years.

Speaker 1  3:09  
So here, the gender pay gap is declining all the time. That's I

Speaker 2  3:13  
mean in this model. Yeah, exactly so, because it's a negative coefficient. So if we again, if we look at the constituent term. What we see is a baseline starting salary difference where males are getting paid more, and the negative point 01 coefficient says that for each additional year, that difference between males and females goes down by point oh one. So we would expect, for example, after 20 years, it would be zero. And if you believe this model right, then it would actually become negative as you go beyond 20 years, so that by 40 years, it's actually negative more. But the specification, yeah, you

Speaker 1  3:51  
might have mentioned this last lecture, but so in all these examples, the minimum is zero. What if we would add like, I don't know, like income that could also be negative. So you look at like depth or something like,

Speaker 2  4:03  
yeah, yeah, good. So yeah, that question. So imagine that, yeah. Let's say for a second, forget about what your since Ph D is or, I guess that could be negative. It could be grad school. Let's just imagine that variable can be negative. And let's look at the sex male coefficient, so this would now be saying that that's still the difference between males and females when this variable is zero, but now that's like a mid length value, rate, it's, it's, it's not the lowest value, it's somewhere in the middle. And then we could say, well, what happens when you go negative from zero with a positive if you go negative from zero for a negative one change, right, you're multiplying this by negative one, and so it's going to be a positive change in this coefficient. So going from zero in the negative direction is going to increase the size of the difference, because you're going to add negative one times a negative number, which is point of one,

Speaker 1  5:01  
and the interpretation of the constituent terms, it would still be when the other variables are zero. Would it be when you're seeing speech D is at its minimum value, which could be minus two for the other variables,

Speaker 2  5:13  
it's still going to zero, right? The constituent terms you're saying, right? So I guess variable here, yeah, yeah, that's, that's always, no matter what, it's always going to be the effect of this variable when the moderator is set to zero. And that's, again, just because, if you take the first partial derivative, it's the intercept of that marginal effect equation, right? And so just, I don't need to write it, I'll just go back to the thing. Let's just look again, right at the equation. This is the constituent term coefficient that is going to be the predicted value of the marginal effect when the other variable is zero, just by definition, that's what it is. So however you code the variable right, zero is going to be something different, but that term will always be the effect from the other variable is zero. Whatever that means. It could mean lots of different things, and that's one of the reasons why we often like to standardize when we have interaction terms, because that ensures that the meaning of zero is meaningful, that it's useful for us substantively, because then we can interpret this as the effect of x1 when x2 is at its mean. And that's usually helpful, okay? And that's also, sorry. One more thing, if you standardize fully so you de mean and have standard deviations of one, right? Now, negative changes from zero are not only possible, but they're force right so now a negative one change is a movement of one standard deviation below the mean of the Moderator. So those are cases where the question you asked is exactly what we're going to see. Okay, sorry, yeah, no, just wanted

Speaker 1  6:52  
to say that. I think in one of the readings it said like when, when it's at zero or it's minimum. So it was a bit confusing, but I guess things meant at zero, meaning it's minimum or something.

Speaker 2  7:07  
I don't know. I would have to see this specifically. Find at all. I can take a look at

Speaker 2  7:16  
your class, no, but I think you know, and I say this like every class, but when in doubt, take the first partial derivative and then just plot the function, and you don't even have to worry about it. Okay. More questions is making sense.

Speaker 2  7:39  
Okay, so let's stop for a second and take any any questions that are getting confusing here, because I'm about, I'm going to move on to a couple other sort of, you know, you know, issues related to interactions, but go through that pretty quick and then move on. So are there other things you want to ask about interactions before we move on?

Unknown Speaker  8:02  
I Yeah.

Speaker 3  8:04  
So for example, if you want to interact like one of your interaction variable is a categorical variable, but it's not like binary. There's more than two categories. In that sense, do you also dummify and interact with each dummy? Yeah, or

Speaker 2  8:29  
that's a great question. So we're actually, we're going to do an example with that, and we it's scheduled for Tuesday, but we'll actually try to be maybe after spring break. But it depends. The answer is that it depends on how you want to treat that categorical variable. So it's very, very common in social sciences to have what is on its face a categorical variable, like a Likert scale from like one to seven, and to treat it as if it's like pseudo continuous so to pretend that it's a continuous variable and use it like a continuous variable, but, and so the question for you is just whether you want to do that, or if it's more like religious identification, in which case it's there's no it makes no sense to treat it as, you know, pseudo continuous, because you can't, like order different denominations of Christian Animation. You have to donate out right? It's a nominal variable intrinsically, and in that case, your intuition is right. If you want to interact religious identification with some other variable, like years since Ph D, or if you had multiple categories of like a gender variable, you need to and we'll talk more about this if it doesn't make full sense, but you treat it like you normally would, where you exclude one category and you dummy out the remainder, and then you interact the other variable with each of the dummy variables. If you don't do that, you're not really capturing the full interactive effect, and there could be, there might be mistakes and inference that as a result of that. So yeah, the short answer is, if you've got a categorical variable that you need to dummy out into multiple dummies, you want to interact the moderator with each of those dummies in most cases.

Speaker 2  10:16  
Yeah, so if you had like, three categories of gender, you would exclude one as the base line and then include two dummies. And you know, maybe it's like you know, refuse to respond, refuse to identify man, woman, between one possibility you could exclude, refuse to identify and then interact. Identified as male with years since PhD, identified as female with years since PhD Being interactions for this gender, one to play some of

Unknown Speaker  10:59  
the scale?

Speaker 2  11:13  
Yeah, so I'm not sure what you're asking, but if it's sort of a more general question about dealing with scaling, right? So, like in this model, everything is good at zero to one.

Speaker 2  11:36  
Medical, yeah. So this is a little bit confusing. These are just names of variables. So like the reason so mail was put zero to one originally, these variables were code like education was originally coded one to six, and then I re scaled it to be zero to one. So I named a new variable, education scale, which I should have named education zero to one, much less confusing, but it didn't. So the reason why male is not doesn't get a scale here is because it was originally because it's zero to one, so there's no change. These other variables were re scaled zero to one, and I created a new variable, questions. I

Speaker 2  12:20  
Yeah. So just a couple things to think about. When you're dealing with interactions, there will be cases where you might wonder, right if you So, let's say you specify some interaction, some interaction, and you think that there might be a different variable that is actually driving the interactive effect, right? Yeah, I have an example here. So let's imagine that you think political knowledge increases the effect of policy preferences on vote choice. More knowledgeable people are more likely to translate their policy preferences votes. That's a reasonable hypothesis. You there's, you know, someone might say it has nothing to do with political knowledge. It's education. And you might say, well, you know, I don't know about that. You know, what do you do? Well, if the person says they want you to control for education, how do you do it? If you're going to, if education is a proposed alternative moderator. You need to control for the interaction of education and policy preference. That's the key control. It's not just putting education in the model. It's putting education in with an interaction with the variable instance. So say that again, right? This is your key interaction, and you're trying to control for the fact that education might be an alternative explanation for that interaction. You need to control for that interaction as well. Not just put education in the model by itself, put it in with the interaction with the other version. So if you have an interaction effect, a control for an interaction effect is itself an interaction.

Speaker 2  14:05  
If your moderator is categorical, like we were just talking about, and when I say categorical in the sense, I mean like nominal, you could estimate separate models for each of the levels of that variable with controls as well, and that would be, in a sense, controlling for that categorical variable because you're estimating different model. Estimating different models in each case, we'll deal with that a little bit more when we get to qualitative. But if it's non categorical, right, you need to include the interaction, OK? So just keep that in mind. This is something that people often don't do, and comes, another thing that often arises, and that's a reason, and that has become more salient in recent years, is that when you think about a moderating effect, right? Or if we derive the moderating

Unknown Speaker  15:01  
effect, it's sorry. Let's look at the marginal effect with with an interaction. If

Speaker 2  15:10  
you write it out in terms of the first partial derivative, the first thing you should notice is that it's linear. It's forcing a linear function onto the conditional marginal effect of x1 and that just comes about because of sort of the nature of the model you specified, right? You include the product term, and that's it, which means that the first partial derivative is a linear function of the moderator. That's the definition of the Definition Given what you did. That might not be a good assumption. It might not be a good assumption that the model that the conditional marginal effect is a linear function of the other variable, just like it might not be a reasonable assumption that age has a linear effect on income. So that's just something to be thinking that way. We tend to think about interactions as already introducing additional complexity, and then it's just like, OK, that's enough. But right, like, this is an assumption you're making. It might be adapt. So keep in mind that you are making that assumption with a simple, linear interaction. And so that might be something you want to look at. And I'll show you an example in a second how to look at that another thing that you might be thinking about, or you might not be thinking about, but you might want to think about, is that there are often cases where I'll give you an example so you can see this, it might be hard to think about without an example where, so if you think about This conditional marginal effect, you could make a prediction about what that marginal effect is for any value of x2 let's say x2 is standardized. You could make a prediction for what the conditional marginal effect of x1 when x2 is negative 10. That's probably outside the range of what you can expect x2 to ever take on as a value, and it's really far out 10 standard deviation below the mean, but you can do it. You could plug that, plug negative 10 in for x2 and get a prediction for the conditional marginal effect at negative 10. But it's probably a meaningless prediction, because you have no data that reaches that far out in the distribution. So what if you ever hear something, a phrase like this, where you know, is the predicted marginal effect you know, in an area of your independent variables with very little support? Right when they when people say support, what they mean is that, is there even any data in that region of your independent variables. If I try to make a prediction for negative 10 standard deviations of x2 someone would reasonably say there's not any support in your data for that, because you don't have any observations with those values. And so basically, what you end up doing is making predictions in a region of the data that or region of the data doesn't and you want to be careful about that, because you're basically extrapolating to values of independent variables that you have no data for. And so these two can actually be somewhat related, in the sense that you might think that the linear extrapolation to very extreme areas of the data set is a bad extrapolation. Let me give you an example so you can see I might not be up to date on this. Our packages are constantly changing, but the most recent time I've done something like this, we've used interplex package, which seems quite good, although one of my grad students did all that. So here's the example. So this is from a paper that I did with Trent polorencia, who at Houston now, and he did the work. So again, you should take a look yourself. But the basic idea was we had a bunch of interactions between personality traits and political engagement, or political knowledge. And so what we wanted to see is how the marginal effect of personality was changing on policy preferences as a function of political knowledge, right, political engagement. And so on the x axis here you have political engagement. On the y axis you have the marginal effect of each of four personality traits. And so what we're plotting is the marginal of how the marginal effect is changing as political engagement changes. And one of the predictions of the model of the theory is that it goes from positive, or, you know, negative, to positive for a lot of these traits. And so here you have the plot right. This dark line is the assumption of linearity. That's the how the marginal effect is changing as a function of engagement, assuming the linear model, that was the basic model that we estimated, what interflex is going to do is it's going to bin it's going to it's going to basically make categorical the moderating variable, and going to put it into three bins, low values, medium values, high values based on a tertile split. And then it's going to estimate the marginal effect in each of those bins. So it's going to estimate a marginal effect for low values of engagement, medium and high. And then what you can do, because you're not imposing any linear assumption. Now you're just estimating a separate effect for each of those categories. If it turns out that it's very non linear, you'll see it, and you'll see it in the data. And so that's what we're doing here. We're sort of seeing how well the marginal effect when we make categorical THE MODERATOR tracks the linear assumption in the original model. You can also see with this rough plot at the bottom where the support is in the data. So engagement in our data is pretty skewed, negatively skewed, so there's less support at the bottom. But we do have some observations, and so we're not extrapolating beyond the data completely, but you might be a little bit concerned about drawing any inferences about these

Speaker 4  21:06  
lower end how are you how to know that? I guess. What are you looking at to see that the data are skewed?

Speaker 2  21:15  
Just so this is like a histogram for values of political engagement. These little bars here, yeah, oh, I see, I see. So most people are up at the top end, and it's negatively skewed in the sense that it's got a long tail. Thank you. Is that just the average

Unknown Speaker  21:29  
for all the values of political

Speaker 2  21:40  
engagement in that category? It's actually, so it's actually simpler than that. It's taking political engagement and saying, and just break it into a three value categorical variable, low, medium and high. So lowest tertile, medium, tertile, upper tertile. And then it's just estimating the model, separating just for, for any observation that falls in the bottom third, it estimates the model, then for the middle third, it estimates the model, and for the top third. So it's actually very simple, no, no, it's usually, it's estimating three new models, and so you get a separate marginal effect for each of them, and then you're just seeing and because you just, you do that, right? It can take any functional form it wants, right? You're not constraining it in any way. It could go down and then up, like this, or whatever. And so you're trying to see, like, I mean, here, that's like, beautiful, but, you know, that's really nice to see this. You might say, well, it's actually a little bit non linear, but that's the idea. Does that make sense?

Speaker 3  22:50  
So would you get the same estimate instead of breaking it into three different models? What you do is you break the political

Unknown Speaker  23:01  
engagement. Into three categorical

Unknown Speaker  23:02  
variables and then interactive

Speaker 2  23:11  
with the other interaction. Yeah, so nothing to say that. Actually, I'm not sure what the default and interflex is, whether to include the categorical interaction in the same model, or to estimate the three separate models. We're going to get to this in like a week or two. But there's two different ways to do that. What you're talking about, right? You could ask, if you estimate three separate models, you're implicitly interacting political engagement with every variable model. If you include the categorical version of the interaction in the original model. You're only interacting the variables the feature. You're only interacting political media. Those actually are two different models, and I'm not sure what the default would say. I actually don't know what you could do either. That's a choice you can make. It would

Unknown Speaker  23:57  
also show a more stack of version of the relationship between the margin of

Speaker 2  24:04  
effective you could make the exact same plot. It would just be two different assumptions. One would allow for interactions of political engagement with every variable in the model, and one would only be interactive with the personality traits. So those are two different assumptions because you're controlling for a bunch of other stuff. There's a lot more parameters being estimated, but it's also more flexible. And there, you know, there's been some papers recently that have advocated for that kind of approach, where, if you've got a, you know, a model like this, you know, actually doing that right, estimating it separately for each value of the moderator, so that you're implicitly allowing for controlling for every possible interaction in the model, so it's more flexible but less efficient. Is that helping or actual question?

Speaker 2  24:58  
So this is something, again, check out the interplex. But like, this wouldn't be hard to do just on the ground, maybe just estimate three separate models and then block the coefficients against the original marketing condition. But this is useful, right? And so like, yeah, and for our paper. I mean, you know, a reviewer might ask you to do this, Trent is very good. And so he did this even down. Just included this in the supplemental appendance as like a robustness check, which would be very common. Okay. So the last thing I want to talk about with interactions is that you can obviously get more complicated. So you can, in principle, include any order of interaction you want. We've been dealing with two way interactions, but you could easily include a three way interaction, where three different variables are interacting with each other. So this is a generic example. We've got three three variables, x1 x2 and x3 and we think that the effect of each of these depends on both of the other two. So now the effect of one variable depends on the values of two other variables. Now the reason why this ends up getting more complicated is because you can't just include that three way product term. Here's the three way interaction, x1, times x2, times x3 here are the individual constituent terms. But now, right? You actually have three more constituent terms, because you have all of the two way interactions that are possible as well, right? So for three variables, x1 can interact with x2 x1 with x3 and x2 with x3 and generally speaking, again, right? This is, again, sort of a general rule that is almost always true. You don't want to exclude these constituent terms from the model. If you do, you're implicit. You're not, you know, averaging over them, you're fixing them to zero. You're fixing their coefficients to zero by assumption, if you exclude them, and usually that's, that's the wrong choice for exactly the same reasons we talked about. It's, you know, the analogy is to fixing intercepts to zero, and you usually don't want to do that, so you want to include these additional constituent terms. But obviously that makes things a bunch more complicated. Now, when you look at the output of your regression with a three way interaction, you're probably not going to be able to make sense and so inevitably, right? What you've got to do is plot things to try to figure it out. I'm not going to give you an example of this, you know, because it would take a while and I got other things to do, but if you run into the situation, right, you need to just be very deliberate and careful about going through and plotting the different possible marginal effects and how they change as a function of the others and trying to make sense of it. I'll give you just one sort of generic example, if you had three variables, and let's say one of them is binary zero or one, one way that you might want to go about plotting, that would be to have x3 equals zero, x3 equals one, then you've got x2 right from low to high, and then You've got, say, the marginal effect of x1 it's

Speaker 2  28:25  
like that or something, but that's the kind of thing you're going to need to do to figure out what's going on. It's very hard to interpret a three way interaction just from the coefficients. Even it's hard to do a two way interaction, much harder to do three way but the logic is the same, right? So you might say, well, you know, if I want the marginal effect of x1 what in the world do I do with this? Stop take the first partial derivative with respect to x1 and you don't have to worry. There it is. It's more complicated now. Now what you've got is a two way interaction where the marginal effect, right, the marginal effect of x1 is an interactive function of x2 and x3 that should make sense, because it's the three way interaction. So now the marginal effect of x1 is itself a two way interaction. It's more complicated, but once you've got that right, this wouldn't be too hard to turn into a plot like that, right? Once you can write down that function and write down that function derivative with respect, all of the same caveats apply right now. You're making lots of assumptions, linear assumptions. So you know, I've done this in my career, probably too much you need you should have strong theory, and you need a lot of data to get any sort of reasonable estimate of these things. You need a lot of data. It's a couple 100 observations. You're not even come close to being able to estimate this reliably. So you should be thinking like a couple 1000 observations of

Speaker 2  30:06  
these okay interactions, take practice. You really just, you need practice. All right, so we've talked about different ways in which non linearity enters into regression models, and that makes things more complicated just for trying to interpret, just doing basic interpretation of the effects of the variables. But it also makes getting uncertainty bounds, characterizing your uncertainty in those estimates quite a bit more difficult as well. And so just giving an example right? And the reason is because we can't rely on just the stuff that comes out of summarize, summary LM. So summary, LM will give you standard errors for each of the coefficients, but like, if you have a model up here, right, with an interaction between x1 and x2 and you want the standard error for the marginal, the conditional marginal effect of x1 when x2 is one that doesn't come out of summary L M, right? Summary L M will give you standard errors for beta one, beta two and beta three. It doesn't give you a standard error for the conditional marginal effect of x1 that's some value of x2 that's not something that just comes out of the can function. So you're going to have to do more work. You want something like that, and almost always you do want something like that. But think about another example. If you've got a log dependent variable, and you're calculating the marginal effect on the level of y, right? That's not going to come out of summary, dot L M. Summary, dot L M is going to give you the standard error for the coefficients in log space, not in level space. So we need alternative strategies, and we're going to talk about four, four ones. The first is, is, you know, will be useful for cases where you're dealing with interactions or polynomials, in many of those cases. So analytic solutions are ones where we actually have a formula for the standard error of the thing we're interested in. That's sometimes the case, but not always. There are many cases, and as you go on to more advanced regression, there'll be many more cases where there just is no analytic solution for the variance of the estimator for the thing that you're trying to estimate. And so what do you do in those cases? Well, we're gonna talk about three different possibilities that are all very popular. The first is simulation based approximations. The second is boot scrapping, again approximate. And these are all approximations. And the third is called the delta method. Again, all of these are very popular. I would say that the delta method is the default for most social scientists, and the reason for that is a combination of stata and the new packages that R has. So in STATA, delta method is the default. So that's what people use in R the best package I have currently found to do this stuff is marginal effects, and that the default is also a delta method for that. Simulation based approximations when I was in grad school were private default because Gary King had a package called clarify in STATA, and this was before stata had its marginal effect package. And so people use that, and that's still popular, I think some degree. And then boots tracking should be probably the most popular, but it's not super good. It gets more complicated, but we'll talk about that. But the basic idea, again, right, is that you've got quantities of interest that are functions of these things rather than the things themselves. And once they're functions of these things, we can't just look at summary LM and know standard error. We have to use more work. So that's okay. So analytic solutions for cases like interactions, conditional, hard, null, effects and interactions are actually not are actually readily obtainable, both because the formula is not too bad, and packages, you know, can get them to you very easily. So let's just start by remembering which you've done a couple times now, right? The variance of a linear combination, right? So why are we thinking about the variance of a linear combination? Well, a lot of the quantities of interest for us are going to be linear combinations of the betas. So and what do we want? Well, we want an estimate of the variance of some Lin of some linear combination of the betas. So maybe it's, you know, beta one plus beta three times x2 that's a linear combination of these things, we want the variance of that thing, and so we need the variance of a linear combination. This is the general formula for the variance of linear combination. If you have some constant multiplying a variable x and some other constant multiplying variable y, the variance of that linear combination is the constant of the first square times the variance of the first plus the constant of the second square times the variance of the second plus two times the each of the constants multiplied times the covariance. Eventually, you can memorize that if you need it, but you can always just look at the deal. But me show you an example of how this actually works out in practice. So let's think about the standard we want a standard error in the context of an interaction. And what we want the standard error for is the conditional marginal effect of x1 so we've got this generic two way interaction between x1 and x2 we're interested in the conditional marginal effect of x1 we take the first partial derivative. It's beta one plus beta three times x2 and now what we're going to do right is we're going to plug values of x2 into that function, then plot it. But we also want confidence bounds for it. So we're going to plot the conditional marginal effect of cross x2 like something like over here. But what we ultimately want is not just that, we also want the confidence bounds for it right, a measure of uncertainty in the conditional marginal effect for each of those values. That's what we're trying to get to. And so we got to figure out right how to calculate those measures of uncertainty for this quantity that is itself a function. And the answer is actually pretty straight forward. We just plug in right the value, if you notice, right right here, this looks exactly like this. It's a x plus b y is one beta one plus x2, times beta three. A X plus B y, it's exactly the same form. And once we recognize that, we can just plug in the values into the formula and get exactly what we want. And so it's actually quite easy to do, even by hand. So if we look, if we actually do that right here, we can get the standard error for the conditional marginal effect of some variable x, and it's just going to be this formula, right? So here we've got, and the reason why we're taking the square root here is because the standard error is the square root of the variance. So this is the variance, the square root of that is the standard error, but otherwise, inside it's just the same formula. Here we're implicitly multiplying by one because there's an implicit constant of one beside the first variable, so one times the variance of that, beta k, this would be beta one. Here, the second, the second term right. The variable is the interaction term. So here I'm representing that as beta k, subscript kl, where we've got two variables, interactive right, x, k and x and x l. So this is the variance of that interaction term. In this case, it would be theta three. The multiplier is the constant associated with that variable squared, which is x2 x2 squared. In this world, B is x2 and so that goes in as x2 squared. The last term is two times each of the constants, so two times A times B. A is 1b, is the interactive the moderating variable times the covariance of the two variables, the CO the two variables we're talking about again, are beta one and beta three or more generically, the constituent term and the interaction term. So the constituent term and the interaction term have covariance, and that's what you plug in for that. And notice, right, all these things are readily obtainable. The you know, the multiplier for that second term is just the moderating variable. So that's you have that the variance of beta k and the variance of beta k, l, so the variance of this, this and their covariance are just from the covariance matrix of the beta extracted a bunch of times. You can pull all those things out very easily plug them into this formula. So

Speaker 5  39:22  
steps, says that the standard error with its conditional marginal effect of x1 given its interaction with x2 but that means that each estimate is for specific values

Speaker 2  39:37  
exactly, exactly. Yeah, and that's what you see over here, right? So if you remember that the y axis here is the conditional marginal of theft, we have a confidence bound at every possible value of s2 right? Each each slice here is itself with a confidence

Speaker 5  39:52  
bound. So you would be doing that calculation across your values of

Speaker 2  39:57  
x2 Exactly. And I'll show you a concrete example of that, conceptually, are we sort of getting in? All we're trying to do is shove this into this. And so if you say, OK, well, how do I do that? Just think about, well, you've got, this is x, this is Y. X is multiplied by, implicitly by one, so A is 1b, through the second variable is multiplied by x2 so B is two, is x2 you're just trying to map this to this.

Speaker 2  40:37  
You can also use this generic approach for quadratic column. I mean, you can use it for any order polynomial. For a quadratic polynomial, it fits very simply to what we just did. And if you remember right, the only difference is that the that I ended right in there today, oh, we have a quadratic model. If we take the first partial derivative with respect to x2 where x2 has a quadratic term, right, the corresponding conditional marginal effect now is the first partial derivative, beta two plus two times, beta 3x two, right? So the only difference, right, it has the same exact form, except there's this additional two that gets goes along for the ride, and that just means that when you plug that in, you got to remember that it's there. I forgot this many times. That should be there, right? And it's also over here as well. That's the only difference. But otherwise it's the same logic, right? It's just an interaction a quadratic a quadratic polynomial for a variable, is just an interaction of that variable itself. So it makes a lot of sense that it should be basically the same idea. Just remember the two. OK, so let's take an example. So here's an example with an interaction. We're not going to pay attention to the quadratic term. We could do an example with that too, but we're going to focus on the interaction with between income and political knowledge again. So what are we trying to do? Let's say we want to know how the effect of income is changing across political knowledge. That's our key quantity of interest, right? So what we want to do is calculate, derive the function for the conditional marginal effect of income as a function of knowledge. We want to plot that, and then we want to put confidence bound on them. We want something that's going to look something like that. So again, first thing to do, say, I want the conditional marginal effect of this. Take the first derivative with respect to income and see what we get. We do that, we get beta five plus beta seven political knowledge. And that's just coming right out of this, beta five plus beta seven times knowledge. So this is the quantity of interest, but it has a variable, so that means that it's going to be a function, a linear function, like I drew over there. But we just want to take those quantities and plug them into our generic formula. So that's what I'm doing. I'm going to create a new variable called the standard error of income that's going to be the square root of everything in here. Let's go back and take a look at what our formula looks like. It's going to have the variance of the constituent term plus the moderator square times the variance of the interaction term plus two times the moderator times the covariance of the constituent term in the interaction term. So I've got three components, those three components right here. I'm even writing in the implicit one. There's implicitly a one here. I'm just going to write it in to be very deliberate about what's about the formula. So it's a squared times that variable. What's that variable? It's the variance of this estimator. Well, I can just pull that out at the variance, covariance matrix, the code of my model, and then just get the variance of the income of that I don't even have to memorize the number. I can put in the variable name and get that entry in the diagonal of that covariance matrix. The second term, this is where. This is where. It's a little tricky, but we'll make sense of it. The second term is the moderator square times the variance of that interaction term. Well, what's the moderator square? You can pick any value of that moderator, but what I want is a line, right? So I want a bunch of different values of the Moderator. So I'm just going to plug in a sequence of values from zero to one in in movements of point o5 so this is going to be 21 different values of knowledge from zero to one. Knowledge scale ranges from zero to one. And I just want a series of values across zero to one in relatively small units of movement. And that's that's going to allow me to write a draw a plot like this. If you wanted just a single value, you can just plug in whatever value you're interested in. But if you want to do something like this plot, you want to plug in all possible values. And R is smart, it'll just do the whole calculation for every value you give it. I'm plugging in all the values I'm interested in, of the moderator, squaring them and then pulling out the variance of the interaction term. The last term is two times A times B, so two times one, times all the values of knowledge that I interested in, times the covariance between the constituent term and the interaction. So those are the three components of that formula. You just got to be deliberate. Think about it and plug them in carefully. Once I've done that, I'll then have a standard error for every single value of knowledge, for the marginal effect of income, and I can use that to construct confidence interval, and this is what I would get. So here is on the y axis, the conditional marginal effect of income. On the x axis is political knowledge. The dark line, right, the solid line, I should say, is the conditional marginal effect itself. We can see, as we saw in the last class, it's declining as a function of political engagement. But now I've got two additional lines that are telling me the confidence balance on those conditional marginal effects for any given value of political knowledge. I can draw a line, take a slice, and say, here's the predicted conditional marginal effect, and here's the 95% confidence

Speaker 5  47:05  
bound on that conditional margin. So does it actually matter by what scale point, oh, five, etc, is it just, should we just look at the variables themselves in the scale and make something that's like some level

Speaker 2  47:20  
of Yeah, yeah, that's a good question. There's a sense in which it doesn't matter. And the sense in which it doesn't matter is that, because this is linear, if you just picked zero and one, you could draw a line between the two points. The problem with that is that when you draw the confidence bound, you would only get those two points and then adjust straight line in between them. And the way that that uncertainty works is that the uncertainty and the conditional marginal effect will be smallest at the mean of the moderator. That's where the most information is. And so as you move away from the mean of political knowledge, you can see the confidence bounds get bigger, and the more fine grained a set, a movement unit that you use, the better you'll capture that non linearity. So the confidence bounds are non linear, even though the effect is line, the marginal effect function is linear. And that's because, again, the information is most concentrated around the mean, and gets smaller as you get away from and so the confidence bound should be non linear like this, and they'll always look something like that. And you can capture that better with small let

Speaker 2  48:35  
me show you another example. I'm not going to go through and show you that calculation again, and eventually, not too long now, we're going to get to just using a package in the first place. You might be, you know, two reasons I got well ended. But you know, first, you want to know what's going on under the functions. And we're going to come up, I'm going to show you examples later, especially when we get the bootstrapping, where the defaults right, it might not be good choices, and that might indicate that you need to do something yourself, or at least know what's happening in the function. So you want to be able to do this, I think. But we will get to a package. So what I want to show you is I'm going to do the same thing for the conditional marginal effect of age, which is a quadratic effect. But I'm not going to show you the code for it. That's there in the lecture, in the lecture UMD file, if you're interested. What I want to show you is the end result, because this is an interesting case right here. This is the conditional marginal effect of age on the y axis as a function of age itself. And again, the reason for that is that we have a quadratic polynomial for age in the model. So that means that the effect of age is changing as a function of age itself. So here I got the conditional marginal effect of age. It's getting it's starting out negative at low values of age, it's getting going closer to zero as we increase in age, and then eventually it becomes positive. Now what's nice about having the confidence bounds here is that at every value of age, I can do a significance test just by looking at the clock, and because this is a 95% confidence bound, if it overlap zero, I can say the conditional marginal effect is insignificant at that value of age. So all the way up until here, at about point five, the effect of age is negative and statistically significant. But once I get to about point five or point six, it becomes statistically insignificantly different from zero, and I can see that because the confidence bounds are overlapping the zero confidence,

Speaker 5  50:51  
using the standard errors we calculated with the analytic solution, but that requires us

Speaker 2  50:59  
to pursue right so in this case, it depends on whether you're assuming normality or not. What but if you assume normality, you have finite sample t values that you can use. If so in calculating the confidence interval, if you if you can assume the errors are normally distributed, you can calculate the confidence bounds using the T, and then they're exact. If you can't assume normal errors, then you're exactly right. The default that you should do in using this analytic solution is just use the T for the reasons we talked about previously, which is that both the estimator and the T converge to normal the inclusion. Conceptually. Does this make sense? I understand that the underlying calculations might be practice or whatever, but the basic idea is you've got a quadratic relationship between age and economic preferences. That means the effect of age is conditional on itself. We take the first partial derivative, we get negative point one, six plus point one, two times age. We plot that as a function of age, and then we use the method I just showed you to get the confidence bounds for that condition.

Speaker 2  52:30  
Okay, let's take a look at what this might look like in using a package. So the marginal effects package, which I've got a link to it there, is quite good. It's pretty relatively new, but it's got a lot of functionality. Seems relatively stable at this point. People like it, they're using it, so I feel reasonably comfortable with it, but I've been burned in the past. I thought when I taught an advanced regression Class A while ago and I was teaching Gary King's extended software called selling for R it was like it had all these errors, and just like a disaster, it does. It happens, you guys. So you know, as always, right? Buyer beware with any package, but I feel reasonably comfortable with this one. Okay, so let's see. Let's see. Let's try to do something, right? So this package is really easy to use, which is great. So there's this slopes function. So here m5 is just one of the models that I've estimated. I've estimated a model and I stored it in m5 if I call slopes m5 and then tell it a set of variables that were in the model, it's going to give me the marginal effects for each of those variables that I call because, sort of, marginal effects was created with the idea that, you know, it's hard to get this stuff for non linear models. It's sort of assuming that you're dealing with a situation like that right off the bat. And so when you call slopes, it's going to estimate the marginal effect for every observation in the data as a default just like we talked about. So if you don't want that, right, if you want to set your other variables to particular values, you got to tell slopes that. That's what you want to do. So here I'm saying I want the marginal effect for the variables in n5 but only for male education and income. And what I want you to do is not calculate the marginal effect for every observation data. What I want to do is hold all variables at their mean and then calculate the marginal effect. So this is kind of like a default use, although I'll show you another second. So if I do that, what do I get? Well, I get this table out. What does this mean? Here's each of the variables that I wanted, education, income, male, what's this first thing here, contrast. So what that so marginal effects is smart, somewhat smart, if it knows, or at least it's trying to guess what the underlying variables are like, right? So for education and income, it's guessing that these are that I want them to treat these as continuous. And so this contrast, d y, d x, is saying this is a marginal effect, and it's a marginal effect because it's a continuous variable. But for male, right, which is coded 01, it's guessing that I want this treated as a categorical variable, a binary variable. And so it's saying I'm just going to do a mean difference here, one minus zero. I'm not going to call it a marginal effect, because it's categorical. The marginal effect is smart in that way. And it's guessing that I want marginal effects here, but I want first difference here. That's nice in some many cases. That won't matter for the reasons we've talked about, but it does, then it's nice to have guess. So that's it's telling you marginal effect, marginal effect, first difference, it's giving you the estimate for each of those things, right? So here's the estimated marginal effect of education when everything else is held to a mean, when everything is held to its mean. Here's the marginal effect of income when everything is at its mean. Here's the first difference between male and female when everything is held at its mean. Here are the standard errors for those effects. I'll talk about this in a second. Here's a Z value, a p value for that Z value, and a 95% confidence. So this is super easy to use if you want to get something quick right that is not coming out of the default summary, dot, L, M, output, do so

Speaker 2  56:45  
we'll go through more of this stuff. The first question would be, why is this z and not t? And the answer is, because, and this is again, right? You know, know what your packages are doing. By default, the marginal effect package will give you the delta method, standard errors and significance test. We haven't talked about the delta method yet. We're going to do that next week, but that's its default. And it doesn't matter what model you estimate, it's going to give you delta method, standard errors. Even if you want your estimated simple linear model and you want t values, it's going to give you delta method standard errors. And that's may not be what you want, right? It makes sense in the context of the LIN of a non linear model, you know, for reasons we're talking about. But this is just a simple linear model. You're going to get a Z value even if you have, you know, even if you can assume normal errors, and you have a sample size of 50, it's still going to give you delta method standard errors, and those are that's a bad choice in that context. So this is the default. You should know that. Okay, so, so that's you know, if I want effects holding everything at particular value. So I can easily do that with this function. What if I want to do something like I did over here? So in this case, let's say I want the conditional marginal effect of income, holding knowledge at a variety of different values. So basically, I want to do the exact same thing that I did with this code over here. I want to do this exact same thing, but I want to use marginal effects package instead. Not too hard again. I can call slopes. This time, I only want the marginal effect for one variable income scale. And here I'm going to, instead of putting everything at its mean, I'm going to choose values of knowledge that are interesting to me. So here zero to one by point one units actually did something different than the last time I did point o5 the last time. So I'm just saying hold knowledge at a set of different values and estimate the marginal effect of income in each of those values. Data grid here is actually unnecessary in this particular case, but I'm showing you because it's the expand, it's the grid, dot expand function just put into marginal effect. So if I had two variables here, say knowledge and like gender or something, if I did data, dot grid, it will give me the expansion of both every possible combination of values for those two variables. So that's very convenient if you want to calculate the effect for a whole set of variables that are combined with each other. But here I just want the same thing I did before. So if I call this with slopes, it gives me the estimated conditional marginal effect of income at every value that I specified for knowledge exactly the same as I did before, gives me the standard error, Z, value, P value, 95% confidence interval. If I store this in some kind of table, I can then just plot right my estimates and the 95% confidence interval by just pulling out the relevant columns in the table. And that's exactly what I do in the code these slides, which you can see if you're interested. And I get the exact same plot basically that I got before. So this is doing this, except with the delta method. In the delta method for approximation. So again, right? Keep that in mind, this is actually exact, right? This is the analytic solution. If you call marginal effects, right, you're going to get something that looks very similar in most cases. But it's the delta method approximation. It's not going to give you, right, the analytic solution. Now, in these data, I have enough data that doesn't matter because delta method, delta method is asymptotically normal, and the analog and mine, basically they converge to each other as like T converges to normal as n goes to infinity, and the delta method converges to normal as we'll talk about as n goes to infinity. So these converge to the same thing, which is why it looks almost identical, but with a small sample size. Again, this might not be you might want to have to do you might want to do this yourself using but for most cases, right? You have decent amount of data. And there's other you know options that I'll show you in marginal effects that you could use, like boot strapping. The last thing I want to show you marginal effects is that so, as we talked about, right? In cases where you have a non linear model and there really is a different effect for every observation in the data, usually what you want to do is average over those observations, right? You want to calculate the marginal effect for every observation in the data and then take the average of all of this. Well, marginal effects has a really nice default, a really nice function that you can use that will do exactly that without any other work. So if I use average slopes instead of slopes on my model fit function and a set of variables I'm interested in, it will do exactly that. It will calculate the marginal effect of each of these variables for every observation in the data, and then it will just average over all those estimates and give you the average as the estimate. So in many cases, that's what you're going to want to do, especially when you get to more advanced models. And so this function would be like a work force for you. This is what's going to give you what you would want in most cases, like in STATA, if you're running like a logit or a probit, and you do marginal effects, this is exactly what it's doing. So it's almost like an attempt to replicate Stata.

Speaker 2  1:02:38  
Just another thing that this can do. There's another function in the marginal effect, called hypotheses, that will do the same thing with the car. The car package, linear. Test function, linear, linear. Something function, linear. Contrast. Did that I showed you a couple weeks ago. You could test coefficients against each other, you can test one coefficient against zero. You can test a coefficient against point five. Whatever you want, right any linear combination of coefficients you can test using hypotheses and marginal effects. So that's another option for you, beyond the car package. If you like marginal effects and want to use it, you can do most things you want to do within that package. And again, the default is going to be delta method, the standard error. So keep that in mind when I showed you these hypothesis tests, we use T for asymptotic properties, it's using the delta method, yeah, oh, sorry, okay, so summarize in many cases, right where you have interactions, or where you have interactions or polynomials, right? There's a relatively straightforward analytic solution.

Speaker 6  1:04:02  
I it's

Speaker 2  1:04:11  
actually the case now that I think that I want to think about this just a little bit more. But in the case of the simple conditional marginal effect in an interaction, the delta method solution is actually going to give you the analytic solution. They're actually

Unknown Speaker  1:04:27  
the same. The caveat,

Speaker 2  1:04:29  
I guess that I said where you want to be aware that this the delta method does hold right, but it actually doesn't matter these simple cases that we're dealing with, because the delta method solution, if you actually go through the math, you end up with the analytic formula for the standard error. So I was being a little confessionistic here about about that the delta method is going to give you the same answer as you're going to get with this, this formula in these cases, because of the nature of the approximation in these simple cases where you have a linear marginal effect function, okay, yeah, so I think so the up shot right is that. Let me summarize, in cases where you've got interactions or polynomial functions of independent variables, right, and you're interested in these conditional marginal effects, and you don't have like, something like a log variable on the left hand side, there's no other complexities, right? There are these analytic solutions for the standard errors that will allow you to calculate confidence bounds significance tests using a formula, right? You don't need any sort of method of approximation marginal effects is a great way to do this without having to do by hand. I was saying before, right, that you should be careful knowing this is the delta method. But now that I think about it, right, the delta method gives the same analytic solution, just for a different mechanism. And so in these, these sort of simple cases, you can use marginal effects

Unknown Speaker  1:06:05  
because it'll give the exact same answer. It's not in that sense of the prosecution, and I can show you that. So this is a very easy way to do something.

Speaker 2  1:06:20  
So the up shot of all of this, I guess, is that if you've got these cases with interactions or polynomials, right, you're going to want to start playing around with marginal effects, because that's going to make it easy for you to make, you know, get stuff like this, and then make plots like this, and we'll give you problems for the next problem set.

Speaker 2  1:06:50  
Okay, I mean, I can't really go on the simulation today, but we'll pick that up again next week. But before we end, are there? I know there's a lot of stuff here, like conceptually, are things trying to make sense? Are there, like conceptual issues that are coming up that you're struggling with, trying to

Speaker 6  1:07:20  
think about what we're doing so right.

Unknown Speaker  1:07:25  
Well, so, oh yeah, we have left.

Speaker 2  1:07:35  
I think we're gonna, I think we're probably gonna cancel that for next week, right on the border of spring break. So I'm gonna week travel. I think we'll cancel next week. But what I would probably do, and I'm not sure that's definitely it will be to make that time available in a sense, like collaborative office hours, so that what I'll do is post a problem set this week. It'll be due after spring break. You will have lab this Friday. Next Friday, you will have, like an open office hours type thing, where any of you are working on that problem set. You don't have to, but if you are, you can come by and you might talk through problems or whatever. So it won't be like a required lab, but it

Unknown Speaker  1:08:21  
will be available.

Unknown Speaker  1:08:22  
I Yeah.

Unknown Speaker  1:08:25  
Again, this definitely a term you have a topic list.

Transcribed by https://otter.ai
