Unknown Speaker  0:00  
Be about. And once you know that, in terms of the short

Speaker 1  0:10  
answer, substantive is that, do you also want us to say whether or not it would be statistically significant, just in the fact that, like the substantive interpretation kind of relies upon whether or

Speaker 2  0:24  
not it's significant. Yeah, if for some reason I wasn't explicit in asking for that, it would be a good idea to use statistical significance as part of your substantive interpretation, because that would be standard for paper. But I would try to be as explicit as I can for the things that I'm asking you to do. But if I for some reason, I forget or something, it would be good just as a default to if you think about it as writing an article, interpret the coefficient. What does it mean? And then this is statistically significant to be like a standard kind of sequence of things. But more questions

Speaker 3  1:04  
Yeah, once you put our mid term grades in campus, would you say that the total grade that's calculated is accurate, or, like, the Canvas grade is different than the ones that we might get? If that makes sense,

Unknown Speaker  1:16  
are you talking about just the mid term grade? Or,

Speaker 3  1:18  
no, not the overall the overall grade, yeah, just the way I know, like, how it's progressing, yeah,

Speaker 2  1:28  
there. I mean, if it depends on how you look at it, right, it's accurate in the sense that, you know, given the information that I have available, it's, it's doing the calculation that you know, I have listed on the syllabus 20% for this and 40 the reason why it's it's not, you know, you might interpret it as a good or bad estimate of the final grade, depending on how you think about the fact that you know the quiz grade is taking the full it basically taking the quiz grade you have now, and using that as the final quiz grade, right mid term, and using that as the final exam grade. And so depending on how you project yourself into the future, it's more or less accurate. But if you assume that every quiz and exam is taken from a random, randomly drawn from some distribution, then it's like in expectations, right to

Speaker 1  2:25  
go back to the material, in terms of the actual knowledge, like some of the chapters start to touch a little bit about upon z scores in the sense of overall standardization. And I'm just wondering if that is in something that we should be familiar with in terms of interpretation.

Speaker 2  2:45  
You should I'll give you an answer. Tell me if I answer your question, but you should definitely be able to interpret standardized, co efficient from models where the variables are standardized. For these stories,

Speaker 1  3:01  
is that into, like a 01 sort of binary from

Speaker 2  3:05  
the minimum value. So, yeah, so 01 you should also be able to interpret basically, you know, one sort of standard kind of problem would be that I give you a regression table, and I tell you how all the variables are coded, and I ask you to give me a clear, substantive interpretation of the coefficient given what I told you about the variables and how they're coded. So I might tell you that the variables are all code zero to one, or I might tell you they're coded in their scale, mean, zero, standard deviation of one, or I might say it's in dollars or whatever, and you should be able to move freely among those different possibilities.

Speaker 2  3:53  
Right? Feel free to reach out before the exam if you have more questions. So I want to start today by just returning basically at the end of last class, I started thinking about the delta method while I was telling you about the output of this table. And then I confused myself, and I probably confused you. So I just want to clarify situation, so as I'll show you, when we get to the delta method in a case where, in many cases and cases with interactions or polynomial terms, as I'm going to show you, the delta method actually gives you a perfect approximation. It perfectly reproduces the analytic standard error for, say, a conditional marginal effect. In an interaction model. You'll see why that is. But for now, just in those kind of cases, you end up with a perfect reproduction of the analytic standard error, rather than a delta method approximation, they get the same answer. And so what I said was, oh, maybe this, you know, because I was talking about how, marginal effects always default to Z scores and assumes the normal distribution for the sampling distribution, and how that might not be what you want, because if it's a t distribution, then you're getting something worse than you would if you t then I confused myself by saying, Well, if the delta method standard error is The same as the analytic one. Maybe it doesn't matter, but I was right the first time. It doesn't. You know the standard error from the delta method might be the same as the analytic standard error. But once you have that standard error, you have a choice whether to treat it as a T or a Z, and marginal effect is treating it as a Z. And so if you can assume normally distributed errors, then it should be a t distribution. And if you have a small sample where you can assume normally distributed errors, it can make a difference. So the upshot of all of this is that it's true marginal effect uses delta method as its method of calculating standard errors. It's also true that in some cases, the delta method will give the same answer to the standard error as the analytic solution, but the choice to use a normal distribution instead of a T can be consequential Nonetheless, if it's a small sample where you're assuming normally distributed errors, and in that case, you might prefer to use a T, because it's more accurate and you know the T is the right distribution, in which case this is only an approximation, and you might hear about that. So sorry, sorry, I confused. I confused that last time, but I was right the first time. Right. Be, just be wary of that one situation, right? Where you know the finite sample sampling distribution is T. You have small sample, so it's T is only you know somewhat related to the normal. And if you use marginal effects, just know it's defaulting to the normal, right? It's basically assuming the asymptotic distribution, not define it, but in most cases, that's not going to matter. Okay, so we've talked about the analytic approach so far. And you know, in many cases, that's that sort of totally reasonable approach. And you know, like I we said here, even though marginal effect is using the delta method in many cases, that will still give you the exact analytic solution anyway. And so that's a good way to think about things. For interactive models and for polynomial models, where there is an analytic solution. That's not always the case. It won't, will not always be the case that there is an analytic solution. And the cases where there isn't an analytic solution are like sort of more prototypical non linear models, like having logs on the left hand side, or when you get into like logit and probit, and you want to talk about predictive probabilities, there's no analytic solution. And so there's other strategies we can use, and we're going to talk about three, and all three of these are going to be approximate solutions for those kind of cases. And so we'll talk about simulation, boot strapping and the delta method, and all of them are very common. Means simulation. What do we mean by simulation? By simulation, we mean repeat. In this context, by simulation we mean repeated sampling from the sampling distribution of beta so we have a vector of beta paths or coefficient estimates from our model, and we have the variance covariance matrix of that, and we have some assumption about the distribution, so maybe it's worse, and we are going to assume a multivariate normal distribution. Once we know the coefficient estimates the variance covariance matrix, and we make an assumption about the distribution, we can start drawing from that distribution. It's just a multivariate normal, right? We can start taking random draws from that distribution. We can do it as many times as we want, and every time we do it, we have a simulated vector of coefficient estimates. Now think about what if we did that many, many, many times, right? We're going to have a whole set of simulated coefficient estimates, and that set is going to be an approximation of the sampling distribution itself. Now you might say, Well, why would you want to do that? Given that we know this, we've assumed the sampling distribution Well, it's useful if what you actually want is some function of the betas rather than the betas themselves. So if, instead of summarizing the uncertainty in beta one. What I actually want is beta one plus beta three times x2 that's, you know, the sampling distribution alone doesn't tell you that. So instead, what we want to do is to use each of those simulated coefficient vectors to calculate this function. And then, if we do that, you know, say, 1000 times, you then have a sampling distribution for the function of the betas, rather than to the beta themselves. So another way to put that right is just that, once we have all these draws from the sampling distribution, we can calculate whatever quantity we're interested in for each of those draws, and then summarize the distribution of the calculated quantities, and that is an estimate of the sampling distribution for that function of the beta. And once we have that right, we can use the characteristics of that distribution to calculate things that are of interest to us, like the standard error of this function of the data. So we can just take the standard deviation of all of those simulated quantities of interest, and that will give us an estimate of the standard error that quantity of interest. We can calculate 95% confidence intervals by just looking at the 2.5 and 97 point fifth percentiles. Or you can just the sampling distribution itself and give your reader as much information as you want. So if this still seems a little fuzzy, we're going to do an example. But this is, this is actually very easy to do by hand, and I'll show you what you know, what the sim function actually does under the hood. It's not much, but I like to use this arm, the arm package and the sim function from the arm package. The arm package is applied regression modeling. It's Gelman, Andrew Gilman and Jennifer Hill's package for their applied regression modeling textbook. It's got a lot of stuff in there that's useful, especially if you do hierarchical, multi level modeling. But for our purposes, we're just going to use their sim function, which is a convenient way to do this, but it's very easy to do by hand as well. Marginal effect does have the functionality to do simulation, both simulation and bootstrapping through they have a function called inferences that you can use after you've done your initial call marginal effect, and I'll show you that. But they label both of these experimental. And as you'll see, for boot strapping, it's actually just calling the boot package from marginal effects. So it's just using this other package, although it is kind of convenient for some purposes, but for sim, you for simulation, you actually have to be a little careful, because there is a difference between marginal effect simulation approach, as far as I can tell, and arms. Simulation approach and arms is better, in particular, as you'll see, the arm sim function sample, first samples from the sampling distribution for sigma, and then uses that simulation to sample for beta, and so it propagates the uncertainty and the error variance into the uncertainty. As far as I can tell, marginal effects does not do that, so it's a little hard to tell. I looked in the help file and it seems like they don't. So I would, if you're going to do simulation, I would use on just for that reason, and also the fact that they all caps say experiment. OK, so let's take an example. So we have some model four here. Let me see if I can pull up what model four is just, we're

Speaker 2  13:37  
all, yeah, I think we're just, we're again, just dealing with, you know, one of these models, let's just, I think we're just dealing with this. So we're predicting economic preferences from income education, okay, so it's easy enough. Once you have the model logic from LM, you can just call the sim function from arm. The first argument is just that model object, and then you just tell it how many simulations you want. The number of simulations is the number of times it's going to draw a simulated coefficient vector from the sampling distribution for the coefficients. So this is going to end up storing a matrix that has sins number of rows. So if it's 1000 here, it's got 1000 rows, and it's got one column for each of the coefficients in your model. So actually, we see exactly what the model is here. There's an intercept, age, quadratic age, male, education, income for So again, what is it doing? There's a sampling distribution these coefficients. It's multivariate normal. That's what we're going to assume. Every time you reach in and pull out a simulation from that sampling distribution. We're pulling out a vector. We're pulling out a vector at length k plus one, because there's k plus one coefficients, it's a coefficient vector. We're pulling that out of a multivariate normal, and then we're just stacking them on top of each other. So if we do this 1000 times, we get 1000 vectors of coefficient estimates. Because we're pulling them out of the distribution, they're going to vary from, from, you know, from time to time, right? Each time you reach in and pull out another vector, there's going to be random variation in each of the coefficients, so they're different each time. And the variation across all those 1000 rows, like, if you just look at the intercept, the variation across the 1000 rows for this column, for the intercept, tells us about the uncertainty in our estimate for that intercept. Similarly right the variation across the coefficient for age tells us about the variation in that beta one.

Speaker 1  15:57  
So when we're actually taking one singular sample, and we're not doing over repeated simulations. Is it just taking like? Does it matter like, if we're doing like, we're just taking m4 as an L on function, and then we get the first value? But is there always some understanding that it could just be changing slightly? Well,

Speaker 2  16:17  
you would never, you would never want to only use once, right? Because you if what you want is just the coefficients, the best guess coefficient estimates, then you just use your LM output, because that you know the summary of m4 is giving you your best estimates of all these coefficients. The only reason we're using simulation here is to try to characterize the uncertainty in this estimate.

Speaker 4  16:42  
Yeah. I But how do we know that the variance of the coefficient here is an unbiased estimator of the actual

Speaker 2  16:56  
variance? So it's not it's not unbiased, it's consistent. It converges asymptotically. Yeah, so, so it's not an unbiased and that's one of the limitations of simulation, is that we're assuming the asymptotic sampling distribution we're going to we're assuming that we're, let me put another we're taking draws from a multi variant normal with a mean equal to the true coefficient estimates from OLF and a variance covariance matrix. That is, I mean, I'll show you exactly what it is. But basically, you could just assume that our estimated covariance matrix over here, and we're assuming a normal distribution. So the normal distribution is the asymptotic distribution for overlap, but for a finite sample, it's only an approximation. We can think of it as being like, you know, this sampling distribution we're using here is what we converge to as n goes to infinity.

Speaker 4  17:53  
And to clarify like the N that you're using to say that as n go to infinity we convert, is it the number of simulations, or is that the number of

Speaker 2  18:04  
sample? Yes. So the asymptotic distribution for data is, is a function of the sample is going as the sample size goes to infinity, the number of simulations is, how good your approximation of the approximate distribution. So you so think that so we had an approximate sampling distribution, because we know what the asymptotic distribution is, but we only have a finite sample. How good we do at approximating that approximate distribution depends on the number. But this is so easy to do, there. It's never an issue, right? This is, this takes less than a second taste, like, you know, a 100th of a second to do. And so getting 1000 10,000 50,000 is extremely easy with current computers. So this the approximation that's due to the number of simulations is never an issue. We can always just, you know, if you're worried about it, just add

Speaker 5  19:07  
another 10,000 Could you talk a bit more about how we're justifying our assumption that the beta is drawn

Speaker 2  19:15  
from a multi value distribution? Yeah, so, so if you remember back to our week on asymptotics for OLS. We showed that the asymptotic distribution of beta is is normal and so and so. Basically, what we're doing, we're drawing from a multivariate normal, because we're just going to for this approach. We're going to assume the asymptotic distribution, which is also what we do whenever we can assume the normally distributed standard error, normally distributed error term. In that that case, we're also relying on the asymptotic properties of OS. So we're doing that here as well. I

Speaker 5  19:56  
guess in that one we rely on the asymptotic properties of like the sample size, what is what we feel this is about the number of

Speaker 2  20:05  
simulations. No, no. So this is the same question. Yeah, exactly. So the asymptotic properties that we're relying on are, are related to the sample size, the number of simulations is not, is not driving the asymptotic properties. It's just generating variation, so we can identify exactly right. And so the more simulations you use, the better your approximation to that asymptotic distribution would be right. But that's not what's driving me. You can't another way for that would be you can't buy, you know the normality of the true sampling distribution by increasing the number of simulations, it'll just give you a better approximation to the approximation. You

Unknown Speaker  20:50  
have to assume that your sample

Speaker 2  20:53  
itself as a sound point is large enough, exactly. And maybe the way to think about it right is like, the reason why that's true is because you are quite literally just taking drugs from a multivariate normal so you've already assumed the distribution, right? And the assumption you made is the asymptotic distribution. So all that's to say, right? Yeah, to use this, you're sort of in the same scenario that you're in whenever you you cannot assume normally distributed errors, where we have to rely on asymptotic properties. And so whatever your sample size is, that will tell you how confident you are in that if your sample size is like pretty small, I would recommend maybe moving to bootstrapping, although that has its own asymptotic limitations. That seems to be better, often for small samples, but if you're worried about the number of simulations, right, just increase the numbers. That's easy. You can't, can't change whether how well you're doing with respect to asymptotic normality. But you can do better approximating your approximating distribution by just increasing

Speaker 2  22:13  
numbers, you mean this up here simulation. So each simulation is, by definition, one draw, but because you're drawing from a multivariate normal, you're getting a vector for each draw. So each draw is each of these rows is one draw, and the size of the vector that comes out of the multivariate normal is k plus one, because you have an intercept in K predictors. What is drawing?

Unknown Speaker  22:49  
Because, like for model,

Speaker 2  22:57  
so it's drawing from so if you do beta tilde is a drop a simulation. It's taking that from a multivariate normal with a mean equal to beta hat OLS and a variance covariance matrix that I mean for now this is enough X trans of x, that's what it's trying and so it's taking the OLS regression estimates, using that as the mean of the multivariate normal, and it's using sigma, the estimate of sigma squared hat times the unscaled covariance matrix As the variance. There's something slightly more complicated to what sim is doing that I'm going to show you. But that's the basic idea. This is exactly, actually what marginal effects does. The marginal effect simulation package just draws from this distribution Exactly. And so what is this? Well, it's the sampling you know, this is the estimated sampling distribution for the coefficient. If you take the square root of the diagonal that you get the standard errors. But again, the thing you might be thinking is, well, we already know the standard error, so why we care about doing this? And the answer is, because think about what you can do with each of these roads, any function of these coefficient estimates can be calculated, and then it can be calculated once for each of the rows, and then once you've got those 1000 versions of that function of the coefficients, you can summarize that uncertainty, right? So this is not useful if all you want is standard errors for the coefficients, because you get that from OS. This is useful when what you really want is some function of the coefficients, and you don't have an analytic formula for it. So you can simulate the standard errors by calculating the quantity of interest every for every row and then summarizing across those rows. And because this simulation matrix captures all the variation that's present in the sampling distribution. It will also capture the variation that's present in a function of the coefficient. That's the conceptual idea behind the reason

Unknown Speaker  25:19  
you say simulation is not unbiased. It

Unknown Speaker  25:24  
is because we're drawing from the

Speaker 3  25:28  
distribution that is based on the that is theta distribution, based on

Speaker 2  25:35  
the standard deviation hat, the signal hat. No, it's because we're, because we're drawing from the asymptotic distribution, right? So, so if let's, let's say you, you run an OS model and you assume that the errors are normally distributed. If we assume the errors are normally distributed, and we use sigma squared hat, the distribution is t, right? So, so when we assume normally distributed errors, and we have an estimate of sigma squared, we know that the sampling distribution is T, but the asymptotic distribution is normal, because both T, T converges to normal as n goes to infinity. And even when we can't assume normally distributed errors, we've shown that the OLS sampling distribution also converges to the normal distribution as n goes to infinity. So the multivariate normal is the asymptotic distribution for beta, and that's what we're drawing from. So in that sense, we're not unbiased, because we're relying on a consistency property, rather than a finite sample property, we're relying on an asy property, not a finite sample property. Unbiasness is a finite sample property we can claim. We can claim convergence and distribution as n goes to infinity, but we don't make any claims about the unbiasedness of the variance, the monthly variable distribution that we're assuming the data has come

Unknown Speaker  27:10  
from that

Unknown Speaker  27:12  
dates our initial estimate as its

Unknown Speaker  27:20  
name. Yes, exactly.

Speaker 2  27:26  
Okay, so I'm just showing you, so when you get an object out of arm's sin function, so if you've never seen like, if I knew more about programming, I'd probably know the answer, but you'll see it's using at instead of the money sign. That's because these are s4 objects, not s3 objects in R and why that's the choice that they make, and why no one else uses s4 most of the time, but arm does. It's that's why it's at, it's using an s4 object scheme rather than the s3 object scheme. But in any case, right? It's the same basic idea. You can pull out the coefficients with CO, and you can pull out the estimates, the samples from the sampling distribution for sigma, from sigma. Now this part you don't understand yet, so let's go go over what sim is actually doing. So the idea behind sim is that there's uncertainty in two places here, there's uncertainty in sigma, sigma squared, we're only getting an estimate of the error variance, but that error variance also goes into the estimation of the sampling distribution for beta. And so we say we want to propagate the uncertainty in sigma into the uncertainty for beta. If we didn't do that, we'd be underestimating the uncertainty in beta. And so that's actually what marginal effect does, because it only takes draws from the variance covariance matrix, assuming sigma hat from the OLS regression, there's a sense in which it's under estimating the uncertainty in data. And so what arm does is it propagates the uncertainty in sigma squared into the uncertainty in data. So what is it doing? Well, it's pulling out the stuff from OLS it's taking our OLS estimate of sigma. It's taking our coefficient estimates from OS, and it's taking so this CO unscaled is, is x transpose, X inverse. So this is the scaled covariance matrix, because the sigma square times this, this is the unscaled covariance so variance, just getting so what's your sample size, what's the number of parameters and how many simulations Do you want? This is just setting everything up. This is the key of what sim is doing down here. What is it doing for each of the simulations, right? For i and one through number of simulations, it's first taking a draw of sigma from its sampling distribution, and then it's using that draw of sigma from its sampling distribution to take a draw of beta from its sampling distribution, right? And again, the reason why you have to do that is because sigma is in right, the variant is in the sampling distribution for beta. So each time you take a draw of sigma from its sampling distribution, we're going to get a slightly different answer that's going to go into the simulation for beta. And in that sense, we're propagating the uncertainty and sigma into the simulation of beta. But, but the whole you know, this isn't a lot of code, but it might look a little confusing, but the core of it is just here, take draws of sigma from its sampling distribution, and then use that simulation to take draws of beta from its sampling distribution. Do that many, many, many times, and then you'll have lots of different betas that you can use to do whatever you want, but you're not usually going to use

Speaker 2  31:08  
these again. What I mean? We talked about this a lot, but we are assuming the asymptotic distribution is multivariate normal. If you have small samples, you might be worried about this. So if you're worried about it, you might want to turn to boot strapping, which is what we'll talk about now. And bootstrapping is useful in such context because it doesn't make any distribution

Unknown Speaker  31:34  
assumptions. Okay, so

Speaker 2  31:37  
let's take an example. So again, this is, this is m4 this model m4 where we've got a quadratic relationship between age and policy preference, age, age square. And we might want the conditional marginal effect of age, as we've done before. And we might want to be able to plot that with confidence bounds. To get confidence bounds, we need uncertainty in our conditional marginal effects, and we might not remember the analytic solution to that, right? Maybe we don't remember it, maybe we don't want to look it up, whatever. And so instead, we can use simulation to get the same result as we got using the analytic solution. And so I just want to show you, sort of found doing that right? If you use the five, it's actually really easy, because you know what the you know the formula is for the thing you're looking for, right? So if our model is y, equals beta zero plus beta one, age plus beta 2h squared, plus a bunch of other stuff. And we take the first derivative with respect to age, respect to age, we get beta one plus two, beta 2h and this is the quantity we're interested in. We want the conditional marginal effect. That's what it is, and we want uncertainty now for it. So what we're going to do is take that formula and apply it to every single vector of simulated coefficients that we just pulled out of the sampling distribution. So if we did 1000 simulations, we're going to calculate this 1000 different times. So that's what I'm doing up here, right? I'm saying for every row of coefficients in the simulated coefficient matrix. So this is saying for yeah, I've got this coefficient simulated coefficient matrix for each row, apply this function to that row. And so what it's going to do is pull out each of these rows and plug it into this formula. And this formula says, Well, I don't want all the coefficients. I just want the one for age and the one for age square. And in particular, I want the coefficient for age plus two times age times the coefficient for age square. The only, I mean, you could pick a single value of age, like 40 or, well, I guess it zero to 105, or something. But I want to do it for every value of age in the data. So I'm going to, I'm going to calculate this formula for every single value that I'm interested in, which is from zero to one. I'm going to move from zero to one on age by point o5 in this so what this actually ends up doing is calculating 21 conditional marginal effects per age across all 1000 simulations. So this is now a matrix that has 21 rows where each row is one of those conditional marginal effects of interest, and each column is one of the simulations that I get when I take a drop from my sampling distribution. Now what I'm going to want to do is summarize across right those columns for each row, I want to characterize the variation across the 1000 simulations. And if I do that, I get a standard error, and then I can use that standard error to calculate a 95% confidence down. That's all I'm doing. Actually, I'm not even using the standard error to calculate a 95% confidence down. I'm just using the 2.5 and 97.5 quantiles to get that copy.

Unknown Speaker  35:26  
But if you wanted to, you could also use the normal distribution to

Speaker 2  35:31  
get right. So again, what did I do? I took a bunch of draws from the multivariate normal for which is the asymptotic sampling distribution for beta. And I said, Well, what quantity am I interested in? I'm interested in the conditional marginal effect of age, and the formula for that is beta one plus two times beta two times age for every single one of those rows of simulated coefficients. I just plugged in the coefficients to this formula because I'm calculating across values of age, there's actually 21 different values that are calculated here, right one for each of the values of age I'm interested in. So I end up with 21 final estimates of the conditional marginal effect of age across age, and each of those is for 1000 different simulations sample, and then I can just do whatever I want with those. And if I want to get a standard error, I just take the standard deviation of all of those reds, but I want confidence interval, so I just find the quantiles of all of those reds. And then what might I want to do with this last table? I can plot the conditional marginal effect as a function of age with 95% confidence zone. You said, standard deviation, standard deviation of each row. So each row right? So remember again, each of these rows is the conditional marginal effect of age at a particular value of age. So if I take the standard deviation of a given row, it gives me the standard error for that. That's what's going on right here. I'm saying for each of the rows in this calculate the standard deviation here. This is saying, for each of the rows in this find the two point fifth quantile, which is the lower bound of the 95% conceptually, as this, feel where the points of confusion I

Speaker 2  37:47  
and again, you know, in terms of, like, thinking about what's going on, say, you know, there's some function of the coefficients I'm interested in, and I don't know the analytic standard error of that function of coefficients. Maybe it's a conditional marginal effect, whatever. So instead of trying to solve for the standard error, I'm going to take a bunch of draws from the sampling distribution, calculate that quantity for each of those draws, and then just summarize the variation in all of those calculate. Can

Speaker 4  38:21  
you go back to the slide that you break down the simulation? So what we're doing here is we're drawing a sigma randomly from a chi square distribution that takes something as it mean. Sorry,

Unknown Speaker  38:45  
so i

Speaker 2  39:00  
k. So that's that's where we start. And if you just multiply this over and divide by this you get so this is distributed chi square and n minus k minus one. So if we move chi square to the denominator over here, and move this over here and take the square root, we get exactly that okay

Speaker 4  39:28  
and okay. And that requires us to already assume that there is a distribution that sigma follows. Hence the other assumptions, I guess my question is just like, when I think of bootstrapping, I thought of like drawing samples from the original sample, like the observations, re estimating the model, and each re estimation will get a different beta. I'm just wondering, like, if, if this is just, like, a, like, a better way to do the same thing, or, or, is it fundamentally

Unknown Speaker  40:11  
different?

Speaker 2  40:13  
Is it is fundamentally different in the sense that in a very important way, right? And, and I think it makes it worse. The fundamental difference is that simulation is not taking draws from the empirical distribution of the data and re estimating the model. It's assuming the sampling distribution and taking draws from the sampling distribution. So simulation assumes the coefficients are distributed. Multivariate normal bootstrapping makes no distributional assumptions about coefficients, because all it's doing is re sampling from the data over and over and so whatever distribution there is, it will approximate that distribution. So the fundamental difference is whether you're assuming a multivariate normal, which is the asymptotic distribution, or you're making no assumption, the functional form of the distribution at all. And also, I mean, you know, for some Well, that's the big difference. Are you assuming something about the distribution, or are you making no distributional assumptions? Kind of the way to think about

Speaker 4  41:23  
but the flip side is boot scrapping will be very computationally.

Speaker 2  41:29  
It's more computationally compared to this exactly, especially for, you know, one of the methods I'll show you for boot scrapping can take quite a while. This is sampling from the distribution for OLS, for any compilation, just super easy, and so you can get a lot of simulations that where this can become computationally expensive is if you're using the simulations to do complicated calculations, because you have to do the calculation over and over again. But drawing from the sampling distribution doesn't take any

Unknown Speaker  42:00  
example, and who strength is

Speaker 6  42:02  
the next, boot is the next. Oh yeah, sorry, that's good. I confused the two things, yeah,

Unknown Speaker  42:10  
other conceptual issues here.

Speaker 2  42:19  
For these kinds of things, it really helps to just do examples, to do practice like you have a particular problem, you have a quantity you want to calculate. This is easy to do. That's easy. Then you have to think for a second about, well, how do I use this to get the quantity of interest that I want? And then what do I do with all those simulations just going through that a couple times for problems of interest will give you a lot of insight into what's going on. If it still seems a little big, once you've done it a few times, it'll seem very important.

Speaker 2  42:59  
Okay, but more questions. All right, so the next so that's simulation, and that's very versatile. I think I said this in one of the other classes back when I was a grad student, simulation was very popular because Gary King had a clarity, had this program clarified. That was in STATA, and everyone used stata and clarified was great for this stuff, especially for non linear models. People moved away towards the delta method. One of Gary King students, I think, wrote a clarify package for R that I tried out, and he was so slow I like, didn't even believe it. So I don't recommend package unless they fix it. And but there is something out there clarifying R if you want to try that too, simulation, yeah,

Speaker 5  43:49  
it feels like the assumption of asymptotic normality is doing all of the heavy lifting, in your opinion. Like, How valid is that? Is that assumption when it's made? Like, it just seems too easy to be like, in a sample size of like, who has an observation step? Or I can basically, like, simulate this whole pain, and that's going to give me, like, uncertainty, propagation and enough variation. Like, it feels kind of like I'm quite like emphasis.

Speaker 2  44:23  
I think there's two ways to think about it. The first is that I think in most cases, you're going to have a sample size that's good enough for this to be a pretty good approximation. So I wouldn't worry about it too much. That would be my personal opinion. You know, take it with a grain of salt. But I think because we're relying on the on the sort of asymptotic property. And, you know, in most cases, we're going to get pretty good approximations for a decent size sample. I think you're okay in this context with this, it'll be pretty good. That's one way to think about it. So that I would say the second thing is that, you know, if you're worried about it, though, right, like Bootstrap, why not? I think there's not, there's decent amount of evidence that bootstrapping is going to be at least as good, if not better, for finite samples. So you know, there's using bootstrapping as your default would be a totally reasonable thing to do. As a data science person, I think that would be a good choice. So yeah, if this bothers you, that makes sense turn to bootstrap do it. It's just a little bit less easy, but for most purposes, it's fine and it's not that hard to do. The third thing I would say is that just make sure you remember that when you're just doing your standard OLS stuff, you're almost always relying on the same assumption, because unless you are very confident that your errors are normally distributed, you're always relying on the asymptotic normality of the OLS estimator, even When you're just reporting your coefficients from your OLS output. So that assumption is hidden under most of what you're going to do with OLS, no matter what. So if it makes you feel uncomfortable, that's reasonable. But if you're also just reporting sort of like co efficient coefficient standard errors t test from an OLS regression cable, you're pretty much relying on that same assumption for that as well. Now you can bootstrap that stuff too. So if you want to just make bootstrap in your default for everything, that's reasonable, but the standard OLS output is also most of the time relying on that same assumption. This is relying on asymptotic and

Unknown Speaker  46:36  
that your initial will be a co efficient estimate is somewhat accurate.

Speaker 2  46:41  
It's the same because when you calculate like a, you know 95% confidence interval for just a coefficient, you get a LM you're assuming, unless you assume the errors are normally distributed, you're relying on asymptotic formality for those confidence I it. But I think you know your feeling is great, right? So, like, you know one, one path is to embrace that and say, I really should be boot strapping most of the time. And there's many people to do that. Okay, so, so let's move on to that simulation. So the next things we interpret are the class of things we're talking about, of which bootstrapping is one part, are called resampling methods. And so the idea, the concept, the conceptual idea behind resampling methods is that you don't want to make any assumption about sampling distribution. And so instead, you're going to use the data that you have to approximate the sampling distribution without making any functional form assumption. And so when we say re sampling, what we actually mean is that we're re sampling from the data that we have. We're not going out into the world and taking new samples. We're taking our sample we have, and we're sampling from our sample. That's why it's resampling. And so what we're going to do is take many re samples from our own data, and we're going to estimate the same model on each of those re samples, and then we're going to use the variation across those re samples to summarize the uncertainty in our estimates. Because each of these re samples will be slightly different, we'll get slightly different estimates, and the degree to which they're different is an approximation of the uncertainty in the parameters we're estimating. We're going to talk about two different things, Jack Knife standard errors, as we'll see sometimes, and Bootstrap standard errors. And I should also say, right, this is like a pretty complicated literature, and it stretches the bounds of my knowledge for sure. And a lot of what I'm presenting here I'm drawing from relatively new econometrics textbook from Hanson, which is actually really fantastic, and where I delve into it. And so in chapter 10, he has, like, an extended discussion of boot strapping that's really, really good. So recommended to go deeper. Let's start just by thinking about advantages and disadvantages of re sampling methods, and then we'll get into the actual methods themselves. The main advantage is that, yeah, I think that's perfect, right? The main advantage most of the time, the first and primary advantage, is that you don't need to know how to calculate the standard error or the variance covariance matrix range. You don't need an analytic solution, you know. So for any quantity that you might be interested in, you can calculate uncertainty using boot strapping, because you don't need to know how to calculate the standard error. And that's that's very similar to simulation, in that sense, the other main motivation for re sampling methods is that you don't want to make assumptions about the sampling distribution. You don't you don't want to rely on asymptotic normality. And so re sampling is great for that, because you don't have to make any assumption about the distribution, that it's normal, that you have a big enough sample, etc. It makes no distributional disadvantages. Can be computationally more expensive, but it depends a lot on the particular approach you use. And for many, many cases, it's not that bad. And as I'll talk about towards the end, right? Like, you know, one approach to doing this is to use smaller approximate, you know, use smaller numbers of boot straps early on. And then once you've settled on the model you want to estimate, just let it run overnight, go to sleep and wake up the next morning, and we'll be done. So this is shouldn't typically be a constraint for you. And then, you know, practically, the statistical theory is difficult, and oftentimes there aren't any, ne necessarily, like, really firm recommendations that you can give. And so again, you know, part of that is just limits of my knowledge, but part of it is that even some of the proofs you know, the proofs that people or the claims that people make about certain methods, are actually not real groups. They're just heuristic groups, so it's a bit more complicated than the other stuff we've talked about. And then the other again, is this a disadvantage, or is it just like a point of confusion? This is such a huge literature. There are so many different things that people talk about when they talk about boot strapping. We're not we're going to talk about a bunch, and we're not even going to cover everything, so it's a very large literature, and it feels very confusing when you delve into it, trying to find your way through it. But again, like, if you're looking for like a go to default approach, this is pretty good. Okay, so even if you don't use jack knife, you might see it, but it's also useful to go over, just because it develops that of an approach that you'll see again later on. And so let's start to define the jackknife. Let's start by defining what we would call a leave one out estimator. So a leave one out estimator is an estimation procedure that quite literally takes one of the data points and excludes them. So it keeps all the data points but one, and it estimates the coefficients using whatever model you're in. That's why it's called lead one out. So obviously, right? If you have a leave one you're using this, you have a choice of n different data points to leave out. You're only leaving one out. You can leave out any of the n data points you have. So the first thing you should think is, well, why? How do I choose? And the answer is, don't choose. Do it n times. So estimate your model using all n possibilities. The very first time you estimate your model, leave out observation one, the second time you leave out observation two, the third time three, all the way up to n. So you'll now have estimated the model n times, and each time, you're going to get a slightly different answer, because you're going to leave out a different data point each and so the jack Knight variance estimator does this. It essentially calculates the uncertainty in the parameters you're interested in by looking at the variance across these n lead one out estimators. So it's going to look at the variation in your estimates across all these n estimations, and use that as an estimate of the uncertainty. And so this is the jack Knight estimator for the variance covariance matrix of whatever parameter you're interested in. And so it might be data by data hat will be data hat in this class. So this is just your sample size minus one divided by your sample size, times the sum from one to N of this quantity. And what's this quantity? Each of these is a leave one out estimator. But this is the estimate. If this was beta hat, it would be the beta hat you get from OLS when you leave out observation I so again, you can do that n times. Leave out observation one, then two, then three, then four. What you're subtracting from it is the mean across all those leave one out estimators. You're squaring each of those differences and summing them up. This is just a variance. It's the variance across the leave went out estimator, essentially,

Speaker 7  54:44  
I was just going to ask if you could re say so, would you mind just saying that again? Yeah,

Speaker 2  54:56  
so, so think first about what, what a lead went out estimator is, right. Like, concretely, you might have an OLS model, and you've got like, five predictors, and you want to estimate it using OLS a leave one out estimator is just the OLS estimates where you leave out one observation from your data. Maybe it's the first one second, because you're only leaving out one observation, you could estimate that same model n times, and each of those estimations is going to leave out one of the observations, right? It's going to be a different one each time, because it's a different one each time. You're going to get variation in your estimates across all those different applications. And all this is doing is calculating essentially the variance across all those n estimations. It's a scale variance, but it's basically just a variance, right? This is an estimate minus the mean squared sum divided by n. That's just the variance. So the jack Nick is really just calculating n lead one out estimators, seeing how much they vary, and then using that as the estimate

Speaker 8  56:08  
of the variance all rows of observations, or like, one entry,

Speaker 2  56:13  
yeah, yeah, yeah, the whole row. So if you have a data frame that is n rows and k plus one columns, you're leading up one road, one entire observation. Like, it's like you didn't collect that observation.

Speaker 2  56:39  
So usefulness, excuse me, the usefulness of this is very flexible, right? You can do this for, again, you don't need to know anything about the analytic solution to a standard error. For a standard error to do this, you can just do it and you'll get a variance estimate. So it's super flexible in that sense. Another advantage of it is that it overlaps with other approaches. I'll get to the second, but they're very flexible. You don't need the analytic solution. You're not making any distributional assumptions. And asymptotically, it's approximating other estimators that you might be interested in. And so in that sense, it's, it's, you know, doing something very similar to what you would do otherwise, without making any distributional assumptions. So asymptotically, it approximates the delta method. It also approximates you don't know what this is yet, but the HC three heteroscedasticity consistent estimator, which is like the go to robust estimator, which we're going to talk about in two weeks, it approximates that as well, but again, without making any kind of distributional assumptions at all. The down side is that it requires n separate estimations. But again, that's typically not a perfectly big deal. So super flexible method, relatively easy to calculate. Now I give you some code here to show you how to do it. I couldn't find any like simple function and hard to do this, but because it so closely approximates HC three, and I'm going to show you ways to get HC three standard errors, like you probably almost just use that as a jack. But the basic idea, right, if you want to sort of get what's going on conceptually, is that I've got a model, right, an OS model, m5 let's call it, and I'm just going to re estimate that model over and over again, once for each row of the data, and each time I'm just going to take out an i th row of The data. So all this is doing is up to me updating in the sense of estimating n5 again and again and again, each time, pulling out one row and leaving it out of the estimation. And then down here, right? I'm just plugging in all that into the formula for the jack. So that's easy enough. Down here I'm just showing you, right? The jack knife, standard errors are similar, but not the same as OLS. They're different. Makes sense, right? OLS is making assumptions. Jack Knight is not making distributional assumptions. And then I'm just showing you here again. We're going to get to get to this in a couple weeks, but if I calculate H, c3 standard errors, they're identical to the check next standard errors, you'll know more about what that means. So if you're interested in doing this, you can use that code to do it. I'm not going to go through this, but if you want the variance, CO, variant, covariance matrix, instead of just standard errors, I'm going to show you how to do it using matrix multiple position. Okay, so, Jeff, net flexible. You'll see it sometimes approximates, H, c3, standard errors. But the more popular approach is this. Jeff, so that's where we're going to send resurrection. Ok? So the idea of boot strapping is very similar to Jack Knight, but different in an important way. Instead of calculating the model again and again, leaving out one observation, you're going to estimate the model again and again and again, but taking random samples from your data with replacement over and over again. So think about what that means, right? You've got a data frame. It's n rows, k plus one columns on each bootstrap replication you're going to randomly sample from the rows with replacement. So you're going to reach in, pull out a row and store it, and then put it back. And you're going to reach it again, pull out a row, store it, put it back. What's that going to do? And then you're going to take n of those samples. You're going to sample n times with replacement from the data frame. Now, why is that key, that with replacement? Because, given that you're putting the row back every time you sample from it, sometimes you're going to get the same row you already sampled. Sometimes you're not going to get any of you're not going to get one row at all, right? You're going to take n samples, and one row is going to be excluded altogether. So what sampling with the replacement does is that it gives you a new data frame that's different from the original one, and it's approximating what would happen if you went into the real world and took a new sample, you get a slightly different set of data and so that so that's what's happening, right? And every time you get a new data frame of n, observations can be slightly different from the original one, and you're going to estimate the model again, and it's going to use slightly different coefficients. And if you do that many, many times, you're going to get a whole distribution of coefficients, and you can use that distribution to summarize uncertainty without making any distributional assumptions at all? Yeah,

Speaker 8  1:02:05  
may I missing something doesn't that rely on the assumption that your sample distribution reflects the population distribution? Yes,

Speaker 2  1:02:13  
it absolutely does so if you're if you're trying to make inferences to a population and your sample does not reflect the population, then you know this doesn't work, but you're also just screwed generally, right? So, like, nothing you're doing that we've talked about is going to work for you if that's true, you know, because all of these things are at their core, attempting to make inferences to a population by assuming random sampling from population. I go back to like the first chapter of Wooldridge. One of those key assumptions is random sample from population. And so if you don't have that, you know you're in trouble, which is often the case, because

Unknown Speaker  1:02:56  
you would use it if you have a small sample size, right?

Speaker 2  1:03:00  
Yeah, yeah. Oh, okay, sorry, that's a that's a more nuanced question. And so that that's true, right? So if you have a small sample size, your your empirical sample rate is, is randomly different even with a probability sample from the population. And so bootstrapping is, is asymptotically valid for the things that you're interested in, but it's not, it's not finite sample perfect for the things we're going to do. And so, so this is one of the kind of complicated things about choosing among these different methods is, like, on the one hand, right? With bootstrapping, you're not making any distributional assumptions. You might say, like, I don't want to assume the asymptotic normal distribution, because I don't feel confident in it. And so you turn to the bootstrapping, which makes no distributional assumption, but it's only asymptotically valid. You're it's hard to escape right the the asymptotic aspect of all this, you just really can't escape it. What you should be thinking about is, can I do better than I would by not making distributional assumptions? And it does seem like that's the case. So again, there's no proof that's going to tell you that in your given situation, with a small sample, Boots trapping is going to do better than assuming asymptotic normality. But it does seem like the literature suggests that that's that you will be better exactly because. But again, right, your question is a good one, because, you know, if you're thinking about a bootstrap confidence interval, and you want to note, you're perfectly, you know, replicating the true confidence interval. That's true asymptotically, but not in a fine example,

Speaker 5  1:04:48  
when you see doing federal name feathers reflect the population

Speaker 2  1:04:52  
estimate. So if we think about it in terms of like a confidence interval, you could say, like a confidence interval should capture the true parameter 95% of the time. And so one way to think about accuracy would be, is your, you know, 95% confidence interval truly capturing the true parameter 95% of the time? Or is it off, right? Like, are you only getting 80% of the time, even though you're saying it's 95% of the time? Or is it, you know, if the true confidence interval is asymmetric, right? Are you? Are you treating it as symmetric? And you're missing on one end more than on the other, those would be ways that you're off, right? So to think of the way to think about is that the sampling distribution that's approximated by the bootstrap sampling distribution, the bootstrap sampling distribution, converges to the true sampling distribution, asymptotic, and so all the characteristics of that will also be asymptotically valid, but not for a finite example, you don't know how good the approximation is, just like you don't for using simulation or any other kind of reliance on asymptotic. Everyone. Frustrating. Does that at least make sense? Even if it's just like, dissatisfying. This is the world we live in, right? It was like, You're not going to get any firm answers unless you can make really strong assumptions. And that's going to be true of all these methods. And then so again, the question is, just, do we? Is there reason to think we can do a bit better, on average, using this method than that method? Again, I think the answer for boot is specifically Yes, but it's not a satisfying answer.

Unknown Speaker  1:06:42  
I Okay,

Speaker 2  1:06:46  
so again, right? If we take all these repeated drugs with replacement from our data, and we calculate the quantity of interest every time for each of those re samples, the distribution of those quantities gives us an estimated sampling distribution like we were just talking about, and that estimated sampling distribution converges to the true one as n goes to infinity, without making any assumptions about the true form of that distribution, right? We haven't made an assumption about the form of the distribution, but it will approximate the true distribution as it goes as end of this. Just again, you know, if you have a hard time thinking through like, I mean, the first thing to think when you hear boot strapping is, like that. Can't possibly work. Like that makes no sense. That's what I think. Okay, and so it's surprising that this works. It was like a huge innovation in the 1970s when there's been a huge literature after it, because it's surprising that this works. But just to see kind of what's going on here, I'm just going to I'm going to pretend my population is the numbers one through 10, and then I'm going to take samples from that population of the integers one through 10. And what I'm doing is sampling from that population a bunch of times, but I'm going to each of those samples is the same size as the thing I want to sample from. So if, like, I really should have written the sample. So imagine that I have a data set. Sorry, if this was my sample from the population that would be a bit better do this one through 10, right? Is my sample, my true sample, the sample that I got from the population. I can sample from that a bunch of different times, and each time I take a sample the same size as my original sample, but I make sure that I do a good replacement. And so what's going to happen? Well, each of the times I do a bootstrap replication, I'm going to get a slightly different set of observations from my sample, right? So in this case, I got six twice, I got eight twice, I got four twice. I never got one. So one was completely excluded. 10 appeared twice. Other numbers appear twice. In this one, I get a slightly different right? One is in this one, and I get eight three times, and I never get six or whatever. So each time you do it, you get a sample that includes some subset of the original observations where some are repeated and some are missing altogether. And so because your data is different every time, each time you estimate the model on that new set of data, you're going to get slightly different answers, just like with the jack knife, but using random sampling with replacement instead of doing it once for each

Unknown Speaker  1:09:38  
observation. So you

Speaker 2  1:09:42  
want to see it, you know, for an actual case here. So if i So, if I pull out right here, I've got the M for model, and if I pull out the object, the thing in that object called model, it's the model matrix. So it's the data, the data, that I use to estimate M for. So this call to sample is saying list the integers from one to the number of rows in the model matrix. So it's one to n. If there's 1000 observations, it's one to 1000 and it's going to sample from those integers. So if I have 1000 observations, it's going to take samples from the integers from one to 1000 how many samples is it going to take from those integers? It's going to take the number of rows in my in my model matrix and my data. I want a sample size that's equal to my original sample. It's going to sample n times from the integers one to n with replacement. What's that going to give me? It's going to give me a list of integers that correspond with rows of my data, right? If I get one, that means the first observation. If I get 10, it means the 10th observation. On some of these, these sampling attempts, right? I'm going to get number one is going to appear 10 times, and number two will appear zero times. For each of those new samples. I'm then going to estimate the exact same model that I originally estimated, and I'm just going to store those coefficients, and now I'll have exactly what I had with the simulation matrix. I'll have 1000 rows of coefficient estimates that vary depending on how much variation there is in the data, then I can just use all those simulated coefficients to get all the quantities that I'm interested in. I'll have 1000 estimates of the intercept, 1000 estimates of beta one, 1000 estimates of beta two, and I can just take the standard deviations of those to get standard errors. And those are bootstrap standard errors. And that's what if you do this, and then you calculate the standard deviation across the rows of one of these coefficients, you get the bootstrap standard error for that. That's all it is. Calculate confidence intervals for it, and there's lots of different ways to do that. And that, OK,

Speaker 2  1:12:13  
but let me say this last thing, because there's confidence interval things like a whole bag of worms and worms. So like I said, if you take the standard deviation of all of these estimates, you get the bootstrap standard error. It consists that is a consistent estimator of the standard error for the types of models that we've been working with, most of the models. That is not always the case. If you have non linear models or non linear functions of the estimated coefficients, the simple just taking the standard deviation of your bootstrap estimates will will sometimes be an inconsistent estimator of the true standard error. And so the recommendation that I mean, Hansen goes through and talks about how bad this can get in situations and non linear functions, and it's pretty bad. And so he recommends as a default, a trimmed estimator. And by trim, it just means cutting off 1% of the at the extremes, so like point five on each extreme of the estimates and then just doing what you would normally do that's consistent under much more general conditions. The problem is that this is not you don't find this like in software usually. I don't think the default for bootstrap standard errors is to give you the untrimmed standard error. And again, that'll work fine for most linear models, and for many linear functions of coefficients, but if you're dealing with, like, non linear functions of coefficients, like, if you're taking the ratio of two coefficients, like beta one to beta two, or something, this will do really badly, and you want To use the trimmed but again, I don't see like easy ways to do that in like standard software. But the up shot is that this is really easy to do by hand. So you know, if you just use the simple bootstrap code and then just trim off 1% of the extremes, instead of just using, you know, the raw output, like, that's easy to do, and so you can do it yourself. In most cases, it's possible that that there's something in, like the boot package. But this is, this is an issue. I mean, like Hanson discuss discusses. Almost everyone just uses the default, and many times will be very wrong. So it's like a problem, pretty wide square problem. So keep that in mind. But the other shot of this is that we're going to move on to confidence intervals, and in most cases, reporting confidence intervals is more important, I think, than reporting a standard error. And many of these methods for confidence intervals aren't going to suffer this problem. They don't rely on trimming and so like, if you use the default confidence interval methods that I'll recommend, you don't even have to worry about this at all, but know that this exists and that people are going to make mistakes doing this. Okay, I'm over. We're out of time, spring break, I guess, after the mid term. So we're a little bit behind. We're about half a class behind. No, we're like, the whole class behind. That's okay. I will over spring break, I'm gonna think about the sold us and see where I can make sure we stay on track so that we don't skip over. You. Have any questions about mid term? Don't see me. Won't be around. Otherwise, I was seeing Thursday. I Yes, yes, oh, yeah, so bubble sheets on Thursday, and then I'll have you just on typing for the other answer. But the bubble sheets are very well. The other option would be to just have to do it directly in Will everyone be able to have a laptop for the exam? Okay? I think I'm just going to do for the exam at all here. So you'll do the multiple trade proof grade. Yeah, don't worry about the double sheet for so

Speaker 7  1:17:11  
Very tired and rely and I Can't remember the

Unknown Speaker  1:17:32  
very tired, I don't

Speaker 4  1:17:53  
no, oh, my god, don't wish for that, because It means that we rotate to Another

Unknown Speaker  1:18:02  
project. I don't

Speaker 4  1:18:02  
especially

Unknown Speaker  1:18:22  
for I'm like, yeah, oh yeah, it's the place that feels where I really feel like I'm on my

Speaker 1  1:18:58  
bed. Yeah, do you

Unknown Speaker  1:19:22  
how Do now possible opportunity I donation. Wait. You going there?

Speaker 7  1:19:53  
There's also the Big Hall room. Oh, yeah,

Unknown Speaker  1:20:09  
I you have to get A trap and then suddenly

Unknown Speaker  1:20:31  
learn The Echo. So. Thank You.

Unknown Speaker  1:23:52  
Song I I know You're not but I

Speaker 9  1:25:04  
here. Thank you. To the

Speaker 2  1:25:36  
issue is like, I can talk about But it's like, sometimes It feels like what

Speaker 10  1:25:46  
I'm green juice, gamble, they have cheese. That's good. This thing.

Unknown Speaker  1:26:53  
Yeah, Dominic, you

Unknown Speaker  1:27:06  
don't have

Unknown Speaker  1:27:54  
up, Marty, Marty, thank you. Okay, 68

Unknown Speaker  1:28:16  
the Rich be

Unknown Speaker  1:28:50  
Okay,

Transcribed by https://otter.ai
