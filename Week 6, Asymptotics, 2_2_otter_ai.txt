Speaker 1  0:00  
Conditions under which OLS is consistent, but not necessarily unbiased. And so instead, right then there's it's a weaker assumption to get consistency than it is to get unbiasness. Un biasness. Un biasness requires that the error term be uncorrelated with any function of the x variables, which is what this condition means, zero assumption set for consistency. We just need a zero correlation between the error term and each this assumption says which we could break down. This is encompasses these two sub assumptions, expected value zero covariance between each of the independent variables in the error term equals zero for all independent variables, because it's a weaker assumption, right? That means that this can be true and this is not true, and so it can be the case that there are functions of x that are correlated with U, even though each individual X is uncorrelated with u. And so there are cases where the OLS estimates are bias, but still consistent. And so I gave you example

Speaker 1  1:28  
was a situation where the true model is so the case for the true model is y equals i.

Speaker 1  1:45  
For zero plus beta one, beta 1x plus beta 2x squared, where x, importantly, x is drawn from a normal distribution, a standard normal distribution. So this sort of with this model, right? What is the relationship between x and y in reality? Right? What's the true model? But you're at it, and it's symmetric. And so this is a situation where, well, this is also true of x and x squared. So the point I'm trying to make right with this example is that this is a case right, where, if you estimate instead of the true model, right, you estimate the model beta zero plus beta one x1 sorry, theta one x1 and you exclude the quadratic term that we know is actually in there, right? Well, what happens? It goes into the error term x1, plus u, where u is equal to beta 2x squared plus sort of we could call this u star, right beta 2x squared plus the original u from the true model. But right. So the first thing you might think right as well, this is a classic situation of Omitted Variable bias, and so we're going to violate that conditional mean zero assumption. But it turns out, right, that with this model, x1 and x and x, x1 and x1 squared are not correlated. So the relationship between x and x squared, it looks like this. What's a correlation? A correlation tells you about a linear relationship between two variables, basically, can you draw a line right, either this way or that way that captures some sort of covariance between the two variables? But you can't do that with x and x squared when they're both drawn from a normal, standard, normal there's no linear correlation. So in this case, it's actually the case that x1 is not correlated with this error term. Even though a function of x is in the error term, a function of x is in that error term, the model is Miss specified in that sense. But because there's no correlation between x1 and the function of x1 we have this exact situation that I'm talking about where this is not true, but this is and so this is a situation where we would expect in which it's true that OLS is going to be consistent, because it meets this assumption. There's no correlation between x and the error term, but it's going to be biased, because there is a correlation between a function of x and the error term, namely x squared. And so you can show this right? I'm just doing a simulation where I'm generating y's from that model, and then I'm, instead of estimating the true model, right? I generate my y's from the model. But then I estimate the Miss specified version of the model, and I simulate that many, many times, and I try to see what the expected value of beta one is going to be. And I do that for both, you know, the Miss specified model and for the correctly specified model. If you use the correctly specified model right, we hit the true value of beta one in expectation on the notice 1.00 if instead we estimate the Miss specified model, and we use a small sample size of only 10, we can see that the coefficient is extremely bias in this particular case, not even close to the true value. But what happens if we increase the sample size? And the other thing to note right is that when we correctly specify the model, it doesn't even matter that we only have 10 observations. With only 10, we still get expected value equal to the true value. And when we properly, when we specify the whole thing, when we have the Miss specified model, it's biased. If we increase our sample size to 1000 nothing really changes. Nothing changes for the unbiased model. We're still hitting it in expectation the true value. But we see a drastic change for the Miss specified model. And in particular, even with a sample size of only only 1000 we're getting an expected value of beta that's very close to the true value. And if I increase this to 10,000 this would get even closer to 100,000 we get even closer to one so what's happening, right? What's happening is that as the sample size gets closer to infinity are the probability that our estimate for beta one diverging from the true value goes to zero, because that's the definition of consistency. So even with this Miss specified model, where a function of x is in the error term, as n goes to infinity, beta one converges on the true value of beta one, beta one hat converges on the true value of theta one. And so this is just a simple demonstration of exactly this point there, that there are situations where right, we're meeting the one assumption, but not the other. We're meeting the less restrictive assumption and getting a consistent estimator that is none the less bias. Okay, so let's take questions about this. I know we were running up against time last time, so people see what's going on

Unknown Speaker  7:58  
here. Okay?

Speaker 2  8:00  
Because the estimated value is converging towards the true value, does it also mean that as n goes to infinity, the like the biasness of the estimator also decrees so?

Speaker 1  8:18  
So I'll try to be careful here. So, like, if you want to be like, you know, pedantic or something, right? So, so technically, what you said is not right, right? Because bias is a finite sample property, so it doesn't bias is just a characteristic of the estimator for any sample, in a sense, right? So,

Speaker 2  8:45  
so advice and consistency is the way we describe like the procedure, which is the like, the estimator, and regardless of the sample size,

Speaker 1  8:54  
yeah, exactly right. So that's what I want to say. I feel, I feel like I'm not going to say this in a way that's going to be useful. So sorry. I think the way to think about it right is that when we're talking about a biased estimator, what we mean is that the expected value diverges from the true value. When we're talking about consistency, we mean that as sample size goes to infinity, the difference between your estimate and the true value can goes to zero, probabilistically, goes to zero. And so, yeah, I don't know. Maybe that just didn't help at all. I want to be careful about it, because I don't want to confuse the two concepts that we're talking about, but you could, but at any given sample size, right? For a consistent estimator, you could think about how big the gap is likely to be on average. And so, you know, this first example I gave, this is a consistent, you know, this is shown trying to show consistency in the sense that, you know, the the the amount of deviation is going down. Now, if this was, you know, a bias estimator that is consistent, you know, there's a sense in which right the the expected value is, because is convert, you know, the expected value is converging on the true value. So there's a sense in which the bias is going to zero Between unbiasedness and consistency for understanding this distinction. Sorry,

Speaker 3  10:42  
yeah. Itself

Speaker 4  10:46  
is draw, it's forcing consistency even when it might not actually be consistent in the model. Or is it that the larger the sample you get, it always becomes consistent, and OLS is showing us that. So

Speaker 1  11:06  
it does, I think so. So see this up, so, so the it's not, it's not something mechanical about the OLS procedure that's making it consistent. It's a property of that procedure that, if this assumption is met, as n goes to infinity, you converge probabilistically on the true value. There's a distinction there, right? Like this idea of like a mechanical effect would be that there's nothing interesting going on. It's just it has to be the case that as n goes to infinity, that this produces that OLS a procedure produces that result. But that's not true, because if this is, if this condition is not met, then that won't happen. So so if this, if this assumption fails, as n goes to infinity, the OLS estimator will converge on some other value than the true value. It'll still converge on something, that it'll converge on something that's not the

Unknown Speaker  12:01  
true value. So would olas tell us if it's converging on something

Speaker 1  12:06  
that's not the true value? No, no, no, no. So these are assumptions that you have to make. So you have to think, right, just like in the case of there's no, there's no big difference here between the unbiasness case of endogeneity and this consistency case. In both cases, you have to think, is there reason to believe that any one or more of my independent variables are correlated with the error term? When are independent variables correlated with the error term, they're correlated with the error term. When there is an omitted variable that is correlated with both y and x, right or that has a marginal effect on y is correlated with x and is excluded from the model that's Omitted Variable. Omitted variable problem, it's endogeneity, and that the only difference between the case that we had previously talked about in this one is that it's a slightly weaker version of endogenous, of exogenated, that we have to that we have to assume. But the same basic idea, right? And this is a theoretical claim that has to be made, and if that assumption is a good one, you get the result that you want, but in any given sample, right? You can never know what's going on, you only ever have the one sample, so you don't know for sure more questions. So so it's not converging, and we don't see consistency,

Unknown Speaker  13:29  
or we don't assume

Speaker 4  13:36  
consistency. Is it that we're not using all of us to predict the model, or is that we just force the assumption and then say it's probably not actually converging the

Speaker 1  13:51  
way? That's a really good question. So So one possibility right is that, and this will be the case many times in many contexts, is that people might be skeptical of this, that you're meeting these assumptions, and they'll say it. And, you know, actually, in your homework, I think, for this week, right, there was like a seminar participant says that you've excluded this variable. That's exactly the kind of situation that you face all the time in social sciences. And, you know, one response to that is to say, theoretically, that's just, you know, that's wrong, right? There's no reason to think what you just said is true, and you can try to defend it theoretically. And that's what people often have to do, because it's very hard to deal with the problem of admitted variable bias when you don't have the omitted variable in your data set. And so what do you do when you don't have the omitted variable, you can only depend it theoretically, or you can have a different research design, like an experiment or go find some natural, exogenous variation. But let's say that you do have so yeah, and so it's sort of like the more general to get to the question of, like an alternative estimate, right? So one possibility is that you really don't, can't do anything, and you have to defend it theoretically, but there's nothing empirical you can do, because the research design you have is the one you have, you don't have the variable to control for. You know, you're just stuck. But there are alternative estimators to use that solve this problem. They're called instrumental variable estimators, and we're going to talk about those in several weeks. Actually, we're going to talk about, potentially, we're going to get to that today, in a second, just very briefly, but an instrumental variable estimator, an I V estimator, is one where sort of the logic of it is that it a rates. You use it as like a proxy for the variable that you think is endogenous. So if x1 is correlated with the error term, the covariance of x1 and u does not equal zero. If you can find another variable in your data set, right, you're not controlling for the omitted variable. But if you can find another variable z that is correlated with x1 but only operates on x1 on the dependent variable through x1 then that's called an instrumental variable, and you can use that. You can use an instrumental variable estimator to get a consistent estimate of the coefficient of interest. So there so the short answer is yes, there are alternative estimators, but as you probably come across at some point, instrumental variable estimators are extremely difficult to apply in practice, because you need a very particular kind of instrument, and the assumptions that are required for that instrument are usually very difficult to meet. So most of the time in bio sciences, people are either defending this assumption theoretically, or they're trying to come up with another research design that can supplement their evidence in order to make a stronger claim. They might have an observational data set, but people are skeptical of this, and then they have a natural experiment or a regression discontinuity design that tries to reinforce their claim using a design that doesn't fail. This assumption. More question, yeah,

Unknown Speaker  17:30  
how does the concept of confidence interval relevant to the

Speaker 1  17:36  
consistency, since there both about the population afterwards? Yeah, we're gonna get to in one

Speaker 1  17:49  
second. So this whole setup here raises kind of a subtle point. So So again, right if, if that's the model, model, as I've set it up, right, with the true model being x plus x squared and x being distributed north, standard, normal, and we see, we see bias with small sample size, well, we see bias in general, and then we see convergence to the true value as sample sizes get larger. I've set this up in a way to make that true, but it's pretty rare that you would get a situation where this is the case right, where OLS is going to be biased but consistent, because it requires day light between these two assumptions. You need to have this be false, but this be true, and that's actually not that going to be that common. And to see why that's the case, you can actually, if we think about why it is that there's no correlation between x and x squared, which is the key, right? Because x squared is in the error term, it's, it's because you can't draw linear right? You can't draw any sort of linear relationship here, but in any given finite sample, right? Let's imagine we're just taking draws of x. So we're only going to get like, let's say we only get 10 or something observations, probably

Speaker 3  19:22  
I say something

Speaker 1  19:29  
like that. So I've drawn, like you know, 10 or so observations of x in my 10 sample, my unit 10, my 10 unit sample, there is a correlation between x and x squared, I can draw a linear relationship. So in reality, right? The two variables are uncorrelated, but if I take a finite sample in that sample, they are correlated, and they always will be, because I'm never going to get a perfect balance right and a correlation of zero, or at least it's very, very unlikely. And so in finite samples, there is a correlation between these two, these two variables, but not in expectation. And so what that means is that the reason why this is actually the case is due to the fact that we're treating x as fixed. We're conditioning on x as though it's a fixed rather than a random variable. If instead, we treated x as random and we took expectations over draws of x in expectation, this correlation is actually zero, and so there would be no there would be no bias. And so if we treat x as random, right, these assumptions coincide, it's only because we're treating x as fixed that they diverge. And so then the question becomes like, Well, isn't that like a free lunch? Why not treat x as random? And I think, right, like, this is kind of the rabbit hole I started going down. Like, I think we pay for it in the increased variance associated with the estimator for beta, because once you treat x as random, right, the sources of sampling variability that come into the estimation of beta is no longer just the error term, it's both the error term and x, and that increases the variance. And so I think there's essentially no free lunch here, right? But sort of the subtle point is that this divergence that Woolridge is talking about between consistency and unbiasedness comes about because of the assumption of a fixed x and sort of the quirks of taking a finite sample. But it's also exactly why it's consistent, even if it's even if it's bias, because note that as we take in, you know, as n goes to infinity, this line converges to zero correlation, and so we end up with an, you know, an estimator that converges to the truth value. And if we instead treated x as random, we'd be averaging over all the possible samples that we could get, and all the possible samples we can get are going to be, you know, all over the place, but they're going to average out to zero. And so treating x as random would make the estimator unbiased. Okay, so what are the practical implications of all of this? Which is what I want you to take away, right? This is sort of the big one. Don't worry about this subtle point. There's a small number of situations where OLS is going to be biased but consistent and it's just not going to matter enough for you. For practical purposes, if you knew you were in one of these situations, you would want to specify the model almost surely. But you would want to specify the model that you know is true, right? If you knew that this is true, right? X and X squared is the true model, you can include x squared in the model. So you should just do that. And another kind of situation where this would come up is if you had x1 and x2 both standard, normal and you had an interaction between them, if you excluded the interaction, estimates of beta one and beta two would be biased but consistent. But then you should just include the interaction in the model. So So for practical purposes, this just isn't going to matter that much for you. And right? If there is a correlation between the picker and the return that it's not consistent anymore either. So Right? These are a small number of situations. This is the larger number of situations. And so for practical purposes, you just don't have to worry much about this subtle point. You want to focus on thinking about whether there's any correlations between x and the error term, and just worry about sort of the estimator being bias. Just

Speaker 5  23:53  
to clarify, when you say about the correlation, you mean that that isn't a theoretical correlation between the predictor and the editor of not sample

Speaker 1  24:02  
Exactly, yeah, you want. What you want to worry about is whether there is a theoretical, theoretically in the population, right? There is a correlation between any of your independent variables in the error term and potentially function. So the key up shot raise is just that the difference between OLS being consistent and, you know, and unbiased, is just very unlikely to come up in practice, even though it's true that that's totally possible, and I've given you an example where it is true. Okay. Okay, so to get to this question of inference, again, our properties of the expected value of beta are important. But if we want to make inferences, if we want to say things like beta is very likely to be between 10 and 20, we need to know something about the sampling distribution. You can't just know beta or the expected value of beta, even just expected value in variance. We need to know what the sampling distribution looks like. And so it turns out, and this is extremely useful for us. In fact, you know, this is not very useful practically. This is extremely useful practically. And so it turns out that if we drop the sixth assumption that we talked about last week, the sixth OLS assumption, which is that the error term is normally distributed, the normal assumption, if we drop that it we can still say that beta hat is asymptotically normally distributed. So this is super important, right? Last week, we assumed that the error term was normally distributed, and that allowed us to say something about a finite sample, sampling distribution for beta namely, that if we're estimating the error variance, it's distributed T on n minus k minus one in any finite sample. The only reason we knew it was distributed T is because we assumed that the U were drawn from a normal distribution independently. So that's a very, very stringent assumption and will often not be true. Just to give you a very straight forward example that anyone doing well, pretty, pretty much everyone in this room, and especially if you're a political behavior person, often our dependent variables are like five point scales. It's impossible for that to be a case where there's a normal business like literally impossible. So very, very often it's the case that this assumption of a normal distribution for the errors is going to be wrong. And so it's extremely useful for us practically that it turns out that beta is asymptotically normally distributed even when this assumption is false. So even if the error terms are not normal, beta is asymptotically normally distributed. Now again, right? We're dealing with asymptotic properties. So we don't know finite sample properties. We don't know what the exact distribution sampling distribution is for beta hat. For any given sample, it's literally unknown. We don't know what it is. But if we have large samples, where I put large in quotes, because we don't necessarily know how large is enough, what we the only thing we do know, but again, right? This is useful for our purposes, is that as the sample goes to infinity the district the sampling distribution for beta hat becomes multi variate normal, with a expected value equal to the true beta and a variance covariance matrix equal to The exact variance covariance matrix you can use right so, so just to summarize, right, if we cannot assume that the U are normally distributed, we can still rely on the fact that as n goes to infinity, the sampling distribution becomes closer and closer to a multivariate normal distribution. And why that's useful for us is that we're going to rely now on that fact to make to generate our confidence intervals, to do hypothesis tests, to do all the things that we wanted to do with T, but now we can, we can't technically do with T because we don't know that the error is enormous. Okay, let's take a stop for a second and take questions. Is this anything here? Conceptually confusing?

Speaker 1  28:55  
The basic idea there, though, like the sequence of we made this assumption. We got a finite sample, sampling distribution. We lose the assumption. We rely on the asymptotic property. The asymptotic property is this. So

Speaker 1  29:15  
let me show you. So again, very simple model Y is going to be a function of two independent variables. So here I'm just saying. Give me an x matrix that is a column of ones and then two variables, independent variables that are drawn from a multivariate normal and they're correlated point five. Matter my beta coefficients, I have three of them, and intercept beta one, beta two. I'm just going to assume they're one and right? And so what I'm going to do here is take this setup, but I'm going to generate data for y from this basic you know, here's the expected value of Y, but then I'm going to generate data for the actual value of y using two different kinds of error distributions. This first one is for normal errors. So here's just, this is just the OLS estimator, where y is equal to the expected value plus a random draw from a normal distribution. So that's the assumption that we've been making last week with a finite sample sampling distribution. What if y is not normally distributed? What if the error term is not normally distributed? So here I'm going to assume a different error distribution. I'm assuming a beta distribution, and a beta distribution ranges between zero and one, and you know, can look like a lot of different things, but something like that, and I'm multiplying it by 50, just because I don't want it to be between zero and one, and I want it to be closer to the amount of variance that's in that normal assumption that I have there up shot here, right? We have a simple model Y regress on x1 x2 we've got a first assumption, that is the one we made last week, normal errors. We have a possible second assumption, which is that they're not normally distributed, but they're distributed with this awkward beta function. So if we simulate that many, many times, what is the you know? What is the sampling distribution for beta look like. And so I'm just going to focus on on the first beta, beta one under and this is with a sample size of 10. Yeah, a sample size of 10. So a small sample. In the first case, with the small sample and a normally distributed error term, the sampling distribution is normal, which is what we proved last week. And you can tell it's normal, first because it looks normal, but second because, well, it looks normal. It doesn't have any skew. It's symmetric. And one measure of symmetricity, something like that, is the difference between the mean and the media, the mean and the median are both one that tells you it's symmetric. If it's asymmetric, it all mean and median will deviate. So with normal errors, right, we get the result we proved last week, finite sample. Even with a sample of 10, we get a normal sampling distribution with the beta and a sample of 10, the sampling distribution is skewed. It's positively skewed, right? There's more errors out here than there are this way, and you can see that in the difference between the mean and the median, the mean is about equal to the true value, right? The median is point eight, two. So it's a skewed distribution. It's not normal. It's not t we don't know what it is. We would never know what that that is, the sampling distribution before. The only reason I was able to show this is because I'm generating fake data. So if you can assume normal errors, you get the result you get the result, you want a normally distributed sampling distribution. If you can, you might have some random thing that you have no idea what it is that's with a small sample. Let's do the exact same thing, but with a sample of 1000 I'm doing the exact same thing I just did with a sample of 1000 now the sampling distributions are both effectively normal, like this is truly normal, like provably normal. This is converging on normal as the sample gets larger and by 1000 observations. In this case, we have effectively a normal distribution. So that's what we mean by asymptotic number, even when the error, the errors are distributed non normally, and, in fact, quite non normal. As long as the sample size is pretty large, we're getting convergence to a normal sample. So it'll always be approximate, but we can, we can work with that through the approximate so obviously,

Unknown Speaker  34:03  
the sample size here matters a lot. Does the number of parameters also factor in place or no, it's just based on,

Speaker 1  34:11  
yeah, the number of parameters does through the degrees of freedom. And so it's not, it really what I should be saying, and I wasn't careful about this as a good question, is that it's not the sample size, per se, it's the degrees of freedom. And so sample size minus, I mean, it's true that as the sample size goes to infinity, this is true. But what you should be thinking about is the degrees of freedom, especially if you have a small sample in a lot of parameters, if you have 100 sample, but 100 unit sample, but you're estimating 50 parameters, you really only have a 50 unit sample. So that's right, for sure.

Unknown Speaker  34:53  
More questions do

Unknown Speaker  34:59  
you're not

Speaker 1  35:08  
getting it's not that you're getting closer to convergence. It's that it's converging. So it means that straight it's as the sample size goes up any given F, the deviation between any given estimate that you obtain from a sample, and the true value is getting, I mean, this is, this is not the formal way of saying it, but it's getting smaller and smaller as n goes. So you know, if you have a very large sample, you're more confident that your estimator is producing something close to the true value in expectation. I mean this is, this is the formal way of saying that right as n goes to infinity, the probability that your estimate deviates from the true value, that the deviation between these two being greater than some negligibly small value goes to zero. But the way to understand it conceptually is that with larger sample sizes, the expected deviation is smaller,

Unknown Speaker  36:10  
more questions.

Speaker 1  36:16  
Okay, so, so the reason why this is such a useful property is exactly because we want confidence intervals and statistical tests and things like that. So just to summarize, right? Well, so there's this result which is just true. Sampling Distribution is not t on n minus k minus one. If you cannot assume normal errors, we can make the you know, we can get this result of asymptotic normality. That's so that's all true, but in practical terms, this ends up not mattering all that much. And this is why. So Wooldridge talks about this, but this is why. So it's true that the use of t as an exact sampling distribution is not justified if u is not normal. The only way we get that t distribution analytically is if we assume normal errors. However, both t right. So the first thing to note is that the t distribution converges on the normal distribution as n goes to infinity. So if you plot the t distribution with 1000 degrees of freedom, it looks exactly like a standard normal distribution. So t converges on the standard normal as n goes to infinity, and the sampling distribution for beta k hat also converges on a normal distribution as n goes to infinity. And so given that both of these are converging on the same distribution, and both as n goes to infinity, and we have no sense that one is converging faster than the other, we might as well just use t all the time. It's better, right? Using T is better if the errors are normally distributed, because then we know the exact Sam distribution, and it's no worse than the general case, because it's converging on the normal just like the sampling distribution is converging So, so the sort of subtle point here, right, is that there's like the truth of what is going on behind the scenes, Right, which is this, and then there's the practical implications, which is that both the sampling distribution for beta k, when errors are not normal and the t distribution are both converging on the same thing. So it just doesn't matter that much whether we use T or not, and because T is essentially no worse when errors are not normally distributed, but it is better when they are normally distributed. We should, might as well distance t all the time. So

Speaker 4  38:51  
in theory, then if you're piloting or if you're if you're conducting a study, you want the highest n possible, so you would always just use t. So the only instance in which you're not using T is if you're piloting a study or you know that your T is,

Unknown Speaker  39:08  
well,

Speaker 1  39:10  
okay, so let's think of it. Let's think of it. So, so let's say you have a small sample size and you're very confident your errors are not normally distributed there. The answer is, you know, like, the simple answer is that, yeah, T is not justified in that case, because you know that, because the normal, the errors are not normally distributed, the sampling distribution is not t, you know that, and so T is not justified. The problem is, is that it sounds like you want to move to the normal, but that's also not justified, because the normal is only you're only it's only an approximate approximation with the sampling distribution, and that approximation gets better and better as n goes to infinity. So if you have non normal errors in a small sample size, it's true that t is not really justified, but moving to the normal is also not justified, and so you're kind of screwed either way, right? And so what you would do in a case like that is what we're going to talk about in like a week or two, which is boot scoping.

Speaker 5  40:11  
I think it's a related question, but better under conditions of normal

Speaker 1  40:16  
errors, no worse compared to wait. So under conditions of normal errors, you

Unknown Speaker  40:21  
see that using the t distribution is at all

Speaker 1  40:24  
better yeah, compared to the normal, the standard normal yes or yeah just it's better than assuming the sampling distribution is normal, because we know it's T Yeah. And in the more

Unknown Speaker  40:34  
general case, it doesn't really matter,

Unknown Speaker  40:37  
because it's open margin to normal

Speaker 1  40:40  
Exactly. So it's no worse than just using the normal, exactly. Yeah, okay, yeah, because using the normal requires on the asymptotic normality, and using T you're also converging on asymptotic normality, and so they're both, it's effectively the same, same problem, and so you're not getting worse. Yeah. I mean, what you said is, right? Is, is there anything else,

Speaker 1  41:09  
and so, so in practice, right? What is that going to mean? It means that when you know, unless you have a situation like Neha was just talking about where we're going to come up with alternative strategies for dealing with that in a couple weeks, you're just going to default to the t distribution, regardless of assumptions about the error term, and regardless about the size of your sample, you're just going to use t all the time. And so you're going to do the same exact significance test, the same exact confidence interval that we talked about last week. All that's going to be the same. Okay, more questions about

Unknown Speaker  41:53  
this. Why do people

Unknown Speaker  41:59  
like in any context,

Speaker 6  42:05  
given what you just said, it would seem that the standard should just be

Speaker 1  42:11  
to use the t distribution. It is. So with linear regression, it is. So that's pretty much, I think, in every package that you would encounter, you're going to get t statistics out of a linear regression. You for sure. So when people.

Speaker 1  42:26  
use that scores of papers, that's just like a different like because that order paper isn't that the standard n. So it depends on what you're talking about. I mean, if like z scores in general, they could be used for standardizing variable sort of things like that. And when you talk about a z score in terms of standardizing a variable, that just means that you're subtracting up the mean and dividing by the standard deviation, but you're not necessarily assuming it's normal. The place where you where you will see Z and is often the default, or is the default for whatever packet you're going to use is if you're running models, generalized linear models with like non linear link functions. So Logit, probit, order, log order, probit. And in that case, you're relying on asymptotic normality. And you know, why can you use T in that case? I just don't. I don't think there's any underlying there's no finite sample equivalent in in those generalized linear model cases where it would make sense to think about it as T. So you're from the very beginning of the models you're relying on asymptotic normality and you're estimating those estimators are maximum likelihood estimators, rather

Speaker 7  43:49  
than for like, different tests that you use, like proportions test, because like, what it's giving you is a z score, and that's what you can connect to the distribution.

Speaker 1  43:57  
Yeah, yeah. It's very those are very closely related, as you know, Z test for proportions and like Logit, proba or those are closely related things. So I guess the answer is, you know, if you're seeing Z scores in a paper, my guess would be that they're estimating something using maximum likelihood, and that it's like a non linear model of some sort, and in those cases, you're relying from the very beginning, right? There is no finite sample distribution equivalent to OLS. It's just you're relying on the asymptotic. You're relying on the convergence of the sampling distribution for those coefficients to the normal but all the tests are the same, right? So, like when you get, if you run a load for probit and r of theta, and you get a set of coefficient outputs, it will give you beta standard error, Z, P value. You're doing the exact same thing that you're we've been doing with T. The only difference is you're using the standard normal has the cumulative distribution function instead of the T on n minus k map. So k map. More

Speaker 1  45:16  
questions. OK, so the last thing I want to talk about in this section is this page of Wooldridge, which I got a question on for the discussion on the discussion board, on canvas. And I totally agree that this is a very confusing section. It's like, he's saying asymptotic efficiency of all US, and then he's talking about instrumental variables, and it's like, what is going on? So it is confusing. The simple answer to what's going on here, and the simple answer to what's going on here that's related to the title of the section is that he's proving that OLS is asymptotically efficient within a particular class of estimators. And so it's asymptotically as n goes to infinity, the OLS estimator has a variance that is no greater than any other possible estimator within a particular class of estimators, and that particular class of estimators is what he's developing here. But it's a little obscure exactly how to think about this class of estimators. And so I don't want you to focus so much on trying to understand that class of estimators beyond what I think is really interesting aspect of this discussion, which is that what he's developing here is the instrumental variable estimator. And so we're going to get into this more, but what he's doing is defining a new variable z, and that variable is some function of x, some function of x, but it can be any function of x. It could be a highly non linear function of x. So it's not going to be perfectly correlated necessarily with x, but it's going to, you know, in general, it's going to have some correlation with x, but not a perfect correlation. And what he's showing is that you can define a new estimator that's not an OLS estimator. Instead, it looks like the OLS estimator in the sense that it has the same structure, but the numerator and denominator are different. In the OLS estimator, this would be x minus x mean times y, and this would be the variance of x in the denominator. So it's basically covariance over variance and covariance of X and Y over variance of x. This new estimator, he's defined defining puts this z variable into the numerator, where z is some function of x, and then puts the covariance between z and x and the denominator. And this is just what we are, what we are doing when we estimate an instrument instrumental variable to regression, we're taking some variable z and using it almost as a proxy for x, and to the extent there is a correlation between z and x, we what he's showing is that we get a consistent estimate, our estimator, this instrumental variable estimator, is consistent for the original beta one that we're interested in. And he shows that by rearranging this and we did a different form of this in matrix algebra, but this new instrumental variables estimator is equal to the true value beta one plus this thing on the right hand side, and that thing has the covariance between z and u in the numerator. So if we can assume that this function of x is uncorrelated with u, that whole thing drops out, and we're left with a consistent estimator for beta one. That's exactly what I was talking about over over here before, where I was saying, you know, when we have a situation where we're worried that x is correlated with the error term, if we can find another variable z that's correlated with y only through x in the sense, and what that means in practice is that it's uncorrelated with the U, then we have a consistent estimator for beta one, even if x is endogenous. So he's developing the class of estimators that we think of as instrumental variables estimators, and saying that within that class, OLS is asymptotically efficient, in the sense that it has no higher variance than any estimator you can come up with in this class. One more thing, and so that's a result that we're going to use. We'll use when we talk about instrumental variables estimators, in the sense that it'll be the case often, that there's a big trade off that you're going to face between getting a consistent estimator using an I V estimator and getting a biased, inconsistent, but lower variance estimator using OLS. Right? So if you think there's endogeneity, OLS is biased and inconsistent, but it has lower variance than the IV estimator. And so if your I V estimator has really high variance, it doesn't matter that it's consistent, it's not, you're not going to get a good estimate. So that's kind of the result that he's developing here. That's going to be useful later. Yeah,

Speaker 7  50:16  
just to clarify, or if you need to, like, further, if you could further define or explain what he means by like, the class of estimators, are you just saying that that class is instrumental variable estimators.

Speaker 1  50:28  
So, so the class, the class of estimators. When he says class of estimators, he means any alternative estimator to OLS that can be shoved into this framework. So, so this is very general in the sense that so beta one tilde is equal to this thing here. And the question then is, well, what is what is z? And z is some function of x. So if you can come up with some estimator that can fit into this framework, then it's part of that class of estimators. And it turns out that this is, this is called the instrumental variables estimator that we will use when we're trying to deal with endogeneity. And then he developed the multi variate version of that further down, which is the one that we'll use when we get there.

Speaker 8  51:16  
So to summarize what you just said, using that when Max is, not exogenous, and we have this assumption of the covariance between v and f, v and u equals to zero, so that even though it is not exogenous, we can still Have a estimator that is unbiased,

Speaker 1  51:39  
not unbiased, consistency. Yes, exactly. The key assumptions for IV estimators are that the correlation between the instrumental variable z and x is non zero, and the covariance between z and u is zero. And if those two assumptions are met, then you have a consistent estimator for beta one, even if x is endogenous in the model. And so when people talk about exclusion restrictions, which you'll hear all the time in political science, they're talking about this kind of problem where you're trying to figure out if z is properly excluded from the model. And by properly excluded, you mean that it has a zero correlation with u, because a zero correlation from u means that it's not it doesn't have any influence on Y except through x. That's what it means to be properly excluded. Ok. Okay, so, yeah, is that saying

Speaker 5  52:46  
something more general about how we should trade out bias and efficiency in that sometimes even if you know is bias, it has efficiency gain that

Speaker 1  53:00  
still preferred. It's part of that more general point. So it is one instance of this much more general problem that I tried to highlight throughout the semester, which is that very often it's the case that we have estimators that are biased but more lower variance compared to estimators that are unbiased or consistent but have higher variance, and we're worried about trying to figure out which one is better. And so that is true, right? You know, in instrumental variables analysis, you know, the default tendency is to think there might be endogeneity. Let me go to IB. But if the end You know, if the correlation of the error term is small, so you don't think there's all that much bias you and you have a very poor instrument. If your instrument is bad, your variance of the ID estimator will be very large, and it might just be worse than suffering the bias of the OLS estimator. That can definitely be true. So yeah, it's one instance of this much more general. I

Speaker 1  54:05  
more questions. So that's, I mean, someone asked about this. That's what's going on in this section. But if this is confusing, Ray, I'm just kind of giving you a taste of what's to come. This is not, I don't expect you to understand everything here yet, because we're going to spend an entire week on IBS develop it in depth at that point. So we have like 15 minutes left in today's class. There are two options we're moving on next week to Well, I guess let me, let me preface this by saying, Let me preface this. So first, I understand that this asymptotic section is a bit confusing. What I really care about is that you take away the key points of this. And if we were going to go through and sort of take, take away the key points, right? The first point is that OLS is consistent, and it's consistent under this assumption that very closely related to the concept of endogeneity that we've been talking about, we're worried when we're worried about consistency, we're worried about the extent to which an independent variable is correlated with the error term, which is just exogenating. We're worried that x is not exogenous, that there's some Omitted Variable in U that's correlated with both x and y. Yeah, I understand,

Speaker 9  55:41  
like the depth I

Speaker 1  55:51  
don't. I don't care whether you're able to prove this or not. I wanted to show you this because it gets us this assumption. What I want you to know is the assumption. So this tells you how we get to what assumption is needed for consistency. What I care about is that you know that assumption and understand it and understand what happens when violated. But I don't care whether you can prove that that's the assumption. So I want you you should know the difference between these two assumptions, right? And you should know sort of what this means. But yeah, the key point in both of these cases right is we're worried that there's some excluded variable that's correlated with both x and y that creates this correlation between the independent variable and the error term. And if that's true, then OLS is no longer unbiased, and it's not and it's inconsistent, it's no longer consistent. So that's that's important. That's super important. This is the main problem that we take all the time. This remainder thing is, is subtle and not all that interesting, right? So you don't really care that much whether you understand understand that, but it is something that we'll get talked about, and it's worth thinking about, perhaps, but don't worry too much about that. What I re what I think is extremely important, right? Is knowing that sort of understanding this idea of the asymptotic normality of beta hat, because it's very important to this result that we get, which is that, even though we started out by assuming that the error term is normally distributed, and that was very useful for us, in practice, because of The asymptotic normality of beta hat, we're not nearly as concerned about the normality of the error term as we would be other x, right? It just, it turns out, because this is true, right, asymptotically, that we just don't have to worry as much about the normality of the error term as we may think. So. It's great if that's true, but it's just not as big a deal as you might and the situations that you are going to be worried the most about are exactly the ones that I think nehan was talking about before, which are the situations where you have small sample sizes and non normal errors, that situation where everything just kind of falls apart with respect to sampling distribution, and you need an alternative strategy for standard errors. We'll talk about that.

Speaker 1  58:33  
Yeah, and so, I mean, those are the big points. There's a lot of subtlety here, but the big points are, those are those are the big points. Okay, so that's number one. Number two is that this is this sort of the end of this week. Sort of represents the end of our six week. Kind of slow introduction to OLS, how to think about linear regression, interpret things, calculate standard errors, do confidence intervals, all the basic stuff. As we move into next week, that's when things start picking up more so starting next week, we're going to be doing different kinds of functional form assumptions, so non linearities and relationships, interactions, polynomials. Then we're going to dive into more advanced methods for inference. So there's a sense in which things are going to feel like maybe they're getting they're speeding up. And the reason why is because, you know, we took a slow introduction so that we could develop good foundations, and then I want to rely on those foundations to do more advanced things. So the upshot of all of that is that if you're feeling, you know, if you're not very comfortable with certain aspects of what we've gone through in the first few weeks, is we need to solve that like ASAP, because it's going to get more complicated as we go forward. And so I need you to understand how to interpret a coefficient, how to do a derivative right, like, you know, being able to, you know, look at some of this notation and just sort of see it right without having to think too much about it is very useful. All this kind of very basic stuff, standard error, confidence interval, I'm going to assume that that's good, right, that you're good with that. And so that's not true, right? If there's a chance that you're going to struggle as we speed up and do more advanced things, so I very much want to work with you to make sure that you have the foundations that are needed to do the more advanced stuff. So if you're worried about that, let's talk, and let's figure out a way to solve it

Unknown Speaker  1:00:41  
so that those are sort of the two preface things. I guess. You know, we are running out of time, but I

Speaker 1  1:00:50  
want to take any sort of questions you have. I mean, I have, I can go into the slides for next week. There's a lot of material, so I'm happy to get started on that. But I'm also happy to just talk more about things that you're confused about, or whatever

Speaker 7  1:01:07  
you want. Yeah, so going back to the wolf bridge page 175 that Z, does that have anything to do with like we if you could just explain one more time like the z like whenever you said, how,

Speaker 7  1:01:30  
if we can assume that this function of x is correlated with you, then the whole thing drops out. The weakness, then we still have an estimator for beta one. Yeah. So our is, our whole objective with this is just trying to still have like, something left in the end. But is that good thing, or is it something we can rely on as, like, a solid estimator? Or can you say the last part? Is it something that we can rely on as a solid estimator? Or is it just like, Oh, here's what's left.

Speaker 1  1:01:57  
It totally depends on how highly correlated z is with x. If so, if you think about, you know, think about a situation, you have some dependent variable, you have some independent variable of interest X, and you're worried that there's some, there's some endogeneity, right? That x is correlated the error term, if you can find another variable in your data that is highly correlated with X, the endogenous variable, but doesn't have any independent effect on y, that's called an instrument. And if you plug that into this estimator, that z variable into this estimator, you'll get a consistent estimate for beta for that effective x, but the quality of that estimate, in the sense of the amount of uncertainty you're going to have in it, is totally dependent on how correlated z is with x. When z is highly correlated with x, we say it's a good instrument. When it's weakly correlated with x, we say it's a poor instrument. And when it's a poor instrument, that doesn't mean this is inconsistent. It's still consistent, but the variance is so big that we're not going to get our confidence bound to be so large, we're not really going to have any good sense of what the true value paid for. So the trick with IV is, is number one, finding something that is correlated with x, but not with y independent of x. That's like, that's the big problem. It's almost impossible to find those in most social science publications. But once you have something like that, the big concern becomes, how strong is the correlation between the two? And

Speaker 7  1:03:41  
then another question that sounds that separate. So when we're talking about our big and small samples, are we thinking of a specific number or like threshold, or is it just like, Well, no,

Speaker 1  1:03:54  
maybe you'll never know. That's probably okay. You can't know because you only have your one sample and you don't know the true model. So there isn't, you can't ever know, right, how good is and how much is enough. I mean, there might be, like, rare situations where you can do some simulations for, like, some general class of problems that you might fit into, but you shouldn't really think that that's gonna be the case for you at any given time. So there. So the first thing to say is, there is no good answer to that question, because it's the very nature of consistency. Is that it only holds as n goes to negative. It is a property of n going to infinity, not of any specific threshold value. Having said that, you can see, I don't want to say anything precise about this, because inevitably, it's wrong in some way. But you know, for typical applications that you're going to come across, you know, you wouldn't be too worried if you had a sample size of, like, you know, a couple 100 or more, or even, like 100 maybe you wouldn't be terribly worried. I would start thinking about, I don't know, 80, for some reason in my head, I have, like, 100 as a threshold where I'm starting to worry and moving to bootstrapping. But even that is, like, it's very kind of arbitrary, and it's not well founded. So all I can tell you is, like, you know,

Speaker 7  1:05:23  
if you have, you know, if you have the typical sample sizes that we, most of the time, are dealing with, you're usually not too worried about. But on the point of simulation, like, is that just like best practice, or I just kind of imagine that it's like some kind of, like, Frank and dine like, thing that is just going to be really messy and really hard

Speaker 1  1:05:41  
to you mean the simulations that I did as examples, and whether you could do that in practice? Yeah, yeah. Usually it's going to be very hard to do it in practice, because it requires particular assumptions about things like, you know, if you go back to my simulation, right, I know the true relationship between the x's. I know, you know, I know everything right? And so you know you could come up with sort of a general class of models that your model might be a part of, and you could explore its properties using simulation. And you can always do that, right? No one expects you to do that, but you could always try to do it. And the question is just, you know, how confident are you that your model is a good match for the class of models that you're investigating through simulation? And to the extent it's a very good match, then you learn about this property scale, but it does require that you sort of fit your you make an assumption about how your model relates to some fabricated class of set of models that you're exploring.

Speaker 2  1:06:46  
Yeah, I'm just wondering about how to substantively understand the asymptotic efficiency. So the first two sentence so under the Gauss mark of assumptions, we know that the OLS estimators are the best, Linear Unbiased, okay, but it is also asymptotically efficient among a certain class of estimators. So, is this? How does is this a certain class of estimators, like, is it like a bigger group, group, than than what we what we compare to under finite sample.

Speaker 1  1:07:23  
I think it has to be smaller because you're because this says it's the best Linear Unbiased Estimator, and that I think that has to be a bigger class than this, because I think this is just a subset of linear estimators, so I think this is getting a little bit beyond what I feel confident saying. But yeah, you're right, right? This is a smaller class than that class, but I can't prove that, so I don't Yeah.

Speaker 2  1:07:57  
So my second question is, it is only asymptotically efficient if all the Gauss map called assumptions are met as well. Because previously we were talking about like, Oh, if this assumption in finite sample isn't met, when n gets to infinity, we get these properties. But like when I read that sentence like, Wait so,

Unknown Speaker  1:08:34  
I mean, I think I

Speaker 1  1:08:39  
wonder if Here he's using Gauss mark of in two different ways. That makes it confusing. Because what I want to say that, you know, we've made this distinction between, we made this distinction between these two versions of this endogeneity. And one I would, I would think of as like a classic, the classic, classic gas mark up, and this one, I think it was as a less stringent version of that. And I think what he's doing is using this one for that asymptotic efficiency. But let's that's a good question that is confusing,

Speaker 2  1:09:16  
because if it's both stringent assumptions at large n,

Speaker 1  1:09:21  
that the, yeah, exactly, exactly. I mean, I'm pretty sure he's just using gas mark up in an ambiguous way, but he really, what he's referring to is this less stringent assumption covariance of un X, because that's, that's the key, you know, that's the key assumption.

Unknown Speaker  1:09:39  
Okay, okay,

Speaker 1  1:09:42  
that's a good question. Now I have to think more about it, and I don't want to. I can't say anything for sure, because I am confused by that in the same way that you are.

Speaker 3  1:10:02  
I I mean,

Speaker 1  1:10:14  
the other thing, the only other thing right that you could say is that you know you can you can both prove you can prove two you can always prove two things, right? You can prove finite sample properties, n asymptotic properties. And so it's also possible, right? Then, for some estimator, it's the best, most efficient estimator for any given sample size, and that it's also asymptotically efficient. And those are two different proofs, right? And so here he relying on, you know, like the large numbers, to get this asymptotic property. So I'm not, I'm just not 100% confident. Let me think about that more. Okay, all right, we're over time. So next week, again, we'll start on some more advanced material. So please definitely come see me for healing. Less than confident about any particular topic. And.

Transcribed by https://otter.ai
