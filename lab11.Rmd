---
title: "Lab 11"
output: 
  html_document:
    theme: united
    toc: yes
    toc_float:
      collapsed: true
editor_options: 
  chunk_output_type: console
---

# Data 

We’re going to return to the `chile` data set. The variables of interest are as follows:

- `vote`: a factor with levels: A, will abstain; N, will vote no (against Pinochet); U, undecided; Y, will vote yes (for Pinochet).
- `statusquo`: higher values = more support for the status quo, standardized to have mean zero and standard deviation of one
- `income`: monthly income, in pesos
- `education`: factor variable with 3 levels: primary only (baseline), secondary (S), post-secondary (PS)
- `sex`: factor variable with two levels, female (baseline) and male (M)
- `region`: A factor with levels: C, Central; M, Metropolitan Santiago area; N, North; S, South; SA, city of Santiago.

```{r}
chile <- carData::Chile

# re-code vote as dummy 
chile$vote_yes <- ifelse(chile$vote == "Y", 1, 
                         ifelse(chile$vote == "N", 0, NA))

# re-code sex as dummy 
chile$female <- ifelse(chile$sex == "F", 1, 0)

# re-code income to 1000s
chile$income_1000 <- chile$income / 1000

# re-code education as dummies 
chile$educS <- ifelse(chile$education == "S", 1, 0)
chile$educPS <- ifelse(chile$education == "PS", 1, 0)

# remove missing
chile <- na.omit(chile)
```

# Cross Validation

Estimate the following model: 

$$
voteyes_i = \beta_0 + \beta_1statusquo_i + \beta_2income1000_i + \beta_3educS_i + \beta_4educPS_i + \beta_5female_i + u_i
$$

```{r}
m1 <- lm(vote_yes ~ statusquo + income_1000 + educS + educPS + female, chile)
summary(m1)
```

## Clustered SE 

Clustered standard errors adjust for the possibility that observations within the same group (e.g., a region) are not independent from each other. 

```{r}
# vcovCL function in sandwich package
se_cluster <- sqrt(diag(sandwich::vcovCL(m1, cluster = ~ region, type = "HC1")))
```

## Coefficient Plot

Create a figure that plots the coefficient estimates for all independent variables along with their 95% confidence bounds (a “coefficient plot”).

```{r}
# confidence intervals 
lci <- m1$coefficients - 2*se_cluster
uci <- m1$coefficients + 2*se_cluster

# data frame  
results <- data.frame(coefficient = coef(m1)[-1], 
           lower.ci = lci[-1],
           upper.ci = uci[-1])

library(ggplot2)
ggplot(results, aes(x = rownames(results), y = coefficient)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = lower.ci, ymax = upper.ci), width = 0.2) +
  labs(title = "Coefficient Plot with 95% Confidence Bounds",
       x = "Variable",
       y = "Effect on Probability of Voting for Pinochet") +
  geom_hline(yintercept=0, linetype="dashed", color = "red") +
  coord_flip() +
  theme_minimal()
```

Use 10-fold cross validation to calculate the out-of-sample (test) mean squared error and $R^2$. 

- Out-of-sample (test) mean squared error measures the average prediction error (how far off the predicted `vote_yes` values are from the actual). (The lower the better).
- Out-of-sample (test) $R^2$ measures the proportion of variance in the binary outcome (vote_yes) explained by the model. (The higher the better). 

How do we expect the out-of-sample (test) metrics to compare to the within-sample (train) metrics from 'm1'? 

```{r}
# set up the training procedure
m1_trainControl <- caret::trainControl(method = "cv", number = 10)

# train the model using cross-validation
m1_10fCV <- caret::train(formula(m1), data = chile, method = "lm", trControl = m1_trainControl)

# cross-validation results 
print(m1_10fCV)

# compare to values from m1
train <- c(summary(m1)["sigma"], summary(m1)["r.squared"])
test <- c(m1_10fCV$results["RMSE"], m1_10fCV$results["Rsquared"])

cbind(train, test)
```

What does it mean if the test RMSE and $R^2$ are similar to the train RMSE and $R^2$?

## Fixed Effects 

Fixed effects adjust for the possibility of unobserved heterogeneity across groups by controlling for group-level omitted/confounding variables. 

Estimate a new model that uses fixed effects for `region` to account for all regional-level variance in `vote_yes`. 

```{r}
m2 <- lm(vote_yes ~ statusquo + income_1000 + educS + educPS + female + region, chile)
summary(m2)
```

Use 10-fold cross validation to calculate the out-of-sample (test) mean squared error and $R^2$. How do we expect the out-of-sample (test) metrics from the fixed effects model to compare to the out-of-sample (test) metrics from `m1`?

```{r}
# set up the training procedure
m2_trainControl <- caret::trainControl(method = "cv", number = 10)

# train the model using cross-validation
m2_10fCV <- caret::train(formula(m2), data = chile, method = "lm", trControl = m2_trainControl)

# compare to values from m1
m1_test <- c(m1_10fCV$results["RMSE"], m1_10fCV$results["Rsquared"])
m2_test <- c(m2_10fCV$results["RMSE"], m2_10fCV$results["Rsquared"])

cbind(m1_test, m2_test)
```

As expected, the out-of-sample (test) mean squared error in the fixed effects model is slightly larger than the out-of-sample (test) mean squared error in the original model. 

The out-of-sample (test) $R^2$ in the fixed effects model is slightly smaller than the out-of-sample (test) $R^2$ in the original model (0.20). 

This is because the fixed effects model is fitting the training data very closely, including noise tied to specific `region` dummies. On new (test) data, those specific patterns don’t generalize, so prediction error (MSE) increases and predictive performance ($R^2$) decreases. 

# Leave One Out CV

## By Hand 

```{r loocv}
error <- NA

for (i in 1:nrow(chile)){
  
  # estimate model without observation i 
  m <- lm(vote_yes ~ statusquo + income_1000 + educS + educPS + female, chile[-i,])

  # use model to predict vote_yes for observation i 
  pred <- predict(m, newdata = chile[i,], type = "response")
  
  # store difference between actual and predicted 
  error[i] <- chile$vote_yes[i] - pred
  
}

# root mean squared error (RMSE)
sqrt(mean(error^2))  
```

## With `caret` 

```{r}
# set up the training procedure
m1_trainControl <- caret::trainControl(method = "LOOCV")

# train the model using LOOCV
m1_LOOCV <- caret::train(formula(m1), data = chile, method = "lm", trControl = m1_trainControl)

# print RMSE 
m1_LOOCV$results["RMSE"]
```

## Analytically 

For OLS, there is an analytic solution for LOOCV (where $h_i$ is the leverage of observation $i$):

$$
\text{LOOCV} = \frac{1}{N} \sum_{i=1}^N \left[\frac{\hat{u}_i}{(1 - h_i)} \right]^2
$$

```{r}
# analytical RMSE
sqrt( sum( (m1$residuals / (1 - hatvalues(m1)))^2 ) / nrow(chile) )
```

# Family-Wise Error Rates 

Consider the following randomized survey experiment, which tested different messaging strategies to promote vaccination against COVID-19.

- `vaccine_message`: the message respondents received. One of:
    + Control -- no message
    + Patriotism -- emphasizing vaccination as good for the country
    + People you know -- emphasizing social norms regarding vaccination
    + Physician recommend -- asking the respondent to consider what they'd do if their personal doctor recommended vaccination
    + Preventing harm -- emphasizing that vaccination is important to reduce risks for others
    + Scientists recommend -- emphasizing the scientific consensus regarding vaccination

- `vaccine_attitude`: respondents' self-reported likelihood of getting vaccinated 
    + 1-7 scale, higher values indicate higher likelihood
    
```{r}
load("data/experiment")
print(s1)
```

What issues might arise here with respect to using p-values in the regression output combined with $\alpha=0.05$ to test each distinct coefficient?

- This experiment has multiple treatment arms. 
- Repeated, independent trials have a family-wise error rate
- Increases the rate of false positives
- Adjust alpha to test for statistical significance

```{r}
# corrected alpha
alpha <- 1 - (1-0.05)^(1/6) # 0.008512445

# significance tests
coef(s1)[2,4] < alpha # patriotism 
coef(s1)[3,4] < alpha # people you know
coef(s1)[4,4] < alpha # physician 
coef(s1)[5,4] < alpha # preventing harm 
coef(s1)[6,4] < alpha # scientists

s1$df[2]
```

Let's make another coefficient plot, this time with base R. 

```{r}
c <- qt(1 - alpha/2, s1$df[2])

df <- as.data.frame(coef(s1))
df$lci <- df$Estimate - c*df$`Std. Error`
df$uci <- df$Estimate + c*df$`Std. Error`

plot(c(1,2,3,4,5), coef(s1)[-1,1], pch=19, xlab="Messaging Treatment", ylab="Estimated Treatment Effect", main="Effect of Messaging on Vaccine Attitudes", ylim=c(-0.1,0.5), xlim=c(0.5,5.5), axes=F)
axis(1, at=1:5, labels=c("Patriotism", "Familiar", "Physician", "Harm", "Scientists"))
axis(2, at=c(0,0.1,0.2,0.3,0.4,0.5))
segments(1, df$lci[2], 1, df$uci[2])
segments(2, df$lci[3], 2, df$uci[3])
segments(3, df$lci[4], 3, df$uci[4])
segments(4, df$lci[5], 4, df$uci[5])
segments(5, df$lci[6], 5, df$uci[6])
abline(h = 0, lty = 2)
box()
```


