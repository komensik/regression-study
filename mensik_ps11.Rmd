---
title: "mensik_ps11"
output: html_document
date: "2026-01-05"
---

```{r setup, echo=FALSE, warning=FALSE, include=FALSE}
library(lmtest)
library(sandwich)
library(dplyr)
library(knitr)
library(kableExtra)
library(ggplot2)
library(carData)
library(broom)

```

# 1.

```{r, echo=FALSE, warning=FALSE, include=FALSE}
refug <- read.csv("/Users/kristina/Documents/GitHub/regression-study/refug.csv")

# recode variable decision so that 1 equals grants the appeal and 0 equals denies the appeal (right now is 2 = grants the appeal and 1 = denies)
refug$decision_num <- ifelse(refug$decision == "yes", 2, 1)


# check judge -> recode to factor
refug$judge_factor <- factor(refug$judge)
#levels(refug$judge_factor) #ordered alphabetically

#check rater -> factor
refug$rater_factor <- factor(refug$rater)
#levels(refug$rater_factor)

#language to factor
refug$language_factor <- factor(refug$language)

#location to factor
refug$location <- factor(refug$location)

#success
#unique(refug$success)
#class(refug$success)

#levels(refug$language_factor)
#levels(refug$location)

#class(refug$success)
```

## 1a.

```{r, echo=FALSE, warning=FALSE, include=FALSE}

m1 <- lm(decision_num ~ judge_factor + rater_factor + language_factor + location + success, data = refug)

# clustered standard errors by nation

cov_clust <- sandwich::vcovCL(m1, cluster = ~ nation, type = "HC1")

cte <- lmtest::coeftest(m1, vcov. = cov_clust)

# table
coef_data <- data.frame(
  Coefficient = cte[, "Estimate"],
  Standard_Error = cte[, "Std. Error"],
  T_Statistic = cte[, "t value"],
  P_Value = cte[, "Pr(>|t|)"]
)

coef_table <- knitr::kable(
  coef_data,
  format = "html",
  longtable = FALSE,
  caption = "Regression Coeff. Estimates with Clustered Standard Errors"
)

styled_table <- kable_styling(
  coef_table,
  full_width = FALSE,
  bootstrap_options = c("striped", "hover")
)
print(styled_table)
```

judgeHeald - the -0.238 coefficient means that having Judge Heald as opposed to Judge Desjardin is associated with a 0.24 decrease in the likelihood of having a case granted, holding all else constant. This effect is highly significant.

locationToronto - holding all else constant, having a case heard or seeking refugee status in Toronto instead of Montreal is associated with a 11.3 percentage point higher probability of being granted the case compared to a case/seeking refugee status in Montreal. However this is not statistically significant.

success - holding all else constant, a one unit increase in the logit of the success rate of all cases from the refugee's nation of origin is associated with a \~20.8 percentage point increase in the probability that a judge grants the appeal. A unit increase in logit is the natural log of the probability of a nation's success rate – so a country with a 30% success rate's logit would be ln(0.30/0.70) \~ -0.8. This is nonlinear, but roughly corresponds to fairly significant changes. It is statistically significant as well.

## 1b.

```{r, echo=FALSE, warning=FALSE, include=FALSE}

clust_se <- sqrt(diag(cov_clust))

lci <- m1$coefficients - 2*clust_se
uci <- m1$coefficients + 2*clust_se

results <- data.frame(coefficient = coef(m1)[-1],
                      lower.ci = lci[-1],
                      upper.ci = uci[-1])

ggplot(results, aes(x = rownames(results), y = coefficient))+
  geom_point(size = 2) + 
  geom_errorbar(aes(ymin = lower.ci, ymax = upper.ci), width = 0.2) + 
  labs(title = "Coefficient Plot with 95% Confidence Bounds",
       x = "variable",
       y = "Effect on Probability of Case Granted") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  coord_flip() +
  theme_minimal()
```

## 1c.

```{r, echo=FALSE, warning=FALSE}

set.seed(123)
m1_training <- caret::trainControl(method = "cv", number = 10)

#train model
m1_10fold <- caret::train(formula(m1), data = refug, method = "lm",
                          trControl = m1_training)
#cross-validation results
print(m1_10fold)

#vs m1 values
m1_sr <- c(summary(m1)["sigma"], summary(m1)["r.squared"])
m1_10f_sr <- c(m1_10fold$results["RMSE"], m1_10fold$results["Rsquared"])
cbind(m1_sr, m1_10f_sr)
```

The out of sample (test) MSE is slightly (0.0081) larger than the in-sample MSE/residual standard error, which suggests the model was slightly overfitted (the DV is a 0-1 variable, so 0.41 means predictions are off by 41 percentage points on average – less stark when compared to 40 percentage points).

The out of sample R2 drops by about 0.034 (from explaining about 25% of the variance to 21.5%) - so 3.4 percentage points of explained variance disappear when moving to new data. This is not so severe as to suggest the model doesn't have predictive capacities, or that the model is severely biased.

There are differences because the model is created to fit (OLS) the observed data. Held-out data doesn't have the noise on the in sample data.

## 1d. 

```{r, echo=FALSE, warning=FALSE}
#dropping success bc success is constant with nation

m1_fe <- lm(decision_num ~ judge_factor + rater_factor + language_factor + location + as.factor(nation),data = refug)
```

## 1e.

```{r, echo=FALSE, warning=FALSE, include=FALSE}
#training proc set up
m1fe_training <- caret::trainControl(method = "cv", number = 10)

#train model
m1fe_10fold <- caret::train(formula(m1_fe), data = refug, method = "lm",
                          trControl = m1fe_training)
#cross-validation results
print(m1fe_10fold)

#vs m1 values
fe_sr <- c(summary(m1_fe)["sigma"], summary(m1_fe)["r.squared"])
fe_10f_sr <- c(m1fe_10fold$results["RMSE"], m1fe_10fold$results["Rsquared"])

cbind(m1_sr, m1_10f_sr, fe_sr, fe_10f_sr)
cbind(m1_sr, fe_sr)
cbind(m1_10f_sr, fe_10f_sr)
summary(m1_fe)

(m1_sr$sigma) - (fe_sr$sigma)
```

First, both models have lower $R^2$ and higher error under 10-fold cross-validation because out-of-sample tests do not “reward"the original models for being fitted to the data's particular random noise.

Nation fixed effects modestly improves in-sample fit but increase variance, which may explain its bigger to cross validation drop in the out of sample. Moreover, the fixed effects out of sample does not account for the fixed effects models' training of data to nation-specific noise - which also explains the \~0.3 to 0.2 drop in R-squared/explained variance, and increase in prediction error (0.4 to 0.41).

Still, the fixed effects models' higher out-of-sample $R^2$ suggests some predictive benefits to accounting for nation-level differences, and underscoring the bias-variance tradeoff. In m1, success it imposes a functional form on how 'nation' matters/says how a unit fares in relation to others in their country, or allows m1 control for country rates. Compared to the fixed effects model, this somewhat obscures country-level variation. The fixed effects model (nation as a factor makes each nation (except for the baseline) a parameter and gives each its own baseline), which allows the model to more/less absorb nation-level differences.

As with the tradeoff, adding country level parameters also muddy interpretability at the cross-national level, but it is in this case potentially useful (depending on your theory/aims) to see by-country variation.

# 2.

## 2a.

```{r, echo=FALSE, warning=FALSE}
library(carData)
data("Baumann")

#summary(Baumann$group)
#view(Baumann)
Baumann$pretest <- (Baumann$pretest.1 + Baumann$pretest.2) / 2

```

## 2b. 

```{r, echo=FALSE, warning=FALSE}
m_post1 <- lm(post.test.1 ~ group + pretest, data = Baumann)

m_post2 <- lm(post.test.2 ~ group + pretest, data = Baumann)

m_post3 <- lm(post.test.3 ~ group + pretest, data = Baumann)

```

## 2c. 

Including the pretest measure might be beneficial because – even though randomization balances all pre-treatment covariates in expectation – covariates might be imbalanced by chance, creating internal validity problems and inflating variance of the estimator.

the analysis involves multiple treatment arms (Basal, DRTA, Strata) across multiple outcomes, and repeated independent trials have family-wise error rates that are elevated. This increases the rate of false positives. Formal adjustments (alpha) might reduce power in this small (n = 66) sample. Moreover, since folks' post-test scores will vary due to prior ability, and while we're not worried about endogeneity, we can reduce residual variance assuming the control accounts for individual variability (increase the model's explained variance/R\^2).

## 2d. 

First, worth flagging that 22 students per treatment group is fairly low statistical power. Second, the error rate this kind of series or "family" of tests – two different treatment contrasts per each three separate regression means 6 hypothesis tests - or the family-wise error rate is: $\leq$ 1 - (1 - 0.05)\^6, or \~ 0.265. There's a 27% probability of incorrectly rejecting a null (a Type I error). One correction would be to use a Bonferroni correction, testing hypotheses at 1-(0.95) to the (0.05/6) or 0.0085 per-test significance levels or 99.15% confidence intervals (this is the Sidak correction to make the family-wise error rate across tests 0.05).

With that said, assignment to the DRTA treatment instead of traditional teaching is associated with a statistically significant **(\>0.008) effect in test 1**, where it is associated with a 3.55 higher score than traditional treatment. It also has a statistically **significant (0.003) effect in test 3,** where it is associated with 5.83 point higher scores. Assignment to the Strat treatment instead of traditional teaching has a **significant effect (0.00005) in test 2,** where it associated with 3.069 point higher scores.

At 95% intervals, the DRTA treatment has significant effects on test 3 (5.83 points) and the Strata treatment instead of traditional teaching has a significant effect on test 1 (1.9 points).

The pretest coefficient tells us that a one unit increase in pretest scores are associated with a 0.9735 point improvement in test 1 (significant with the Bonferonni correction), a 0.2987 point improvement in test 2 (significant at 95% CIs), and 0.3255 point better scores in test 3 (not significant).

This could be inferred as a suggestion that student's performance on the first test has more to do with conditions outside of teaching method than the other tests.

## 2e.

While I can refer to max and min scores on the pre and outcome tests (1-15, 1-13, 1-57), a first mild barrier to substantive interpretation is not knowing how many points these tests are out of. It's not that I need the scale to understand treatment effect, but rather speaking to substantive meaning of these coefficients probably requires more context of the tests and how they differ than we have here – a 2 point improvement on, for example, a 20-point total test means something different than does a 2 point improvement on a 100 point test. This is particularly true given the small sample size, where I'm confident the variance isn't somewhat more reflective of the small sample than it is the heterogeneity of treatment effect.

That said, if the first test's total possible points is close to its highest score (15 points), then DRTA's 3.55 coefficient suggest it is a substantive intervention producing \~24% better scores. If test 3's highest score (57) similarly reflects its max points, then DRTA produces a 15% increase in test scores, which is nothing to disparage and within the context of student scores is a meaningful effect.

If test 2 is out of 13 points (its max score), the Strata intervention also has a substantive effect, increasing scores by 23%.

However, without knowing how these tests are related to one another (or the pretest), it is difficult to speak to the substantive impact of interventions across tests, or the overall meaningfulness of these teaching interventions compared to traditional methods.

## 2f. 

```{r, echo=FALSE, warning=FALSE, include = FALSE}

Baumann$pretest_m <- Baumann$pretest - mean(Baumann$pretest) #centering at pretest mean because I want to show effect relative to mean pre-treatment score 

m_post1c <- tidy(lm(post.test.1 ~ group + pretest_m, data = Baumann))
m_post2c <- tidy(lm(post.test.2 ~ group + pretest_m, data = Baumann))
m_post3c <- tidy(lm(post.test.3 ~ group + pretest_m, data = Baumann))

# Post Test 1
ci95_low <- m_post1c$estimate - 1.96 * m_post1c$std.error
ci95_hi <- m_post1c$estimate + 1.96 * m_post1c$std.error

p_99 <- qnorm(1-0.0085/2)

m_post1c$ci99_low <- m_post1c$estimate - p_99 * m_post1c$std.error
m_post1c$ci99_hi<- m_post1c$estimate + p_99 * m_post1c$std.error

# Post Test 2
ci95_low_2 <- m_post2c$estimate - 1.96 * m_post2c$std.error
ci95_hi_2 <- m_post2c$estimate + 1.96 * m_post2c$std.error

p_99 <- qnorm(1-0.0085/2)

m_post2c$ci99_low_2 <- m_post2c$estimate - p_99 * m_post2c$std.error
m_post2c$ci99_hi_2<- m_post2c$estimate + p_99 * m_post2c$std.error

# Post Test 3
ci95_low_3 <- m_post3c$estimate - 1.96 * m_post3c$std.error
ci95_hi_3 <- m_post3c$estimate + 1.96 * m_post3c$std.error

p_99 <- qnorm(1-0.0085/2)

m_post3c$ci99_low_3 <- m_post3c$estimate - p_99 * m_post3c$std.error
m_post3c$ci99_hi_3<- m_post3c$estimate + p_99 * m_post3c$std.error
```

```{r, echo=FALSE, warning=FALSE, include = FALSE}
label_map <- c(
  "(Intercept)" = "Avg. Basal Student",
  "groupDRTA"   = "Treatment: DRTA",
  "groupStrat"  = "Treatment: Strat",
  "pretest_m"   = "Pretest Score",
  "term" = "Teaching"
)

```

```{}
```

```{r, echo=FALSE, warning=FALSE}

#plotting post test 1

ggplot(m_post1c, aes(x = estimate, y = term)) + geom_point() + geom_errorbarh(aes(xmin = ci95_low, xmax = ci95_hi), height = 0.2, size = 1, linetype = "dashed") + geom_errorbarh(aes(xmin = ci99_low, xmax = ci99_hi),
                                                                                                                                                             height = 0.1, color = "darkgreen", size = 1) +
  geom_vline(xintercept = 0, linetype = "dashed") + scale_y_discrete(labels = label_map)+
  labs(x = "Coefficient Estimate", y = "", title = "Effect of Teaching Styles on Test 1", subtitle = "Centered on Average Pretest Score", caption = "99% CIs reported in green. 95% CIs reported in black.") +
  theme_minimal()
```

```{r, echo=FALSE, warning=FALSE}
#plotting post test 2

ggplot(m_post2c, aes(x = estimate, y = term)) + geom_point() + geom_errorbarh(aes(xmin = ci95_low_2, xmax = ci95_hi_2), height = 0.2, size = 1, linetype = "dashed") + geom_errorbarh(aes(xmin = ci99_low_2, xmax = ci99_hi_2),
                                                                                                                                                             height = 0.1, color = "darkgreen", size = 1) +
  geom_vline(xintercept = 0, linetype = "dashed") + scale_y_discrete(labels = label_map) +
  labs(x = "Coefficient Estimate", y = "",title = "Effect of Teaching Styles on Test 2", subtitle = "Centered on Average Pretest Score", caption = "99% CIs reported in green. 95% CIs reported in black.") +
  theme_minimal()
```

```{r, echo=FALSE, warning=FALSE}
#plotting post test 3

label_map <- c(
  "(Intercept)" = "Avg. Basal Student",
  "groupDRTA"   = "Treatment: DRTA",
  "groupStrat"  = "Treatment: Strat",
  "pretest_m"   = "Pretest Score",
  "term" = "Teaching"
)

ggplot(m_post3c, aes(x = estimate, y = term)) + geom_point() + geom_errorbarh(aes(xmin = ci95_low_3, xmax = ci95_hi_3), height = 0.2, size = 1, linetype = "dashed") + geom_errorbarh(aes(xmin = ci99_low_3, xmax = ci99_hi_3),
                                                                                                                                                             height = 0.1, color = "darkgreen", size = 1) +
  geom_vline(xintercept = 0, linetype = "dashed") + scale_y_discrete(labels = label_map) +
  labs(x = "Coefficient Estimate", y = "", title = "Effect of Teaching Styles on Test 3", subtitle = "Centered on Average Pretest Score", caption = "99% CIs reported in green. 95% CIs reported in black.") +
  theme_minimal()

```
